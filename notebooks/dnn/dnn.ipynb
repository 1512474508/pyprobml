{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/dnn/dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMlYEa1DlIdI",
        "colab_type": "text"
      },
      "source": [
        "# Deep neural networks (Unfinished)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w50lU39oLuw",
        "colab_type": "text"
      },
      "source": [
        "# Software for deep learning <a class=\"anchor\" id=\"DL\"></a>\n",
        "\n",
        "\n",
        "Deep learning is about composing differentiable functions into more complex functions, represented as a computation graph, and then using automatic differentiation (\"autograd\") to compute gradients, which we can pass to an optimizer, to fit the function to data. This is sometimes called \"differentiable programming\". Furthermore, it is desirable to  execute such computation graphs on hardware accelerators, such as GPUs. We list a few popular libraries which support such functionality below.\n",
        "\n",
        "All of these libraries are quite similar (convergent evolution!).\n",
        "The most unique one is JAX, which uses a pure functional programming (stateless) approach. Also, it is much lower level, and gives the user more control (e.g., computing per-example gradients). Overall it provides an experience that is similar to vanilla numpy programming, but much faster (eg using vmap and jit).\n",
        "\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <th style=\"text-align:left\">Name</th>\n",
        "      <th style=\"text-align:left\">More info</th>\n",
        "    <tr> \n",
        "        <td style=\"text-align:left\"> <a href=\"http://www.tensorflow.org\">Tensorflow 2.0</a></td>\n",
        "     <td style=\"text-align:left\"><a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/tf.ipynb\">TF notebook</a></td>\n",
        "    <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"http://pytorch.org\">Pytorch 1.0</a></td>\n",
        "       <td style=\"text-align:left\">\n",
        "         <a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/pytorch.ipynb\">PyTorch notebook</a></td>\n",
        "               <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"http://github.com/google/jax\">JAX</a></td>\n",
        "            <td style=\"text-align:left\">\n",
        "              <a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/jax.ipynb\">JAX notebook</a></td>\n",
        "          <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"https://mxnet.apache.org/\">MXNet</a></td>\n",
        "              <td style=\"text-align:left\">\n",
        "                <a href=\"http://www.d2l.ai/\">  Dive into deep learning book</a></td>    \n",
        " </table>\n",
        "     \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd1rDAfdHyNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}