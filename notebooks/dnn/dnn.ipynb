{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/dnn/dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMlYEa1DlIdI",
        "colab_type": "text"
      },
      "source": [
        "# Deep neural networks (Unfinished)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w50lU39oLuw",
        "colab_type": "text"
      },
      "source": [
        "# Software for deep learning <a class=\"anchor\" id=\"DL\"></a>\n",
        "\n",
        "\n",
        "Deep learning is about composing differentiable functions into more complex functions, represented as a computation graph, and then using automatic differentiation (\"autograd\") to compute gradients, which we can pass to an optimizer, to fit the function to data. This is sometimes called \"differentiable programming\".\n",
        "\n",
        "There are several libraries that can execute such computation graphs on hardware accelerators, such as GPUs. (Some libraries also support distributed computation, but we will not need use this feature in this book.) We list a few popular libraries below.\n",
        "\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <th style=\"text-align:left\">Name</th>\n",
        "        <th style=\"text-align:left\" width=\"100\">Functionality</th>\n",
        "      <th style=\"text-align:left\">More info</th>\n",
        "    <tr> \n",
        "        <td style=\"text-align:left\"> <a href=\"http://www.tensorflow.org\">Tensorflow 2.0</a></td>\n",
        "            <td style=\"text-align:left\"> Accelerated NumPy-like library, autograd, Keras API.</td>\n",
        "     <td style=\"text-align:left\"><a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/tf.ipynb\">TF notebook</a>\n",
        "    <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"http://pytorch.org\">Pytorch 1.0</a></td>\n",
        "         <td style=\"text-align:left\"> Similar to TF 2.0</td>\n",
        "       <td style=\"text-align:left\">\n",
        "         <a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/pytorch.ipynb\">PyTorch notebook</a>\n",
        "         <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"http://github.com/google/jax\">JAX</a></td>\n",
        "        <td style=\"text-align:left\">Accelerated numpy, autograd, JIT, VMAP</td>\n",
        "            <td style=\"text-align:left\">\n",
        "              <a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/intro/jax.ipynb\">JAX notebook</a>\n",
        "              <tr>\n",
        "        <td style=\"text-align:left\"> <a href=\"https://mxnet.apache.org/\">MXNet</a>\n",
        "            <td  style=\"text-align:left\"> Similar to TF 2.0, Gluon  API.\n",
        "              <td style=\"text-align:left\">\n",
        "                <a href=\"http://www.d2l.ai/\">  Dive into deep learning book</a>       \n",
        "</table>\n",
        "     \n",
        "\n"
      ]
    }
  ]
}