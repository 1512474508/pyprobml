{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on linear algebra in NumPy and Jax\n",
    "\n",
    "## TOC:\n",
    "* [Basics](#basics)\n",
    "* [Sparse matrices](#sparse)\n",
    "* [Broadcasting](#broadcasting)\n",
    "* [Einstein summation](#einstein)\n",
    "* [Norms](#size)\n",
    "* [Eigenvalue decomposition](#EVD)\n",
    "* [Singular value decomposition](#SVD)\n",
    "* [Other decompositions](#decomp)\n",
    "* [Matrix calculus](#calculus)\n",
    "* [Linear systems of equations](#linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp # original numpy \n",
    "onp.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics <a class=\"anchor\" id=\"basics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "1\n",
      "(3,)\n",
      "0 1 2\n"
     ]
    }
   ],
   "source": [
    "# Create 1d vector\n",
    "v = np.array([0,1,2]) # 1d vector\n",
    "print(v.ndim) ## 1\n",
    "print(v.shape) ## (3,)\n",
    "\n",
    "\n",
    "# Note that Python uses 0-indexing, not 1-indexing.\n",
    "# Thus the elements are accessed as follows:\n",
    "print(v[0], v[1], v[2]) ## 0 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "2\n",
      "(2, 3)\n",
      "6\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create 2d array\n",
    "A = np.array([ [0,1,2], [3,4,5] ]) \n",
    "print(A)\n",
    "## [[0, 1, 2],\n",
    "##  [3, 4, 5]])\n",
    "print(A.ndim) ## 2\n",
    "print(A.shape) ## (2,3)\n",
    "print(A.size) ## 6\n",
    "print(A.T.shape) ## (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# If we want to make a vector into a matrix with one row, we can use any of the following:\n",
    "\n",
    "x = np.array([1,2]) # vector\n",
    "X1 = np.array([x]) # matrix with one row\n",
    "X2 = np.reshape(x, (1,-1))\n",
    "X3 = x[None, :]\n",
    "X4 = x[np.newaxis, :]\n",
    "assert np.array_equal(X1, X2)\n",
    "assert np.array_equal(X1, X3)\n",
    "print(np.shape(X1)) ## (1,2)\n",
    "\n",
    "\n",
    "\n",
    "# If we want to make a vector into a matrix with one column, we can use any of the following:\n",
    "x = np.array([1,2]) # vector\n",
    "X1 = np.array([x]).T # matrix with one column\n",
    "X2 = np.reshape(x, (-1,1))\n",
    "X3 = x[:, None]\n",
    "X4 = x[:, np.newaxis]\n",
    "assert np.array_equal(X1, X2)\n",
    "assert np.array_equal(X1, X3)\n",
    "print(np.shape(X1)) ## (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Here is how to create a one-hot encoding of integers.\n",
    "\n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "  return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "\n",
    "# Example\n",
    "x = np.array([1,2,0,2]);\n",
    "X = one_hot(x, 3)\n",
    "print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can construct arrays from a list of column vectors as follows:\n",
    "A1 = np.array([ [0,1,2], [3,4,5] ]) \n",
    "col0 = A1[:,0]; col1 = A1[:,1]; col2=A1[:,2];\n",
    "A2 = np.stack([col0,col1,col2],axis=1)\n",
    "assert np.array_equal(A1, A2)\n",
    "\n",
    "# We can construct arrays from a list of row vectors as follows:\n",
    "row0=A1[0,:]; row1=A1[1,:];\n",
    "A2 = np.stack([row0,row1],axis=0)\n",
    "assert np.array_equal(A1, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  8  7 99]\n",
      " [ 6  5  4 99]]\n"
     ]
    }
   ],
   "source": [
    "# We can construct arrays from a list of arrays\n",
    "# using the hstack or vstack functions,\n",
    "# which stack horizontally or vertically,  as illustrated below.\n",
    "\n",
    "M = np.array([[9,8,7],[6,5,4]])\n",
    "C = np.array([[99], [99]])\n",
    "A1 = np.concatenate([M, C], axis=1)\n",
    "A2 = np.hstack([M, C])\n",
    "#A3 = np.c_[M, C] # c_ does not work in jax\n",
    "assert np.array_equal(A1, A2)\n",
    "#assert np.array_equal(A1, A3)\n",
    "print(A1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [9 8 7]\n",
      " [6 5 4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R = np.array([[1,2,3]])\n",
    "A1 = np.concatenate([R, M], axis=0)\n",
    "A2 = np.vstack([R, M])\n",
    "assert np.array_equal(A1, A2)\n",
    "print(A1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 9. 8. 7.]\n",
      " [1. 6. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# A very common idiom  is to add a column of 1s to a datamatrix.\n",
    "# We can do this using horizontal stacking (along the columns) as follows.\n",
    "\n",
    "X = np.array([[9,8,7],[6,5,4]])\n",
    "N = np.shape(X)[0] # num. rows\n",
    "X1 = np.hstack([np.ones((N,1)), X])\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We can flatten a matrix to a vector (concatenating its rows, one by one) using ravel\n",
    "\n",
    "A = np.reshape(np.arange(6),(2,3))\n",
    "print(A.ravel()) ##  [0 1 2 3 4 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy,  arrays are layed out in memory\n",
    "such that, if we iterate over neighboring elements,\n",
    "the rightmost index changes the fastest.\n",
    "This is called row-major order,\n",
    "and is used by other languages such as C++, Eigen and PyTorch.\n",
    "By contrast, other languages (such as Julia, Matlab, R and Fortran)\n",
    "use column-major order.\n",
    "See below for an illustration of the difference.\n",
    "\n",
    "\n",
    "<table border=\"0\" width=\"400\">\n",
    "    <td><img src=\"figures/rowMajor.png\"  width=\"200\"></td>\n",
    "    <td><img src=\"figures/colMajor.png\"  width=\"200\"></td>\n",
    "</table>\n",
    "\n",
    "\n",
    "(Source: https://commons.wikimedia.org/wiki/File:Row_and_column_major_order.svg)\n",
    "\n",
    "Thus in numpy, for speed reasons, we should always write loops like this:\n",
    "```\n",
    "A = np.reshape(np.arange(6),(2,3))\n",
    "d1, d2 = np.shape(A)\n",
    "for i in range(d1):\n",
    "  for j in range(d2):\n",
    "    # Do something with A[i,j]\n",
    " ```\n",
    "\n",
    "For similar reasons, data matrices are usually stored\n",
    "in the form $(N,D)$, where $N$ is the batchsize (number of examples),\n",
    "so that we can efficiently extract minibatches by slicing blocks of consecutive memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n"
     ]
    }
   ],
   "source": [
    "## We can create a tensor in numpy as in this example:\n",
    "\n",
    "T = np.ndarray([2,3,4]) # fill with random values\n",
    "T = np.reshape(np.arange(24),(2,3,4)) # fill with 0..23\n",
    "print(np.shape(T))\n",
    "print(T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "#We can permute the order of the dimensions using np.transpose.\n",
    "\n",
    "x = np.ones((1, 2, 3))\n",
    "print(np.transpose(x, (1, 0, 2)).shape) ## (2, 1, 3)\n",
    "\n",
    "#Note that this does not actually move the data in memory\n",
    "#(which would be slow),\n",
    "#it merely provides a different \\keywordDef{view} of the same data,\n",
    "#i.e., it changes the mapping from $n$-dimensional vectors of\n",
    "#subscripts to 1d integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.298 0.859 1.059 0.369]\n",
      " [1.568 0.942 1.311 0.477]]\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication \n",
    "\n",
    "A = onp.random.rand(2,3);\n",
    "B = onp.random.rand(3,4);\n",
    "C = np.dot(A,B)\n",
    "assert np.shape(C) == (2,4)\n",
    "print(C)\n",
    "C2 = A.dot(B)\n",
    "C3 = A @ B\n",
    "assert np.allclose(C, C2)\n",
    "assert np.allclose(C, C3)\n",
    "\n",
    "#Note that we need to use np.dot(A,B)\n",
    "#if we use A * B, Python tries to compute the elementwise product,\n",
    "#which is invalid, since $A$ and $B$ have incompatible shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "# Outer products\n",
    "\n",
    "x = np.arange(1,3); y = np.arange(1,3); \n",
    "A = np.outer(x,y);\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 5. 7.]]\n"
     ]
    }
   ],
   "source": [
    "# We can sum across the rows\n",
    "\n",
    "X = np.reshape(np.arange(6), (2,3))\n",
    "XS = np.dot(np.ones((1,2)), X)\n",
    "print(XS)\n",
    "XS2 = np.sum(X, axis=0)\n",
    "assert np.allclose(XS, XS2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.]\n",
      " [12.]]\n"
     ]
    }
   ],
   "source": [
    "# We can sum across the columns \n",
    "\n",
    "X = np.reshape(np.arange(6), (2,3))\n",
    "XS = np.dot(X, np.ones((3,1)))\n",
    "print(XS)\n",
    "XS2 = np.sum(X, axis=1).reshape(-1, 1)\n",
    "assert np.allclose(XS, XS2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sum across all entries\n",
    "\n",
    "X = np.reshape(np.arange(6), (2,3))\n",
    "S1 = np.dot(np.ones((1,2)), np.dot(X, np.ones((3,1))))[0]\n",
    "S2 = np.sum(X)\n",
    "assert np.allclose(S1, S2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1., 1., 0., 0.],\n",
       "             [1., 1., 0., 0.],\n",
       "             [0., 0., 1., 1.],\n",
       "             [0., 0., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kronecker product\n",
    "\n",
    "np.kron(np.eye(2), np.ones((2,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4161983\n",
      "5.0\n",
      "3.7022145\n",
      "7.0724216\n",
      "4.320558\n",
      "2.8176277\n",
      "2.5269188897052897\n",
      "12.36230186647291\n"
     ]
    }
   ],
   "source": [
    "# Vector Norms\n",
    "x = np.arange(6)\n",
    "print(np.linalg.norm(x, 2) ** 2)\n",
    "print(np.sum(np.power(x, 2)))\n",
    "print(np.linalg.norm(x, np.inf))\n",
    "\n",
    "# Matrix norms\n",
    "A = onp.random.randn(4,4)\n",
    "print(np.linalg.norm(A, ord=2))\n",
    "print(np.linalg.norm(A, ord='nuc'))\n",
    "print(np.linalg.norm(A, ord='fro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of a matrix - not  all operations are supported by jax\n",
    "\n",
    "print(np.trace(A))\n",
    "print(onp.linalg.det(A))\n",
    "print(onp.linalg.cond(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrices  <a class=\"anchor\" id=\"sparse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t2.0\n",
      "  (2, 2)\t3.0\n",
      "[[1. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import diags\n",
    "A = diags([1,2,3])\n",
    "print(A)\n",
    "print(A.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [0, 0, 4, 5],\n",
       "       [0, 0, 6, 7]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block diagonal\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "block_diag([2, 3], [[4, 5], [6, 7]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Band diagonal\n",
    "\n",
    "See (https://pypi.org/project/bandmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting  <a class=\"anchor\" id=\"broadcasting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, the command A * B computes the elementwise multiplication of arrays or tensors A and B.\n",
    "If these arrays have different shapes,\n",
    "they will be automatically converted to have compatible shapes by\n",
    "implictly replicating  certain dimensions; this is called\n",
    "**broadcasting**. The following conversion rules are applied\n",
    "in order:\n",
    "\n",
    "* If the two arrays differ in their number of dimensions, the\n",
    "   shape of the one with fewer dimensions is padded with ones on the\n",
    "   left side. For example, a scalar will be converted to a vector,\n",
    "   and a vector to a matrix with one row.\n",
    "* If the shape of the two arrays does not match in any dimension,\n",
    "   the array with shape equal to 1 in that dimension is stretched to\n",
    "   match the other shape, by replicating the corresponding contents.\n",
    "* If in any dimension the sizes disagree and neither is equal to\n",
    "   1, an error is raised.\n",
    "\n",
    "This is illustrated below.\n",
    "\n",
    "![Broadcasting](figures/broadcasting.png \"Broadcasting\")\n",
    "\n",
    "\n",
    "Source: Figure 2.4 of [VanderPlas2016](https://github.com/jakevdp/PythonDataScienceHandbook).\n",
    "Thanks to Jake VanderPlas.\n",
    "Made by [broadcasting_fig.py](https://github.com/probml/pyprobml/blob/master/scripts/broadcasting_fig.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  6]\n",
      " [ 3  8 15]]\n"
     ]
    }
   ],
   "source": [
    "# Example: scaling each column\n",
    "X = np.reshape(np.arange(6), (2,3))\n",
    "s = np.array([1,2,3])\n",
    "XS = X * s \n",
    "print(XS)\n",
    "XS2 = np.dot(X, np.diag(s)) # post-multiply by diagonal\n",
    "assert np.allclose(XS, XS2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 6  8 10]]\n"
     ]
    }
   ],
   "source": [
    "# Example: scaling each row\n",
    "X = np.reshape(np.arange(6), (2,3))\n",
    "s  = np.array([1,2])\n",
    "XS = X *  np.reshape(s, (-1,1)) \n",
    "print(XS)\n",
    "XS2 = np.dot(np.diag(s), X) # pre-multiply by diagonal\n",
    "assert np.allclose(XS, XS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein summation  <a class=\"anchor\" id=\"broadcasting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einstein summation lets us write formula such as  inputs -> outputs, which name the dimensions \n",
    "of the input tensor and output tensors; dimensions which are not named in the output are summed over - this is called **tensor contraction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "a = np.arange(3)\n",
    "b = np.arange(3)\n",
    "A = np.arange(6).reshape(2,3)\n",
    "B = np.arange(15).reshape(3,5)\n",
    "S = np.arange(9).reshape(3,3)\n",
    "T = onp.random.randn(2,2,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider einsum with  a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Matrix transpose\n",
    "assert np.allclose(A.T, np.einsum('ij->ji', A))\n",
    "\n",
    "# Sum all elements\n",
    "assert np.allclose(np.sum(A), np.einsum('ij->', A))\n",
    "\n",
    "# Sum across rows\n",
    "assert np.allclose(np.sum(A, axis=0), np.einsum('ij->j', A))\n",
    "\n",
    "# Sum across columns\n",
    "assert np.allclose(np.sum(A, axis=1), np.einsum('ij->i', A))\n",
    "\n",
    "# Sum specific axis of tensor\n",
    "assert np.allclose(np.sum(T, axis=1), np.einsum('ijkl->ikl', T))\n",
    "assert np.allclose(np.sum(np.sum(T, axis=0), axis=0), np.einsum('ijkl->kl', T))\n",
    "\n",
    "# repeated indices with one arg extracts diagonals\n",
    "assert np.allclose(np.diag(S), np.einsum('ii->i', S))\n",
    "          \n",
    "# Trace\n",
    "assert np.allclose(np.trace(S), np.einsum('ii->', S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider einsum with 2 tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Matrix vector multiplication\n",
    "assert np.allclose(np.dot(A, b), np.einsum('ik,k->i', A, b))\n",
    "\n",
    "# Matrix matrix multiplication\n",
    "assert np.allclose(np.dot(A, B), np.einsum('ik,kj->ij', A, B))\n",
    "assert np.allclose(np.matmul(A, B), np.einsum('ik,kj->ij', A, B))\n",
    "\n",
    "# Inner product \n",
    "assert np.allclose(np.dot(a, b), np.einsum('i,i->', a, b))\n",
    "assert np.allclose(np.inner(a, b), np.einsum('i,i->', a, b))\n",
    "\n",
    "# Outer product\n",
    "assert np.allclose(np.outer(a, b), np.einsum('i,j->ij', a, b))\n",
    "\n",
    "# Elementwise product\n",
    "assert np.allclose(a * a, np.einsum('i,i->i', a, a))\n",
    "assert np.allclose(A * A, np.einsum('ij,ij->ij', A, A))\n",
    "assert np.allclose(np.multiply(A, A), np.einsum('ij,ij->ij', A, A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a more complex example,\n",
    " suppose we have a 3d tensor $S_{ntk}$ where $n$ indexes examples in the\n",
    " batch, $t$ indexes locations in the sequence, and $k$ indexes words\n",
    " in a one-hot representation.\n",
    " Let $W_{kd}$ be an embedding matrix that maps sparse one-hot vectors\n",
    " $R^k$  to dense vectors in $R^d$.\n",
    " We can convert the batch of sequences of one-hots\n",
    " to a batch of sequences of embeddings as follows:\n",
    "$$\n",
    "E_{ntd} = \\sum_k S_{ntk} W_{kd}\n",
    "$$\n",
    "We can compute the sum of the embedding vectors for\n",
    "each sequence (to get a global representation\n",
    "of each bag of words) as follows:\n",
    "$$\n",
    "E_{nd} = \\sum_k \\sum_t S_{ntk} W_{kd}\n",
    "$$\n",
    "Finally we can pass each sequence's vector representation\n",
    "through another linear transform $V_{dc}$ to map to the logits over a\n",
    "classifier\n",
    "with $c$ labels:\n",
    "$$\n",
    "L_{nc} = \\sum_d E_{nd} V_{dc}\n",
    "= \\sum_d \\sum_k \\sum_t S_{ntk} W_{kd} V_{dc}\n",
    "$$\n",
    "In einsum notation, we have\n",
    "$$\n",
    "L_{nc} = S_{ntk} W_{kd} V_{dc}\n",
    "$$\n",
    "We sum  over $k$ and $d$  because those\n",
    "indices occur twice on the RHS.\n",
    "We sum over $t$  because that index does not occur\n",
    "on the LHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding example in code\n",
    "\n",
    "N = 2; C = 3; D = 4; K = 5; T = 6;\n",
    "S = onp.random.randn(N, T, K)\n",
    "W = onp.random.randn(K, D)\n",
    "V = onp.random.randn(D, C)\n",
    "Lfast = np.einsum('ntk,kd,dc->nc', S, W, V)\n",
    "# Compare to brute force way of computing L below.\n",
    "# We can only do elementwise assignment to L in original numpy, not jax\n",
    "L = onp.zeros((N,C))\n",
    "for n in range(N):\n",
    "    for c in range(C):\n",
    "        s = 0\n",
    "        for d in range(D):\n",
    "            for k in range(K):\n",
    "                for t in range(T):\n",
    "                    s += S[n,t,k] * W[k,d] * V[d,c]\n",
    "        L[n,c] = s # does not work in jax\n",
    "assert np.allclose(L, Lfast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "\n",
    "path = np.einsum_path('ntk,kd,dc->nc', S, W, V, optimize='optimal')[0]\n",
    "assert np.allclose(L, np.einsum('ntk,kd,dc->nc', S, W, V, optimize=path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue decomposition (EVD)<a class=\"anchor\" id=\"EVD\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.194 0.091 1.052]\n",
      "[[-0.118  0.919 -0.376]\n",
      " [ 0.584 -0.242 -0.775]\n",
      " [ 0.803  0.311  0.508]]\n",
      "[6.194 1.052 0.091]\n",
      "[[-0.118 -0.376  0.919]\n",
      " [ 0.584 -0.775 -0.242]\n",
      " [ 0.803  0.508  0.311]]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.randn(3, 4)\n",
    "A = np.dot(M, M.T) # ensure A is positive definite\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "print(evals)\n",
    "print(evecs)\n",
    "\n",
    "# Sort columns so one with largest eval is first\n",
    "idx = np.argsort(np.abs(evals))[::-1] # largest first\n",
    "evecs = evecs[:, idx] # sort columns\n",
    "evals = evals[idx]\n",
    "print(evals)\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
