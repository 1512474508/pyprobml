{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "In this notebook, we explore various optimization problems and algorithms.\n",
    "\n",
    "## TOC\n",
    "* [Automatic differentiation](#AD)\n",
    "* [Stochastic gradient descent](#SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax version 0.1.39\n",
      "jax backend cpu\n",
      "torch version 1.1.0\n",
      "Torch cannot find GPU\n",
      "tf version 2.0.0-dev20190629\n",
      "TF cannot find GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "figdir = \"../figures\" # set this to '' if you don't want to save figures\n",
    "def save_fig(fname):\n",
    "    if figdir:\n",
    "        plt.savefig(os.path.join(figdir, fname))\n",
    "\n",
    "import numpy as onp\n",
    "onp.set_printoptions(precision=3)\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import grad, hessian, jacfwd, jacrev, jit, vmap\n",
    "from jax.experimental import optimizers\n",
    "print(\"jax version {}\".format(jax.__version__))\n",
    "from jax.lib import xla_bridge\n",
    "print(\"jax backend {}\".format(xla_bridge.get_backend().platform))\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "print(\"torch version {}\".format(torch.__version__))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"current device {}\".format(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"Torch cannot find GPU\")\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"tf version {}\".format(tf.__version__))\n",
    "if tf.test.is_gpu_available():\n",
    "    print(tf.test.gpu_device_name())\n",
    "else:\n",
    "    print(\"TF cannot find GPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation <a class=\"anchor\" id=\"AD\"></a>\n",
    "\n",
    "In this section we  illustrate various AD libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD in JAX  <a class=\"anchor\" id=\"AD-jax\"></a>\n",
    "\n",
    "For some examples of using JAX to compute the gradients, Jacobians and Hessians of simple linear and quadratic functions,\n",
    "see [this notebook](https://github.com/probml/pyprobml/blob/master/notebooks/linear_algebra.ipynb#AD-jax).\n",
    "For an example of using JAX to compute the gradient of the NLL for binary logistic regression,\n",
    "see [this notebook](https://github.com/probml/pyprobml/blob/master/notebooks/linear_algebra.ipynb#AD-jax-log).\n",
    "\n",
    "More details on JAX's autodiff can be found in the official [autodiff cookbook](https://github.com/google/jax/blob/master/notebooks/autodiff_cookbook.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD in TF  <a class=\"anchor\" id=\"AD-TF\"></a>\n",
    "\n",
    "Unfinished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD in PyTorch  <a class=\"anchor\" id=\"AD-pytorch\"></a>\n",
    "\n",
    "Unfinished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent <a class=\"anchor\" id=\"SGD\"></a>\n",
    "\n",
    "In this section we  illustrate how to implement SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD in Jax <a class=\"anchor\" id=\"SGD-jax\"></a>\n",
    "\n",
    "JAX has a minimal optimization library focused on stochastic first-order optimizers. Every optimizer is modeled as an (`init_fun`, `update_fun`, `get_params`) triple of functions. The `init_fun` is used to initialize the optimizer state, which could include things like momentum variables, and the `update_fun` accepts a gradient and an optimizer state to produce a new optimizer state. The `get_params` function extracts the current iterate (i.e. the current parameters) from the optimizer state. The parameters being optimized can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can store your parameters however youâ€™d like.\n",
    "\n",
    "We give some examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD for logistic regression <a class=\"anchor\" id=\"SGD-logreg-jax\"></a>\n",
    "\n",
    "We consider a convex problem, namely MLE for binary logistic regression,\n",
    "where we compute the exact optimum. We solve this using a batch solver (BFGS) to create the gold standard.\n",
    "We use Jax for the gradient computations (see  [this notebook](https://github.com/probml/pyprobml/blob/master/notebooks/linear_algebra.ipynb#AD-jax-logreg) for details). Later we also use Jax's optimization library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a dataset.\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris[\"data\"][:,:3] # Just take first 3 features to make problem harder\n",
    "y = (iris[\"target\"] == 2).astype(onp.int)  # 1 if Iris-Virginica, else 0'\n",
    "N, D = X.shape # 150, 4\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.457 -4.877  9.756]\n"
     ]
    }
   ],
   "source": [
    "# Now let's find the MLE using sklearn. We will use this as the \"gold standard\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# We set C to a large number to turn off regularization.\n",
    "# We don't fit the bias term to simplify the comparison below.\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
    "log_reg.fit(X_train, y_train)\n",
    "w_mle_sklearn = np.ravel(log_reg.coef_)\n",
    "print(w_mle_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to fit the model ourselves by defining a suitable objective.\n",
    "\n",
    "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
    "\n",
    "def predict_logit(weights, inputs):\n",
    "    return np.dot(inputs, weights) # Already vectorized\n",
    "\n",
    "def predict_prob(weights, inputs):\n",
    "    return sigmoid(predict_logit(weights, inputs))\n",
    "\n",
    "from jax.scipy.special import logsumexp\n",
    "#from scipy.misc import logsumexp\n",
    "\n",
    "def NLL(weights, batch):\n",
    "    # Use log-sum-exp trick\n",
    "    inputs, targets = batch\n",
    "    # p1 = 1/(1+exp(-logit)), p0 = 1/(1+exp(+logit))\n",
    "    logits = predict_logit(weights, inputs).reshape((-1,1))\n",
    "    N = logits.shape[0]\n",
    "    logits_plus = np.hstack([np.zeros((N,1)), logits]) # e^0=1\n",
    "    logits_minus = np.hstack([np.zeros((N,1)), -logits])\n",
    "    logp1 = -logsumexp(logits_minus, axis=1)\n",
    "    logp0 = -logsumexp(logits_plus, axis=1)\n",
    "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
    "    return -np.sum(logprobs)/N\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function for comparing solvers\n",
    "\n",
    "def evaluate_params(w_opt, w_est, name):\n",
    "    print(\"parameters from optimal\\n{}\".format(w_opt))\n",
    "    print(\"parameters from {}\\n{}\".format(name, w_est))\n",
    "    print(\"max delta: {}\".format(np.max(np.abs(w_opt - w_est))))\n",
    "    \n",
    "def evaluate_preds(w_opt, w_est, name):\n",
    "    p_opt = predict_prob(w_opt, X_test)\n",
    "    p_est = predict_prob(w_est, X_test)\n",
    "    print(\"predictions from optimal\\n{}\".format(np.round(p_opt, 3)))\n",
    "    print(\"predictions from {}\\n{}\".format(name, np.round(p_est, 3)))\n",
    "    print(\"max delta: {}\".format(np.max(np.abs(p_opt - p_est))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch solver.\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "def training_loss(w):\n",
    "    return NLL(w, (X_train, y_train))\n",
    "\n",
    "def training_grad(w):\n",
    "    return grad(training_loss)(w)\n",
    "\n",
    "onp.random.seed(43)\n",
    "w_init = onp.random.randn(D)\n",
    "w_mle_scipy = scipy.optimize.minimize(training_loss, w_init, jac=training_grad, method='BFGS').x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters from optimal\n",
      "[-5.457 -4.877  9.756]\n",
      "parameters from scipy\n",
      "[-5.457 -4.878  9.757]\n",
      "max delta: 0.0014805528335273266\n",
      "predictions from optimal\n",
      "[0.251 0.    1.    0.048 0.019 0.    0.    0.047 0.34  0.001 0.21  0.\n",
      " 0.    0.    0.    0.01  0.998 0.009 0.297 0.998 0.    0.471 0.    0.998\n",
      " 0.689 0.386 1.    0.992 0.    0.    0.    0.    0.    0.    0.    0.901\n",
      " 0.001 0.    0.    0.    0.993 0.004 0.003 0.    0.    0.004 0.848 0.974\n",
      " 0.    0.935]\n",
      "predictions from scipy\n",
      "[0.251 0.    1.    0.048 0.019 0.    0.    0.047 0.341 0.001 0.21  0.\n",
      " 0.    0.    0.    0.01  0.998 0.009 0.297 0.998 0.    0.471 0.    0.998\n",
      " 0.689 0.386 1.    0.992 0.    0.    0.    0.    0.    0.    0.    0.901\n",
      " 0.001 0.    0.    0.    0.993 0.004 0.003 0.    0.    0.004 0.848 0.974\n",
      " 0.    0.935]\n",
      "max delta: 3.3289194107055664e-05\n"
     ]
    }
   ],
   "source": [
    "evaluate_params(w_mle_sklearn, w_mle_scipy, \"scipy\")\n",
    "evaluate_preds(w_mle_sklearn, w_mle_scipy, \"scipy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2 3.4 5.4]\n",
      " [7.4 2.8 6.1]]\n",
      "[[5.6 3.  4.5]\n",
      " [4.6 3.4 1.4]]\n",
      "[[5.6 3.  4.1]\n",
      " [5.9 3.  5.1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyBatcher():\n",
    "    def __init__(self, X, y, batch_size, seed):\n",
    "        self.num_data = X.shape[0]\n",
    "        num_complete_batches, leftover = divmod(self.num_data, batch_size)\n",
    "        self.num_batches = num_complete_batches + bool(leftover)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.batch_stream = self.make_data_stream()\n",
    "                \n",
    "    def make_data_stream(self):\n",
    "        rng = onp.random.RandomState(self.seed)\n",
    "        while True:\n",
    "            perm = rng.permutation(self.num_data)\n",
    "            for i in range(self.num_batches):\n",
    "                batch_idx = perm[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                yield self.X[batch_idx], self.y[batch_idx]\n",
    "\n",
    "batcher = MyBatcher(X_train, y_train, batch_size=3, seed=0)\n",
    "nbatches = 3\n",
    "for i in range(nbatches):\n",
    "    batch = next(batcher.batch_stream)\n",
    "    x, y = batch\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bare bones SGD\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "def sgd_v1(params, loss_fn, batcher, num_epochs, lr):\n",
    "    itercount = itertools.count()\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for step in range(batcher.num_batches):\n",
    "            total_steps = next(itercount)\n",
    "            batch = next(batcher.batch_stream)\n",
    "            batch_loss = loss_fn(params, batch)\n",
    "            batch_grad = grad(loss_fn)(params, batch)\n",
    "            params = params - lr*batch_grad\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_loss = onp.float(loss_fn(params, (batcher.X, batcher.y)))\n",
    "        loss_history.append(train_loss)\n",
    "        if True: #epoch % 500 == 0:\n",
    "            print('Epoch {}, train NLL {}'.format(epoch, train_loss))\n",
    "    return params, loss_history\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train NLL 0.8947761654853821\n",
      "Epoch 1, train NLL 0.4824911952018738\n",
      "Epoch 2, train NLL 0.4509454667568207\n",
      "Epoch 3, train NLL 0.4528477191925049\n",
      "Epoch 4, train NLL 0.8981762528419495\n",
      "[ 0.28  -1.151  0.869]\n",
      "[0.8947761654853821, 0.4824911952018738, 0.4509454667568207, 0.4528477191925049, 0.8981762528419495]\n"
     ]
    }
   ],
   "source": [
    "onp.random.seed(43)\n",
    "D = X_train.shape[1]\n",
    "w_init = onp.random.randn(D)\n",
    "\n",
    "batcher = MyBatcher(X_train, y_train, batch_size=10, seed=0)\n",
    "max_epochs = 5\n",
    "lr = 0.1\n",
    "w_mle_sgd1, history = sgd_v1(w_init, NLL, batcher, num_epochs=max_epochs, lr=lr)\n",
    "print(w_mle_sgd1)\n",
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version that uses JAX optimization library\n",
    "\n",
    "@jit\n",
    "def sgd_jax(params, loss_fn, batcher, max_epochs, opt_init, opt_update, get_params):\n",
    "    itercount = itertools.count()\n",
    "    loss_history = []\n",
    "    opt_state = opt_init(params)\n",
    "    \n",
    "    @jit\n",
    "    def update(i, opt_state, batch):\n",
    "        params = get_params(opt_state)\n",
    "        g = grad(loss_fn)(params, batch)\n",
    "        return opt_update(i, g, opt_state) \n",
    "    \n",
    "    print_every = max(1, int(0.1*max_epochs))\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "        for step in range(batcher.num_batches):\n",
    "            total_steps = next(itercount)\n",
    "            batch = next(batcher.batch_stream)\n",
    "            opt_state = update(total_steps, opt_state, batch)\n",
    "        epoch_time = time.time() - start_time\n",
    "        params = get_params(opt_state)\n",
    "        train_loss = onp.float(loss_fn(params, (batcher.X, batcher.y)))\n",
    "        loss_history.append(train_loss)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch {}, train NLL {}'.format(epoch, train_loss))\n",
    "    return params, loss_history\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train NLL 0.8947761654853821\n",
      "Epoch 1, train NLL 0.4824911952018738\n",
      "Epoch 2, train NLL 0.4509454667568207\n",
      "Epoch 3, train NLL 0.4528477191925049\n",
      "Epoch 4, train NLL 0.8981762528419495\n",
      "[ 0.28  -1.151  0.869]\n",
      "[0.8947761654853821, 0.4824911952018738, 0.4509454667568207, 0.4528477191925049, 0.8981762528419495]\n"
     ]
    }
   ],
   "source": [
    "# JAX with constant LR should match our minimal version of SGD\n",
    "\n",
    "schedule = optimizers.constant(step_size=lr)\n",
    "opt_init, opt_update, get_params = optimizers.sgd(step_size=schedule)\n",
    "\n",
    "max_epochs = 5\n",
    "batcher = MyBatcher(X_train, y_train, batch_size=10, seed=0)\n",
    "w_mle_sgd2, history = sgd_jax(w_init, NLL, batcher, max_epochs, \n",
    "                              opt_init, opt_update, get_params)\n",
    "print(w_mle_sgd2)\n",
    "print(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train NLL 0.8947761654853821\n",
      "Epoch 1, train NLL 0.4824911952018738\n",
      "Epoch 2, train NLL 0.4509454667568207\n",
      "Epoch 3, train NLL 0.4528477191925049\n",
      "Epoch 4, train NLL 0.8981762528419495\n",
      "Epoch 5, train NLL 0.5361756086349487\n",
      "Epoch 6, train NLL 0.3979598581790924\n",
      "Epoch 7, train NLL 0.3691052496433258\n",
      "Epoch 8, train NLL 0.46815353631973267\n",
      "Epoch 9, train NLL 0.3966706097126007\n",
      "Epoch 10, train NLL 0.36759117245674133\n",
      "Epoch 11, train NLL 0.39221227169036865\n",
      "Epoch 12, train NLL 0.3439350128173828\n",
      "Epoch 13, train NLL 0.3351922333240509\n",
      "Epoch 14, train NLL 0.3376809358596802\n",
      "Epoch 15, train NLL 0.31386876106262207\n",
      "Epoch 16, train NLL 0.3150806725025177\n",
      "Epoch 17, train NLL 0.348689466714859\n",
      "Epoch 18, train NLL 0.31649914383888245\n",
      "Epoch 19, train NLL 0.3438558280467987\n",
      "Epoch 20, train NLL 0.2909436821937561\n",
      "Epoch 21, train NLL 0.2879621088504791\n",
      "Epoch 22, train NLL 0.2820524275302887\n",
      "Epoch 23, train NLL 0.3101750910282135\n",
      "Epoch 24, train NLL 0.28492245078086853\n",
      "Epoch 25, train NLL 0.3714085519313812\n",
      "Epoch 26, train NLL 0.27555203437805176\n",
      "Epoch 27, train NLL 0.2660960853099823\n",
      "Epoch 28, train NLL 0.2631305456161499\n",
      "Epoch 29, train NLL 0.27020037174224854\n",
      "Epoch 30, train NLL 0.334455668926239\n",
      "Epoch 31, train NLL 0.25727248191833496\n",
      "Epoch 32, train NLL 0.2802165448665619\n",
      "Epoch 33, train NLL 0.26199352741241455\n",
      "Epoch 34, train NLL 0.24892841279506683\n",
      "Epoch 35, train NLL 0.2811424732208252\n",
      "Epoch 36, train NLL 0.24817445874214172\n",
      "Epoch 37, train NLL 0.24715225398540497\n",
      "Epoch 38, train NLL 0.2466714233160019\n",
      "Epoch 39, train NLL 0.3042636811733246\n",
      "Epoch 40, train NLL 0.239762082695961\n",
      "Epoch 41, train NLL 0.2385147511959076\n",
      "Epoch 42, train NLL 0.24040529131889343\n",
      "Epoch 43, train NLL 0.23272697627544403\n",
      "Epoch 44, train NLL 0.23069815337657928\n",
      "Epoch 45, train NLL 0.236114040017128\n",
      "Epoch 46, train NLL 0.2885432541370392\n",
      "Epoch 47, train NLL 0.23225350677967072\n",
      "Epoch 48, train NLL 0.22992408275604248\n",
      "Epoch 49, train NLL 0.22491854429244995\n",
      "Epoch 50, train NLL 0.2286291867494583\n",
      "Epoch 51, train NLL 0.22543612122535706\n",
      "Epoch 52, train NLL 0.2235066294670105\n",
      "Epoch 53, train NLL 0.21741239726543427\n",
      "Epoch 54, train NLL 0.21763162314891815\n",
      "Epoch 55, train NLL 0.2920655906200409\n",
      "Epoch 56, train NLL 0.2813718914985657\n",
      "Epoch 57, train NLL 0.220219686627388\n",
      "Epoch 58, train NLL 0.21237613260746002\n",
      "Epoch 59, train NLL 0.21042530238628387\n",
      "Epoch 60, train NLL 0.2109304964542389\n",
      "Epoch 61, train NLL 0.22182431817054749\n",
      "Epoch 62, train NLL 0.25304096937179565\n",
      "Epoch 63, train NLL 0.20946654677391052\n",
      "Epoch 64, train NLL 0.27163058519363403\n",
      "Epoch 65, train NLL 0.20933417975902557\n",
      "Epoch 66, train NLL 0.21246597170829773\n",
      "Epoch 67, train NLL 0.2091362178325653\n",
      "Epoch 68, train NLL 0.2053808718919754\n",
      "Epoch 69, train NLL 0.20730488002300262\n",
      "Epoch 70, train NLL 0.22066721320152283\n",
      "Epoch 71, train NLL 0.21628160774707794\n",
      "Epoch 72, train NLL 0.21179033815860748\n",
      "Epoch 73, train NLL 0.22777605056762695\n",
      "Epoch 74, train NLL 0.2168767899274826\n",
      "Epoch 75, train NLL 0.20367896556854248\n",
      "Epoch 76, train NLL 0.19525325298309326\n",
      "Epoch 77, train NLL 0.21592290699481964\n",
      "Epoch 78, train NLL 0.20874062180519104\n",
      "Epoch 79, train NLL 0.1936051994562149\n",
      "Epoch 80, train NLL 0.19232060015201569\n",
      "Epoch 81, train NLL 0.19535943865776062\n",
      "Epoch 82, train NLL 0.2023833841085434\n",
      "Epoch 83, train NLL 0.1958199441432953\n",
      "Epoch 84, train NLL 0.1893857717514038\n",
      "Epoch 85, train NLL 0.18880225718021393\n",
      "Epoch 86, train NLL 0.1949765831232071\n",
      "Epoch 87, train NLL 0.18783828616142273\n",
      "Epoch 88, train NLL 0.2133907824754715\n",
      "Epoch 89, train NLL 0.18921393156051636\n",
      "Epoch 90, train NLL 0.19776448607444763\n",
      "Epoch 91, train NLL 0.18519653379917145\n",
      "Epoch 92, train NLL 0.18871736526489258\n",
      "Epoch 93, train NLL 0.19792991876602173\n",
      "Epoch 94, train NLL 0.1859096884727478\n",
      "Epoch 95, train NLL 0.18444938957691193\n",
      "Epoch 96, train NLL 0.2936038374900818\n",
      "Epoch 97, train NLL 0.1883985698223114\n",
      "Epoch 98, train NLL 0.19452641904354095\n",
      "Epoch 99, train NLL 0.18879851698875427\n",
      "Epoch 100, train NLL 0.18823175132274628\n",
      "Epoch 101, train NLL 0.1860133558511734\n",
      "Epoch 102, train NLL 0.184955894947052\n",
      "Epoch 103, train NLL 0.19589298963546753\n",
      "Epoch 104, train NLL 0.1785244345664978\n",
      "Epoch 105, train NLL 0.1794901341199875\n",
      "Epoch 106, train NLL 0.18334037065505981\n",
      "Epoch 107, train NLL 0.1794460117816925\n",
      "Epoch 108, train NLL 0.17886123061180115\n",
      "Epoch 109, train NLL 0.1796283721923828\n",
      "Epoch 110, train NLL 0.17628218233585358\n",
      "Epoch 111, train NLL 0.17969505488872528\n",
      "Epoch 112, train NLL 0.20117363333702087\n",
      "Epoch 113, train NLL 0.17530155181884766\n",
      "Epoch 114, train NLL 0.17594289779663086\n",
      "Epoch 115, train NLL 0.1748616099357605\n",
      "Epoch 116, train NLL 0.1792544424533844\n",
      "Epoch 117, train NLL 0.18107476830482483\n",
      "Epoch 118, train NLL 0.1872270554304123\n",
      "Epoch 119, train NLL 0.17242133617401123\n",
      "Epoch 120, train NLL 0.1739545315504074\n",
      "Epoch 121, train NLL 0.17153428494930267\n",
      "Epoch 122, train NLL 0.17183339595794678\n",
      "Epoch 123, train NLL 0.1720171719789505\n",
      "Epoch 124, train NLL 0.17974157631397247\n",
      "Epoch 125, train NLL 0.19102083146572113\n",
      "Epoch 126, train NLL 0.18054097890853882\n",
      "Epoch 127, train NLL 0.17861776053905487\n",
      "Epoch 128, train NLL 0.16913659870624542\n",
      "Epoch 129, train NLL 0.1739560067653656\n",
      "Epoch 130, train NLL 0.17159461975097656\n",
      "Epoch 131, train NLL 0.16879838705062866\n",
      "Epoch 132, train NLL 0.16964216530323029\n",
      "Epoch 133, train NLL 0.17272338271141052\n",
      "Epoch 134, train NLL 0.16915041208267212\n",
      "Epoch 135, train NLL 0.17364105582237244\n",
      "Epoch 136, train NLL 0.20441438257694244\n",
      "Epoch 137, train NLL 0.16664770245552063\n",
      "Epoch 138, train NLL 0.16754847764968872\n",
      "Epoch 139, train NLL 0.16592271625995636\n",
      "Epoch 140, train NLL 0.16743579506874084\n",
      "Epoch 141, train NLL 0.18674150109291077\n",
      "Epoch 142, train NLL 0.16583143174648285\n",
      "Epoch 143, train NLL 0.19117394089698792\n",
      "Epoch 144, train NLL 0.17417651414871216\n",
      "Epoch 145, train NLL 0.17364679276943207\n",
      "Epoch 146, train NLL 0.16871678829193115\n",
      "Epoch 147, train NLL 0.16635777056217194\n",
      "Epoch 148, train NLL 0.1784692257642746\n",
      "Epoch 149, train NLL 0.166129931807518\n",
      "Epoch 150, train NLL 0.1632494330406189\n",
      "Epoch 151, train NLL 0.16266870498657227\n",
      "Epoch 152, train NLL 0.1639423966407776\n",
      "Epoch 153, train NLL 0.16243234276771545\n",
      "Epoch 154, train NLL 0.16768163442611694\n",
      "Epoch 155, train NLL 0.16694091260433197\n",
      "Epoch 156, train NLL 0.16150453686714172\n",
      "Epoch 157, train NLL 0.16647255420684814\n",
      "Epoch 158, train NLL 0.17978893220424652\n",
      "Epoch 159, train NLL 0.16346019506454468\n",
      "Epoch 160, train NLL 0.16191184520721436\n",
      "Epoch 161, train NLL 0.16152271628379822\n",
      "Epoch 162, train NLL 0.1627214401960373\n",
      "Epoch 163, train NLL 0.16541215777397156\n",
      "Epoch 164, train NLL 0.16654638946056366\n",
      "Epoch 165, train NLL 0.17642731964588165\n",
      "Epoch 166, train NLL 0.1689227819442749\n",
      "Epoch 167, train NLL 0.19581495225429535\n",
      "Epoch 168, train NLL 0.2570040225982666\n",
      "Epoch 169, train NLL 0.16063424944877625\n",
      "Epoch 170, train NLL 0.16617979109287262\n",
      "Epoch 171, train NLL 0.16332274675369263\n",
      "Epoch 172, train NLL 0.16109023988246918\n",
      "Epoch 173, train NLL 0.15980204939842224\n",
      "Epoch 174, train NLL 0.15762494504451752\n",
      "Epoch 175, train NLL 0.16897210478782654\n",
      "Epoch 176, train NLL 0.18079786002635956\n",
      "Epoch 177, train NLL 0.16726751625537872\n",
      "Epoch 178, train NLL 0.16633857786655426\n",
      "Epoch 179, train NLL 0.1619562953710556\n",
      "Epoch 180, train NLL 0.15650144219398499\n",
      "Epoch 181, train NLL 0.18166078627109528\n",
      "Epoch 182, train NLL 0.16505485773086548\n",
      "Epoch 183, train NLL 0.16391488909721375\n",
      "Epoch 184, train NLL 0.1563694328069687\n",
      "Epoch 185, train NLL 0.16040915250778198\n",
      "Epoch 186, train NLL 0.15554705262184143\n",
      "Epoch 187, train NLL 0.1554422527551651\n",
      "Epoch 188, train NLL 0.17071887850761414\n",
      "Epoch 189, train NLL 0.165578693151474\n",
      "Epoch 190, train NLL 0.17453324794769287\n",
      "Epoch 191, train NLL 0.15844495594501495\n",
      "Epoch 192, train NLL 0.17386314272880554\n",
      "Epoch 193, train NLL 0.1596575379371643\n",
      "Epoch 194, train NLL 0.1545742005109787\n",
      "Epoch 195, train NLL 0.17078697681427002\n",
      "Epoch 196, train NLL 0.1572847217321396\n",
      "Epoch 197, train NLL 0.15430240333080292\n",
      "Epoch 198, train NLL 0.1536901891231537\n",
      "Epoch 199, train NLL 0.15800078213214874\n",
      "Epoch 200, train NLL 0.15348516404628754\n",
      "Epoch 201, train NLL 0.1532420516014099\n",
      "Epoch 202, train NLL 0.1629810333251953\n",
      "Epoch 203, train NLL 0.15345823764801025\n",
      "Epoch 204, train NLL 0.15287059545516968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205, train NLL 0.1590842455625534\n",
      "Epoch 206, train NLL 0.18633180856704712\n",
      "Epoch 207, train NLL 0.18721412122249603\n",
      "Epoch 208, train NLL 0.1646568924188614\n",
      "Epoch 209, train NLL 0.15484680235385895\n",
      "Epoch 210, train NLL 0.16276364028453827\n",
      "Epoch 211, train NLL 0.15241259336471558\n",
      "Epoch 212, train NLL 0.15300457179546356\n",
      "Epoch 213, train NLL 0.1515686959028244\n",
      "Epoch 214, train NLL 0.16509757936000824\n",
      "Epoch 215, train NLL 0.16148896515369415\n",
      "Epoch 216, train NLL 0.1519506573677063\n",
      "Epoch 217, train NLL 0.15625137090682983\n",
      "Epoch 218, train NLL 0.15091173350811005\n",
      "Epoch 219, train NLL 0.15137307345867157\n",
      "Epoch 220, train NLL 0.15132762491703033\n",
      "Epoch 221, train NLL 0.1574711799621582\n",
      "Epoch 222, train NLL 0.15231293439865112\n",
      "Epoch 223, train NLL 0.16328200697898865\n",
      "Epoch 224, train NLL 0.1506117284297943\n",
      "Epoch 225, train NLL 0.15034520626068115\n",
      "Epoch 226, train NLL 0.15724167227745056\n",
      "Epoch 227, train NLL 0.14984656870365143\n",
      "Epoch 228, train NLL 0.14972393214702606\n",
      "Epoch 229, train NLL 0.14963938295841217\n",
      "Epoch 230, train NLL 0.1496841311454773\n",
      "Epoch 231, train NLL 0.15892404317855835\n",
      "Epoch 232, train NLL 0.15416841208934784\n",
      "Epoch 233, train NLL 0.1639273464679718\n",
      "Epoch 234, train NLL 0.15022531151771545\n",
      "Epoch 235, train NLL 0.15715590119361877\n",
      "Epoch 236, train NLL 0.14886684715747833\n",
      "Epoch 237, train NLL 0.15455389022827148\n",
      "Epoch 238, train NLL 0.15650565922260284\n",
      "Epoch 239, train NLL 0.14857117831707\n",
      "Epoch 240, train NLL 0.15376879274845123\n",
      "Epoch 241, train NLL 0.14836987853050232\n",
      "Epoch 242, train NLL 0.17311973869800568\n",
      "Epoch 243, train NLL 0.1512061506509781\n",
      "Epoch 244, train NLL 0.1647668331861496\n",
      "Epoch 245, train NLL 0.14867323637008667\n",
      "Epoch 246, train NLL 0.15845388174057007\n",
      "Epoch 247, train NLL 0.15867967903614044\n",
      "Epoch 248, train NLL 0.1567123830318451\n",
      "Epoch 249, train NLL 0.15076252818107605\n",
      "Epoch 250, train NLL 0.1591021567583084\n",
      "Epoch 251, train NLL 0.18393228948116302\n",
      "Epoch 252, train NLL 0.14773404598236084\n",
      "Epoch 253, train NLL 0.14970465004444122\n",
      "Epoch 254, train NLL 0.14928755164146423\n",
      "Epoch 255, train NLL 0.15853525698184967\n",
      "Epoch 256, train NLL 0.14730459451675415\n",
      "Epoch 257, train NLL 0.14745116233825684\n",
      "Epoch 258, train NLL 0.15620841085910797\n",
      "Epoch 259, train NLL 0.14722852408885956\n",
      "Epoch 260, train NLL 0.157900869846344\n",
      "Epoch 261, train NLL 0.15652106702327728\n",
      "Epoch 262, train NLL 0.1479092240333557\n",
      "Epoch 263, train NLL 0.16955448687076569\n",
      "Epoch 264, train NLL 0.14687535166740417\n",
      "Epoch 265, train NLL 0.17688623070716858\n",
      "Epoch 266, train NLL 0.14778445661067963\n",
      "Epoch 267, train NLL 0.15532808005809784\n",
      "Epoch 268, train NLL 0.16581465303897858\n",
      "Epoch 269, train NLL 0.14698205888271332\n",
      "Epoch 270, train NLL 0.1472371220588684\n",
      "Epoch 271, train NLL 0.16378702223300934\n",
      "Epoch 272, train NLL 0.15856127440929413\n",
      "Epoch 273, train NLL 0.14997650682926178\n",
      "Epoch 274, train NLL 0.14539067447185516\n",
      "Epoch 275, train NLL 0.1465667188167572\n",
      "Epoch 276, train NLL 0.1526610404253006\n",
      "Epoch 277, train NLL 0.1586940437555313\n",
      "Epoch 278, train NLL 0.14568868279457092\n",
      "Epoch 279, train NLL 0.14510513842105865\n",
      "Epoch 280, train NLL 0.14534899592399597\n",
      "Epoch 281, train NLL 0.14842970669269562\n",
      "Epoch 282, train NLL 0.15381760895252228\n",
      "Epoch 283, train NLL 0.14717285335063934\n",
      "Epoch 284, train NLL 0.14470785856246948\n",
      "Epoch 285, train NLL 0.15779359638690948\n",
      "Epoch 286, train NLL 0.15016666054725647\n",
      "Epoch 287, train NLL 0.14445632696151733\n",
      "Epoch 288, train NLL 0.15412446856498718\n",
      "Epoch 289, train NLL 0.1447286456823349\n",
      "Epoch 290, train NLL 0.1445588618516922\n",
      "Epoch 291, train NLL 0.14418835937976837\n",
      "Epoch 292, train NLL 0.14902283251285553\n",
      "Epoch 293, train NLL 0.18988855183124542\n",
      "Epoch 294, train NLL 0.15407724678516388\n",
      "Epoch 295, train NLL 0.15029534697532654\n",
      "Epoch 296, train NLL 0.14555047452449799\n",
      "Epoch 297, train NLL 0.15294213593006134\n",
      "Epoch 298, train NLL 0.14728672802448273\n",
      "Epoch 299, train NLL 0.14456157386302948\n",
      "Epoch 300, train NLL 0.15045441687107086\n",
      "Epoch 301, train NLL 0.14705616235733032\n",
      "Epoch 302, train NLL 0.15665008127689362\n",
      "Epoch 303, train NLL 0.14365337789058685\n",
      "Epoch 304, train NLL 0.14940756559371948\n",
      "Epoch 305, train NLL 0.15161897242069244\n",
      "Epoch 306, train NLL 0.16582277417182922\n",
      "Epoch 307, train NLL 0.1479550302028656\n",
      "Epoch 308, train NLL 0.1463186889886856\n",
      "Epoch 309, train NLL 0.1488507241010666\n",
      "Epoch 310, train NLL 0.14502692222595215\n",
      "Epoch 311, train NLL 0.1432015746831894\n",
      "Epoch 312, train NLL 0.1867367923259735\n",
      "Epoch 313, train NLL 0.15448981523513794\n",
      "Epoch 314, train NLL 0.1473766267299652\n",
      "Epoch 315, train NLL 0.14291226863861084\n",
      "Epoch 316, train NLL 0.15749883651733398\n",
      "Epoch 317, train NLL 0.14886169135570526\n",
      "Epoch 318, train NLL 0.14342020452022552\n",
      "Epoch 319, train NLL 0.14973896741867065\n",
      "Epoch 320, train NLL 0.15719082951545715\n",
      "Epoch 321, train NLL 0.14329953491687775\n",
      "Epoch 322, train NLL 0.14237023890018463\n",
      "Epoch 323, train NLL 0.14697663486003876\n",
      "Epoch 324, train NLL 0.14552700519561768\n",
      "Epoch 325, train NLL 0.1431635022163391\n",
      "Epoch 326, train NLL 0.14895448088645935\n",
      "Epoch 327, train NLL 0.14193451404571533\n",
      "Epoch 328, train NLL 0.14185050129890442\n",
      "Epoch 329, train NLL 0.146844744682312\n",
      "Epoch 330, train NLL 0.1418333351612091\n",
      "Epoch 331, train NLL 0.1470569521188736\n",
      "Epoch 332, train NLL 0.14229512214660645\n",
      "Epoch 333, train NLL 0.1513698846101761\n",
      "Epoch 334, train NLL 0.181928813457489\n",
      "Epoch 335, train NLL 0.14910274744033813\n",
      "Epoch 336, train NLL 0.1418086439371109\n",
      "Epoch 337, train NLL 0.14316721260547638\n",
      "Epoch 338, train NLL 0.14948756992816925\n",
      "Epoch 339, train NLL 0.1443968117237091\n",
      "Epoch 340, train NLL 0.15662971138954163\n",
      "Epoch 341, train NLL 0.14115923643112183\n",
      "Epoch 342, train NLL 0.1457729935646057\n",
      "Epoch 343, train NLL 0.17620840668678284\n",
      "Epoch 344, train NLL 0.141055628657341\n",
      "Epoch 345, train NLL 0.14107659459114075\n",
      "Epoch 346, train NLL 0.14137983322143555\n",
      "Epoch 347, train NLL 0.16762004792690277\n",
      "Epoch 348, train NLL 0.14268462359905243\n",
      "Epoch 349, train NLL 0.14105331897735596\n",
      "Epoch 350, train NLL 0.14587798714637756\n",
      "Epoch 351, train NLL 0.14287284016609192\n",
      "Epoch 352, train NLL 0.1424284130334854\n",
      "Epoch 353, train NLL 0.15240760147571564\n",
      "Epoch 354, train NLL 0.14765727519989014\n",
      "Epoch 355, train NLL 0.142545685172081\n",
      "Epoch 356, train NLL 0.1408117711544037\n",
      "Epoch 357, train NLL 0.1453731507062912\n",
      "Epoch 358, train NLL 0.14083531498908997\n",
      "Epoch 359, train NLL 0.15495431423187256\n",
      "Epoch 360, train NLL 0.1403295397758484\n",
      "Epoch 361, train NLL 0.14059031009674072\n",
      "Epoch 362, train NLL 0.14619846642017365\n",
      "Epoch 363, train NLL 0.1472715437412262\n",
      "Epoch 364, train NLL 0.14165352284908295\n",
      "Epoch 365, train NLL 0.14684545993804932\n",
      "Epoch 366, train NLL 0.13998478651046753\n",
      "Epoch 367, train NLL 0.14540988206863403\n",
      "Epoch 368, train NLL 0.140699103474617\n",
      "Epoch 369, train NLL 0.15939143300056458\n",
      "Epoch 370, train NLL 0.1454160064458847\n",
      "Epoch 371, train NLL 0.159879669547081\n",
      "Epoch 372, train NLL 0.14566963911056519\n",
      "Epoch 373, train NLL 0.13983826339244843\n",
      "Epoch 374, train NLL 0.1398358941078186\n",
      "Epoch 375, train NLL 0.1398359090089798\n",
      "Epoch 376, train NLL 0.1443185657262802\n",
      "Epoch 377, train NLL 0.13954228162765503\n",
      "Epoch 378, train NLL 0.139905646443367\n",
      "Epoch 379, train NLL 0.14346246421337128\n",
      "Epoch 380, train NLL 0.1493263840675354\n",
      "Epoch 381, train NLL 0.14428573846817017\n",
      "Epoch 382, train NLL 0.14309614896774292\n",
      "Epoch 383, train NLL 0.14894327521324158\n",
      "Epoch 384, train NLL 0.15299829840660095\n",
      "Epoch 385, train NLL 0.16723927855491638\n",
      "Epoch 386, train NLL 0.144594207406044\n",
      "Epoch 387, train NLL 0.14031197130680084\n",
      "Epoch 388, train NLL 0.13949038088321686\n",
      "Epoch 389, train NLL 0.14345881342887878\n",
      "Epoch 390, train NLL 0.1538548320531845\n",
      "Epoch 391, train NLL 0.13945403695106506\n",
      "Epoch 392, train NLL 0.13903068006038666\n",
      "Epoch 393, train NLL 0.13919003307819366\n",
      "Epoch 394, train NLL 0.14374969899654388\n",
      "Epoch 395, train NLL 0.14100150763988495\n",
      "Epoch 396, train NLL 0.13886140286922455\n",
      "Epoch 397, train NLL 0.1402813196182251\n",
      "Epoch 398, train NLL 0.1396946758031845\n",
      "Epoch 399, train NLL 0.1514630913734436\n",
      "Epoch 400, train NLL 0.1451580971479416\n",
      "Epoch 401, train NLL 0.1441916823387146\n",
      "Epoch 402, train NLL 0.1493956446647644\n",
      "Epoch 403, train NLL 0.13887318968772888\n",
      "Epoch 404, train NLL 0.15002015233039856\n",
      "Epoch 405, train NLL 0.1453702449798584\n",
      "Epoch 406, train NLL 0.14508359134197235\n",
      "Epoch 407, train NLL 0.1410275399684906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408, train NLL 0.16923058032989502\n",
      "Epoch 409, train NLL 0.13924555480480194\n",
      "Epoch 410, train NLL 0.1400509476661682\n",
      "Epoch 411, train NLL 0.14456646144390106\n",
      "Epoch 412, train NLL 0.13939201831817627\n",
      "Epoch 413, train NLL 0.14510685205459595\n",
      "Epoch 414, train NLL 0.15961337089538574\n",
      "Epoch 415, train NLL 0.1459842026233673\n",
      "Epoch 416, train NLL 0.17169107496738434\n",
      "Epoch 417, train NLL 0.14021776616573334\n",
      "Epoch 418, train NLL 0.1401820331811905\n",
      "Epoch 419, train NLL 0.13852208852767944\n",
      "Epoch 420, train NLL 0.150923952460289\n",
      "Epoch 421, train NLL 0.15743140876293182\n",
      "Epoch 422, train NLL 0.14411446452140808\n",
      "Epoch 423, train NLL 0.13942688703536987\n",
      "Epoch 424, train NLL 0.1423778086900711\n",
      "Epoch 425, train NLL 0.1515454798936844\n",
      "Epoch 426, train NLL 0.14561130106449127\n",
      "Epoch 427, train NLL 0.13775163888931274\n",
      "Epoch 428, train NLL 0.14315997064113617\n",
      "Epoch 429, train NLL 0.1440708488225937\n",
      "Epoch 430, train NLL 0.15946729481220245\n",
      "Epoch 431, train NLL 0.1529712826013565\n",
      "Epoch 432, train NLL 0.13900813460350037\n",
      "Epoch 433, train NLL 0.1376572549343109\n",
      "Epoch 434, train NLL 0.1935531497001648\n",
      "Epoch 435, train NLL 0.13810311257839203\n",
      "Epoch 436, train NLL 0.1376216560602188\n",
      "Epoch 437, train NLL 0.15407414734363556\n",
      "Epoch 438, train NLL 0.1397857815027237\n",
      "Epoch 439, train NLL 0.14043937623500824\n",
      "Epoch 440, train NLL 0.148426353931427\n",
      "Epoch 441, train NLL 0.14635328948497772\n",
      "Epoch 442, train NLL 0.1382765918970108\n",
      "Epoch 443, train NLL 0.14341308176517487\n",
      "Epoch 444, train NLL 0.13741426169872284\n",
      "Epoch 445, train NLL 0.16956105828285217\n",
      "Epoch 446, train NLL 0.1373118758201599\n",
      "Epoch 447, train NLL 0.1410684883594513\n",
      "Epoch 448, train NLL 0.147114560008049\n",
      "Epoch 449, train NLL 0.13865943253040314\n",
      "Epoch 450, train NLL 0.14726771414279938\n",
      "Epoch 451, train NLL 0.13865211606025696\n",
      "Epoch 452, train NLL 0.13981732726097107\n",
      "Epoch 453, train NLL 0.13842922449111938\n",
      "Epoch 454, train NLL 0.13941225409507751\n",
      "Epoch 455, train NLL 0.13712097704410553\n",
      "Epoch 456, train NLL 0.13922491669654846\n",
      "Epoch 457, train NLL 0.13883619010448456\n",
      "Epoch 458, train NLL 0.13693800568580627\n",
      "Epoch 459, train NLL 0.13699832558631897\n",
      "Epoch 460, train NLL 0.14650049805641174\n",
      "Epoch 461, train NLL 0.15392865240573883\n",
      "Epoch 462, train NLL 0.14091162383556366\n",
      "Epoch 463, train NLL 0.13748474419116974\n",
      "Epoch 464, train NLL 0.13774536550045013\n",
      "Epoch 465, train NLL 0.13930268585681915\n",
      "Epoch 466, train NLL 0.14034977555274963\n",
      "Epoch 467, train NLL 0.13664814829826355\n",
      "Epoch 468, train NLL 0.15823127329349518\n",
      "Epoch 469, train NLL 0.1372230052947998\n",
      "Epoch 470, train NLL 0.14976441860198975\n",
      "Epoch 471, train NLL 0.13759079575538635\n",
      "Epoch 472, train NLL 0.1385035663843155\n",
      "Epoch 473, train NLL 0.1371629685163498\n",
      "Epoch 474, train NLL 0.13673029839992523\n",
      "Epoch 475, train NLL 0.1655530482530594\n",
      "Epoch 476, train NLL 0.13665953278541565\n",
      "Epoch 477, train NLL 0.13689875602722168\n",
      "Epoch 478, train NLL 0.13645190000534058\n",
      "Epoch 479, train NLL 0.13806217908859253\n",
      "Epoch 480, train NLL 0.1480497121810913\n",
      "Epoch 481, train NLL 0.1412646621465683\n",
      "Epoch 482, train NLL 0.1372445523738861\n",
      "Epoch 483, train NLL 0.13699200749397278\n",
      "Epoch 484, train NLL 0.13649173080921173\n",
      "Epoch 485, train NLL 0.14168408513069153\n",
      "Epoch 486, train NLL 0.14171138405799866\n",
      "Epoch 487, train NLL 0.14253409206867218\n",
      "Epoch 488, train NLL 0.13889461755752563\n",
      "Epoch 489, train NLL 0.14590921998023987\n",
      "Epoch 490, train NLL 0.14614681899547577\n",
      "Epoch 491, train NLL 0.1375826895236969\n",
      "Epoch 492, train NLL 0.13723218441009521\n",
      "Epoch 493, train NLL 0.1376865655183792\n",
      "Epoch 494, train NLL 0.13640116155147552\n",
      "Epoch 495, train NLL 0.13708661496639252\n",
      "Epoch 496, train NLL 0.1403483897447586\n",
      "Epoch 497, train NLL 0.16241176426410675\n",
      "Epoch 498, train NLL 0.14643701910972595\n",
      "Epoch 499, train NLL 0.13593454658985138\n",
      "Epoch 500, train NLL 0.1374945342540741\n",
      "Epoch 501, train NLL 0.13591738045215607\n",
      "Epoch 502, train NLL 0.1368248015642166\n",
      "Epoch 503, train NLL 0.13588301837444305\n",
      "Epoch 504, train NLL 0.14035063982009888\n",
      "Epoch 505, train NLL 0.1378990113735199\n",
      "Epoch 506, train NLL 0.13605941832065582\n",
      "Epoch 507, train NLL 0.1358678638935089\n",
      "Epoch 508, train NLL 0.136726975440979\n",
      "Epoch 509, train NLL 0.1370011270046234\n",
      "Epoch 510, train NLL 0.1358920931816101\n",
      "Epoch 511, train NLL 0.13575351238250732\n",
      "Epoch 512, train NLL 0.1437942534685135\n",
      "Epoch 513, train NLL 0.13939358294010162\n",
      "Epoch 514, train NLL 0.13989375531673431\n",
      "Epoch 515, train NLL 0.13774052262306213\n",
      "Epoch 516, train NLL 0.13644738495349884\n",
      "Epoch 517, train NLL 0.139950692653656\n",
      "Epoch 518, train NLL 0.13698641955852509\n",
      "Epoch 519, train NLL 0.13568097352981567\n",
      "Epoch 520, train NLL 0.15757587552070618\n",
      "Epoch 521, train NLL 0.1357613354921341\n",
      "Epoch 522, train NLL 0.13560542464256287\n",
      "Epoch 523, train NLL 0.13557395339012146\n",
      "Epoch 524, train NLL 0.13700498640537262\n",
      "Epoch 525, train NLL 0.13720445334911346\n",
      "Epoch 526, train NLL 0.13688121736049652\n",
      "Epoch 527, train NLL 0.135980024933815\n",
      "Epoch 528, train NLL 0.13547265529632568\n",
      "Epoch 529, train NLL 0.13962195813655853\n",
      "Epoch 530, train NLL 0.2184477001428604\n",
      "Epoch 531, train NLL 0.13959906995296478\n",
      "Epoch 532, train NLL 0.144973024725914\n",
      "Epoch 533, train NLL 0.17918699979782104\n",
      "Epoch 534, train NLL 0.13541512191295624\n",
      "Epoch 535, train NLL 0.13768631219863892\n",
      "Epoch 536, train NLL 0.13729968667030334\n",
      "Epoch 537, train NLL 0.13544894754886627\n",
      "Epoch 538, train NLL 0.15148526430130005\n",
      "Epoch 539, train NLL 0.13648003339767456\n",
      "Epoch 540, train NLL 0.14484012126922607\n",
      "Epoch 541, train NLL 0.13511228561401367\n",
      "Epoch 542, train NLL 0.1561087667942047\n",
      "Epoch 543, train NLL 0.14965739846229553\n",
      "Epoch 544, train NLL 0.14073030650615692\n",
      "Epoch 545, train NLL 0.14003701508045197\n",
      "Epoch 546, train NLL 0.13543453812599182\n",
      "Epoch 547, train NLL 0.13503479957580566\n",
      "Epoch 548, train NLL 0.14232178032398224\n",
      "Epoch 549, train NLL 0.13502340018749237\n",
      "Epoch 550, train NLL 0.13754482567310333\n",
      "Epoch 551, train NLL 0.13506552577018738\n",
      "Epoch 552, train NLL 0.13492217659950256\n",
      "Epoch 553, train NLL 0.14114119112491608\n",
      "Epoch 554, train NLL 0.14202040433883667\n",
      "Epoch 555, train NLL 0.1359807550907135\n",
      "Epoch 556, train NLL 0.1349879950284958\n",
      "Epoch 557, train NLL 0.13654722273349762\n",
      "Epoch 558, train NLL 0.14552374184131622\n",
      "Epoch 559, train NLL 0.13655807077884674\n",
      "Epoch 560, train NLL 0.14021696150302887\n",
      "Epoch 561, train NLL 0.13724148273468018\n",
      "Epoch 562, train NLL 0.13488206267356873\n",
      "Epoch 563, train NLL 0.1405438482761383\n",
      "Epoch 564, train NLL 0.14940808713436127\n",
      "Epoch 565, train NLL 0.14736051857471466\n",
      "Epoch 566, train NLL 0.1386735588312149\n",
      "Epoch 567, train NLL 0.13471084833145142\n",
      "Epoch 568, train NLL 0.1396171897649765\n",
      "Epoch 569, train NLL 0.1433563381433487\n",
      "Epoch 570, train NLL 0.13748246431350708\n",
      "Epoch 571, train NLL 0.1378825157880783\n",
      "Epoch 572, train NLL 0.14777344465255737\n",
      "Epoch 573, train NLL 0.1429247260093689\n",
      "Epoch 574, train NLL 0.13458094000816345\n",
      "Epoch 575, train NLL 0.13474604487419128\n",
      "Epoch 576, train NLL 0.13790880143642426\n",
      "Epoch 577, train NLL 0.13543300330638885\n",
      "Epoch 578, train NLL 0.14867913722991943\n",
      "Epoch 579, train NLL 0.1421191543340683\n",
      "Epoch 580, train NLL 0.15187132358551025\n",
      "Epoch 581, train NLL 0.15160278975963593\n",
      "Epoch 582, train NLL 0.18877750635147095\n",
      "Epoch 583, train NLL 0.13465827703475952\n",
      "Epoch 584, train NLL 0.15612730383872986\n",
      "Epoch 585, train NLL 0.13500051200389862\n",
      "Epoch 586, train NLL 0.13971354067325592\n",
      "Epoch 587, train NLL 0.1352730691432953\n",
      "Epoch 588, train NLL 0.14314189553260803\n",
      "Epoch 589, train NLL 0.1497146189212799\n",
      "Epoch 590, train NLL 0.13453011214733124\n",
      "Epoch 591, train NLL 0.13446877896785736\n",
      "Epoch 592, train NLL 0.1343124806880951\n",
      "Epoch 593, train NLL 0.13937291502952576\n",
      "Epoch 594, train NLL 0.13900931179523468\n",
      "Epoch 595, train NLL 0.13435788452625275\n",
      "Epoch 596, train NLL 0.14321491122245789\n",
      "Epoch 597, train NLL 0.1342458724975586\n",
      "Epoch 598, train NLL 0.13445362448692322\n",
      "Epoch 599, train NLL 0.13433192670345306\n",
      "Epoch 600, train NLL 0.13447850942611694\n",
      "Epoch 601, train NLL 0.13640163838863373\n",
      "Epoch 602, train NLL 0.16809871792793274\n",
      "Epoch 603, train NLL 0.14468470215797424\n",
      "Epoch 604, train NLL 0.13581493496894836\n",
      "Epoch 605, train NLL 0.14674142003059387\n",
      "Epoch 606, train NLL 0.13465577363967896\n",
      "Epoch 607, train NLL 0.137776181101799\n",
      "Epoch 608, train NLL 0.13696859776973724\n",
      "Epoch 609, train NLL 0.13675624132156372\n",
      "Epoch 610, train NLL 0.1412438452243805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611, train NLL 0.1469709724187851\n",
      "Epoch 612, train NLL 0.1351913958787918\n",
      "Epoch 613, train NLL 0.14272460341453552\n",
      "Epoch 614, train NLL 0.1345098912715912\n",
      "Epoch 615, train NLL 0.1357787847518921\n",
      "Epoch 616, train NLL 0.13638199865818024\n",
      "Epoch 617, train NLL 0.1339971125125885\n",
      "Epoch 618, train NLL 0.13429498672485352\n",
      "Epoch 619, train NLL 0.13546636700630188\n",
      "Epoch 620, train NLL 0.1339806318283081\n",
      "Epoch 621, train NLL 0.1349126100540161\n",
      "Epoch 622, train NLL 0.13462679088115692\n",
      "Epoch 623, train NLL 0.1389562040567398\n",
      "Epoch 624, train NLL 0.1733931005001068\n",
      "Epoch 625, train NLL 0.13604310154914856\n",
      "Epoch 626, train NLL 0.15720678865909576\n",
      "Epoch 627, train NLL 0.13395866751670837\n",
      "Epoch 628, train NLL 0.1348501741886139\n",
      "Epoch 629, train NLL 0.13776586949825287\n",
      "Epoch 630, train NLL 0.1355365514755249\n",
      "Epoch 631, train NLL 0.13993515074253082\n",
      "Epoch 632, train NLL 0.14055489003658295\n",
      "Epoch 633, train NLL 0.14677004516124725\n",
      "Epoch 634, train NLL 0.13414545357227325\n",
      "Epoch 635, train NLL 0.14027351140975952\n",
      "Epoch 636, train NLL 0.13380469381809235\n",
      "Epoch 637, train NLL 0.13404354453086853\n",
      "Epoch 638, train NLL 0.13626892864704132\n",
      "Epoch 639, train NLL 0.1418302208185196\n",
      "Epoch 640, train NLL 0.14832496643066406\n",
      "Epoch 641, train NLL 0.14518991112709045\n",
      "Epoch 642, train NLL 0.13870735466480255\n",
      "Epoch 643, train NLL 0.15022005140781403\n",
      "Epoch 644, train NLL 0.13438841700553894\n",
      "Epoch 645, train NLL 0.13380467891693115\n",
      "Epoch 646, train NLL 0.13368752598762512\n",
      "Epoch 647, train NLL 0.13564233481884003\n",
      "Epoch 648, train NLL 0.13365420699119568\n",
      "Epoch 649, train NLL 0.13561278581619263\n",
      "Epoch 650, train NLL 0.133926659822464\n",
      "Epoch 651, train NLL 0.13632696866989136\n",
      "Epoch 652, train NLL 0.13803483545780182\n",
      "Epoch 653, train NLL 0.13428634405136108\n",
      "Epoch 654, train NLL 0.13395501673221588\n",
      "Epoch 655, train NLL 0.1337660849094391\n",
      "Epoch 656, train NLL 0.15029992163181305\n",
      "Epoch 657, train NLL 0.13832606375217438\n",
      "Epoch 658, train NLL 0.1349363774061203\n",
      "Epoch 659, train NLL 0.142250195145607\n",
      "Epoch 660, train NLL 0.13656479120254517\n",
      "Epoch 661, train NLL 0.1418115198612213\n",
      "Epoch 662, train NLL 0.140089750289917\n",
      "Epoch 663, train NLL 0.13891923427581787\n",
      "Epoch 664, train NLL 0.1414913535118103\n",
      "Epoch 665, train NLL 0.14838773012161255\n",
      "Epoch 666, train NLL 0.13636957108974457\n",
      "Epoch 667, train NLL 0.15609750151634216\n",
      "Epoch 668, train NLL 0.14806020259857178\n",
      "Epoch 669, train NLL 0.13444173336029053\n",
      "Epoch 670, train NLL 0.1344297230243683\n",
      "Epoch 671, train NLL 0.14278994500637054\n",
      "Epoch 672, train NLL 0.1362718790769577\n",
      "Epoch 673, train NLL 0.13397948443889618\n",
      "Epoch 674, train NLL 0.133759006857872\n",
      "Epoch 675, train NLL 0.13460862636566162\n",
      "Epoch 676, train NLL 0.14757677912712097\n",
      "Epoch 677, train NLL 0.1336074322462082\n",
      "Epoch 678, train NLL 0.14349038898944855\n",
      "Epoch 679, train NLL 0.13337667286396027\n",
      "Epoch 680, train NLL 0.13336805999279022\n",
      "Epoch 681, train NLL 0.1343861073255539\n",
      "Epoch 682, train NLL 0.13674652576446533\n",
      "Epoch 683, train NLL 0.13974928855895996\n",
      "Epoch 684, train NLL 0.15434600412845612\n",
      "Epoch 685, train NLL 0.13458001613616943\n",
      "Epoch 686, train NLL 0.1395190805196762\n",
      "Epoch 687, train NLL 0.1339205652475357\n",
      "Epoch 688, train NLL 0.1334492415189743\n",
      "Epoch 689, train NLL 0.13333170115947723\n",
      "Epoch 690, train NLL 0.1332986056804657\n",
      "Epoch 691, train NLL 0.13426493108272552\n",
      "Epoch 692, train NLL 0.1356549710035324\n",
      "Epoch 693, train NLL 0.13678404688835144\n",
      "Epoch 694, train NLL 0.13407394289970398\n",
      "Epoch 695, train NLL 0.13317938148975372\n",
      "Epoch 696, train NLL 0.13418839871883392\n",
      "Epoch 697, train NLL 0.1332128345966339\n",
      "Epoch 698, train NLL 0.13489669561386108\n",
      "Epoch 699, train NLL 0.13313958048820496\n",
      "Epoch 700, train NLL 0.13957904279232025\n",
      "Epoch 701, train NLL 0.14292117953300476\n",
      "Epoch 702, train NLL 0.13370190560817719\n",
      "Epoch 703, train NLL 0.1405612826347351\n",
      "Epoch 704, train NLL 0.13371865451335907\n",
      "Epoch 705, train NLL 0.13598482310771942\n",
      "Epoch 706, train NLL 0.13348856568336487\n",
      "Epoch 707, train NLL 0.13510017096996307\n",
      "Epoch 708, train NLL 0.13483992218971252\n",
      "Epoch 709, train NLL 0.13414490222930908\n",
      "Epoch 710, train NLL 0.14410704374313354\n",
      "Epoch 711, train NLL 0.13315778970718384\n",
      "Epoch 712, train NLL 0.13415895402431488\n",
      "Epoch 713, train NLL 0.14122945070266724\n",
      "Epoch 714, train NLL 0.14014019072055817\n",
      "Epoch 715, train NLL 0.13832233846187592\n",
      "Epoch 716, train NLL 0.13551363348960876\n",
      "Epoch 717, train NLL 0.13316060602664948\n",
      "Epoch 718, train NLL 0.13772247731685638\n",
      "Epoch 719, train NLL 0.13453085720539093\n",
      "Epoch 720, train NLL 0.1340484619140625\n",
      "Epoch 721, train NLL 0.1361369639635086\n",
      "Epoch 722, train NLL 0.13340520858764648\n",
      "Epoch 723, train NLL 0.13350379467010498\n",
      "Epoch 724, train NLL 0.1329306960105896\n",
      "Epoch 725, train NLL 0.1372012346982956\n",
      "Epoch 726, train NLL 0.1379227638244629\n",
      "Epoch 727, train NLL 0.14790648221969604\n",
      "Epoch 728, train NLL 0.14746643602848053\n",
      "Epoch 729, train NLL 0.1348172426223755\n",
      "Epoch 730, train NLL 0.1329152137041092\n",
      "Epoch 731, train NLL 0.1349298059940338\n",
      "Epoch 732, train NLL 0.13730774819850922\n",
      "Epoch 733, train NLL 0.13761144876480103\n",
      "Epoch 734, train NLL 0.13383272290229797\n",
      "Epoch 735, train NLL 0.15117517113685608\n",
      "Epoch 736, train NLL 0.1349881887435913\n",
      "Epoch 737, train NLL 0.13483260571956635\n",
      "Epoch 738, train NLL 0.14110909402370453\n",
      "Epoch 739, train NLL 0.15085893869400024\n",
      "Epoch 740, train NLL 0.13673801720142365\n",
      "Epoch 741, train NLL 0.13373038172721863\n",
      "Epoch 742, train NLL 0.15121889114379883\n",
      "Epoch 743, train NLL 0.13291755318641663\n",
      "Epoch 744, train NLL 0.1363559514284134\n",
      "Epoch 745, train NLL 0.13552454113960266\n",
      "Epoch 746, train NLL 0.1359698325395584\n",
      "Epoch 747, train NLL 0.13273116946220398\n",
      "Epoch 748, train NLL 0.13365888595581055\n",
      "Epoch 749, train NLL 0.13628675043582916\n",
      "Epoch 750, train NLL 0.1356096714735031\n",
      "Epoch 751, train NLL 0.13904260098934174\n",
      "Epoch 752, train NLL 0.13568933308124542\n",
      "Epoch 753, train NLL 0.1353912502527237\n",
      "Epoch 754, train NLL 0.13387373089790344\n",
      "Epoch 755, train NLL 0.14012576639652252\n",
      "Epoch 756, train NLL 0.1368296593427658\n",
      "Epoch 757, train NLL 0.13359247148036957\n",
      "Epoch 758, train NLL 0.15644502639770508\n",
      "Epoch 759, train NLL 0.13820503652095795\n",
      "Epoch 760, train NLL 0.13263626396656036\n",
      "Epoch 761, train NLL 0.13573934137821198\n",
      "Epoch 762, train NLL 0.1326739341020584\n",
      "Epoch 763, train NLL 0.1345566213130951\n",
      "Epoch 764, train NLL 0.13553158938884735\n",
      "Epoch 765, train NLL 0.14338389039039612\n",
      "Epoch 766, train NLL 0.132650688290596\n",
      "Epoch 767, train NLL 0.13474445044994354\n",
      "Epoch 768, train NLL 0.13416004180908203\n",
      "Epoch 769, train NLL 0.1329902708530426\n",
      "Epoch 770, train NLL 0.13369104266166687\n",
      "Epoch 771, train NLL 0.13930580019950867\n",
      "Epoch 772, train NLL 0.13262537121772766\n",
      "Epoch 773, train NLL 0.13678409159183502\n",
      "Epoch 774, train NLL 0.13255104422569275\n",
      "Epoch 775, train NLL 0.1375211626291275\n",
      "Epoch 776, train NLL 0.13953204452991486\n",
      "Epoch 777, train NLL 0.13316667079925537\n",
      "Epoch 778, train NLL 0.13321048021316528\n",
      "Epoch 779, train NLL 0.1341007500886917\n",
      "Epoch 780, train NLL 0.1332450807094574\n",
      "Epoch 781, train NLL 0.13725721836090088\n",
      "Epoch 782, train NLL 0.13318976759910583\n",
      "Epoch 783, train NLL 0.1336926519870758\n",
      "Epoch 784, train NLL 0.13495881855487823\n",
      "Epoch 785, train NLL 0.13278624415397644\n",
      "Epoch 786, train NLL 0.13575288653373718\n",
      "Epoch 787, train NLL 0.13309821486473083\n",
      "Epoch 788, train NLL 0.1338401436805725\n",
      "Epoch 789, train NLL 0.13557833433151245\n",
      "Epoch 790, train NLL 0.13820403814315796\n",
      "Epoch 791, train NLL 0.1394745111465454\n",
      "Epoch 792, train NLL 0.13337883353233337\n",
      "Epoch 793, train NLL 0.1423872411251068\n",
      "Epoch 794, train NLL 0.1425221562385559\n",
      "Epoch 795, train NLL 0.14070357382297516\n",
      "Epoch 796, train NLL 0.13253404200077057\n",
      "Epoch 797, train NLL 0.1356768161058426\n",
      "Epoch 798, train NLL 0.13342304527759552\n",
      "Epoch 799, train NLL 0.13259176909923553\n",
      "Epoch 800, train NLL 0.13325057923793793\n",
      "Epoch 801, train NLL 0.1335424929857254\n",
      "Epoch 802, train NLL 0.13605445623397827\n",
      "Epoch 803, train NLL 0.1357206404209137\n",
      "Epoch 804, train NLL 0.14255879819393158\n",
      "Epoch 805, train NLL 0.13268403708934784\n",
      "Epoch 806, train NLL 0.13286477327346802\n",
      "Epoch 807, train NLL 0.13246378302574158\n",
      "Epoch 808, train NLL 0.13533884286880493\n",
      "Epoch 809, train NLL 0.13678814470767975\n",
      "Epoch 810, train NLL 0.13592639565467834\n",
      "Epoch 811, train NLL 0.1335136741399765\n",
      "Epoch 812, train NLL 0.1326725035905838\n",
      "Epoch 813, train NLL 0.1348489671945572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 814, train NLL 0.13963404297828674\n",
      "Epoch 815, train NLL 0.13420981168746948\n",
      "Epoch 816, train NLL 0.1325499564409256\n",
      "Epoch 817, train NLL 0.14294621348381042\n",
      "Epoch 818, train NLL 0.13332805037498474\n",
      "Epoch 819, train NLL 0.13619403541088104\n",
      "Epoch 820, train NLL 0.16551277041435242\n",
      "Epoch 821, train NLL 0.1340806782245636\n",
      "Epoch 822, train NLL 0.13416951894760132\n",
      "Epoch 823, train NLL 0.13639084994792938\n",
      "Epoch 824, train NLL 0.13287357985973358\n",
      "Epoch 825, train NLL 0.13326039910316467\n",
      "Epoch 826, train NLL 0.13373667001724243\n",
      "Epoch 827, train NLL 0.13420967757701874\n",
      "Epoch 828, train NLL 0.1404944807291031\n",
      "Epoch 829, train NLL 0.13226987421512604\n",
      "Epoch 830, train NLL 0.13657937943935394\n",
      "Epoch 831, train NLL 0.13399627804756165\n",
      "Epoch 832, train NLL 0.1330520063638687\n",
      "Epoch 833, train NLL 0.13820239901542664\n",
      "Epoch 834, train NLL 0.13220630586147308\n",
      "Epoch 835, train NLL 0.13337409496307373\n",
      "Epoch 836, train NLL 0.13859711587429047\n",
      "Epoch 837, train NLL 0.14252527058124542\n",
      "Epoch 838, train NLL 0.13282530009746552\n",
      "Epoch 839, train NLL 0.13990940153598785\n",
      "Epoch 840, train NLL 0.14489516615867615\n",
      "Epoch 841, train NLL 0.14103348553180695\n",
      "Epoch 842, train NLL 0.13256390392780304\n",
      "Epoch 843, train NLL 0.1421465426683426\n",
      "Epoch 844, train NLL 0.13500085473060608\n",
      "Epoch 845, train NLL 0.1449560672044754\n",
      "Epoch 846, train NLL 0.14074009656906128\n",
      "Epoch 847, train NLL 0.13767428696155548\n",
      "Epoch 848, train NLL 0.13225889205932617\n",
      "Epoch 849, train NLL 0.1582661271095276\n",
      "Epoch 850, train NLL 0.1545833796262741\n",
      "Epoch 851, train NLL 0.1693289577960968\n",
      "Epoch 852, train NLL 0.1321733146905899\n",
      "Epoch 853, train NLL 0.1377771645784378\n",
      "Epoch 854, train NLL 0.13223610818386078\n",
      "Epoch 855, train NLL 0.14804551005363464\n",
      "Epoch 856, train NLL 0.13829059898853302\n",
      "Epoch 857, train NLL 0.13849617540836334\n",
      "Epoch 858, train NLL 0.13799431920051575\n",
      "Epoch 859, train NLL 0.13212737441062927\n",
      "Epoch 860, train NLL 0.13824313879013062\n",
      "Epoch 861, train NLL 0.13692989945411682\n",
      "Epoch 862, train NLL 0.13207866251468658\n",
      "Epoch 863, train NLL 0.13277266919612885\n",
      "Epoch 864, train NLL 0.13447050750255585\n",
      "Epoch 865, train NLL 0.13204017281532288\n",
      "Epoch 866, train NLL 0.13243389129638672\n",
      "Epoch 867, train NLL 0.13205505907535553\n",
      "Epoch 868, train NLL 0.13261865079402924\n",
      "Epoch 869, train NLL 0.14855529367923737\n",
      "Epoch 870, train NLL 0.13631051778793335\n",
      "Epoch 871, train NLL 0.13767969608306885\n",
      "Epoch 872, train NLL 0.1499500423669815\n",
      "Epoch 873, train NLL 0.1327047199010849\n",
      "Epoch 874, train NLL 0.13290779292583466\n",
      "Epoch 875, train NLL 0.14049161970615387\n",
      "Epoch 876, train NLL 0.1403222531080246\n",
      "Epoch 877, train NLL 0.14399176836013794\n",
      "Epoch 878, train NLL 0.13301986455917358\n",
      "Epoch 879, train NLL 0.1550677865743637\n",
      "Epoch 880, train NLL 0.13211660087108612\n",
      "Epoch 881, train NLL 0.13231335580348969\n",
      "Epoch 882, train NLL 0.13305087387561798\n",
      "Epoch 883, train NLL 0.13199645280838013\n",
      "Epoch 884, train NLL 0.13200260698795319\n",
      "Epoch 885, train NLL 0.14133958518505096\n",
      "Epoch 886, train NLL 0.13447105884552002\n",
      "Epoch 887, train NLL 0.1357947289943695\n",
      "Epoch 888, train NLL 0.13370917737483978\n",
      "Epoch 889, train NLL 0.16602358222007751\n",
      "Epoch 890, train NLL 0.13191257417201996\n",
      "Epoch 891, train NLL 0.13193413615226746\n",
      "Epoch 892, train NLL 0.13207553327083588\n",
      "Epoch 893, train NLL 0.13396182656288147\n",
      "Epoch 894, train NLL 0.13210910558700562\n",
      "Epoch 895, train NLL 0.14092284440994263\n",
      "Epoch 896, train NLL 0.16424649953842163\n",
      "Epoch 897, train NLL 0.15503858029842377\n",
      "Epoch 898, train NLL 0.13189174234867096\n",
      "Epoch 899, train NLL 0.1319587677717209\n",
      "Epoch 900, train NLL 0.13630114495754242\n",
      "Epoch 901, train NLL 0.13266639411449432\n",
      "Epoch 902, train NLL 0.15189775824546814\n",
      "Epoch 903, train NLL 0.13307900726795197\n",
      "Epoch 904, train NLL 0.1366196870803833\n",
      "Epoch 905, train NLL 0.1442744880914688\n",
      "Epoch 906, train NLL 0.13250187039375305\n",
      "Epoch 907, train NLL 0.13242103159427643\n",
      "Epoch 908, train NLL 0.13226546347141266\n",
      "Epoch 909, train NLL 0.13203099370002747\n",
      "Epoch 910, train NLL 0.13187938928604126\n",
      "Epoch 911, train NLL 0.13283675909042358\n",
      "Epoch 912, train NLL 0.13615337014198303\n",
      "Epoch 913, train NLL 0.13342709839344025\n",
      "Epoch 914, train NLL 0.13396748900413513\n",
      "Epoch 915, train NLL 0.13193483650684357\n",
      "Epoch 916, train NLL 0.13218384981155396\n",
      "Epoch 917, train NLL 0.13335204124450684\n",
      "Epoch 918, train NLL 0.13670486211776733\n",
      "Epoch 919, train NLL 0.13527874648571014\n",
      "Epoch 920, train NLL 0.13183635473251343\n",
      "Epoch 921, train NLL 0.13194310665130615\n",
      "Epoch 922, train NLL 0.13341303169727325\n",
      "Epoch 923, train NLL 0.1356634944677353\n",
      "Epoch 924, train NLL 0.13829092681407928\n",
      "Epoch 925, train NLL 0.131765216588974\n",
      "Epoch 926, train NLL 0.14927004277706146\n",
      "Epoch 927, train NLL 0.14509636163711548\n",
      "Epoch 928, train NLL 0.13202737271785736\n",
      "Epoch 929, train NLL 0.13314954936504364\n",
      "Epoch 930, train NLL 0.13720516860485077\n",
      "Epoch 931, train NLL 0.13439743220806122\n",
      "Epoch 932, train NLL 0.1345333606004715\n",
      "Epoch 933, train NLL 0.13768310844898224\n",
      "Epoch 934, train NLL 0.1346706598997116\n",
      "Epoch 935, train NLL 0.14980605244636536\n",
      "Epoch 936, train NLL 0.13600051403045654\n",
      "Epoch 937, train NLL 0.137877956032753\n",
      "Epoch 938, train NLL 0.13187919557094574\n",
      "Epoch 939, train NLL 0.13734662532806396\n",
      "Epoch 940, train NLL 0.13176803290843964\n",
      "Epoch 941, train NLL 0.13768017292022705\n",
      "Epoch 942, train NLL 0.14023390412330627\n",
      "Epoch 943, train NLL 0.13222290575504303\n",
      "Epoch 944, train NLL 0.1382717788219452\n",
      "Epoch 945, train NLL 0.1324572116136551\n",
      "Epoch 946, train NLL 0.14949610829353333\n",
      "Epoch 947, train NLL 0.13381275534629822\n",
      "Epoch 948, train NLL 0.13205094635486603\n",
      "Epoch 949, train NLL 0.1316939890384674\n",
      "Epoch 950, train NLL 0.1316683143377304\n",
      "Epoch 951, train NLL 0.13167569041252136\n",
      "Epoch 952, train NLL 0.13199324905872345\n",
      "Epoch 953, train NLL 0.13955597579479218\n",
      "Epoch 954, train NLL 0.13939054310321808\n",
      "Epoch 955, train NLL 0.13211718201637268\n",
      "Epoch 956, train NLL 0.13951130211353302\n",
      "Epoch 957, train NLL 0.13725200295448303\n",
      "Epoch 958, train NLL 0.13212084770202637\n",
      "Epoch 959, train NLL 0.13536380231380463\n",
      "Epoch 960, train NLL 0.13207879662513733\n",
      "Epoch 961, train NLL 0.13265341520309448\n",
      "Epoch 962, train NLL 0.1318030059337616\n",
      "Epoch 963, train NLL 0.132656991481781\n",
      "Epoch 964, train NLL 0.13637512922286987\n",
      "Epoch 965, train NLL 0.13319042325019836\n",
      "Epoch 966, train NLL 0.13349294662475586\n",
      "Epoch 967, train NLL 0.13548602163791656\n",
      "Epoch 968, train NLL 0.13419923186302185\n",
      "Epoch 969, train NLL 0.13171277940273285\n",
      "Epoch 970, train NLL 0.1407267302274704\n",
      "Epoch 971, train NLL 0.132642924785614\n",
      "Epoch 972, train NLL 0.1329934000968933\n",
      "Epoch 973, train NLL 0.14072827994823456\n",
      "Epoch 974, train NLL 0.15686039626598358\n",
      "Epoch 975, train NLL 0.1321452409029007\n",
      "Epoch 976, train NLL 0.13234134018421173\n",
      "Epoch 977, train NLL 0.13449764251708984\n",
      "Epoch 978, train NLL 0.13168834149837494\n",
      "Epoch 979, train NLL 0.13200190663337708\n",
      "Epoch 980, train NLL 0.13512839376926422\n",
      "Epoch 981, train NLL 0.13371925055980682\n",
      "Epoch 982, train NLL 0.13192501664161682\n",
      "Epoch 983, train NLL 0.13212689757347107\n",
      "Epoch 984, train NLL 0.13165754079818726\n",
      "Epoch 985, train NLL 0.13173602521419525\n",
      "Epoch 986, train NLL 0.13466346263885498\n",
      "Epoch 987, train NLL 0.13155022263526917\n",
      "Epoch 988, train NLL 0.13207800686359406\n",
      "Epoch 989, train NLL 0.1354239135980606\n",
      "Epoch 990, train NLL 0.14274246990680695\n",
      "Epoch 991, train NLL 0.13915221393108368\n",
      "Epoch 992, train NLL 0.13828544318675995\n",
      "Epoch 993, train NLL 0.14566822350025177\n",
      "Epoch 994, train NLL 0.14166782796382904\n",
      "Epoch 995, train NLL 0.1315191388130188\n",
      "Epoch 996, train NLL 0.1326497495174408\n",
      "Epoch 997, train NLL 0.1315276026725769\n",
      "Epoch 998, train NLL 0.13199496269226074\n",
      "Epoch 999, train NLL 0.13909515738487244\n",
      "Epoch 1000, train NLL 0.13635067641735077\n",
      "Epoch 1001, train NLL 0.13185226917266846\n",
      "Epoch 1002, train NLL 0.13499316573143005\n",
      "Epoch 1003, train NLL 0.13151796162128448\n",
      "Epoch 1004, train NLL 0.15302874147891998\n",
      "Epoch 1005, train NLL 0.14408954977989197\n",
      "Epoch 1006, train NLL 0.13212423026561737\n",
      "Epoch 1007, train NLL 0.14097405970096588\n",
      "Epoch 1008, train NLL 0.13547037541866302\n",
      "Epoch 1009, train NLL 0.13539591431617737\n",
      "Epoch 1010, train NLL 0.13724757730960846\n",
      "Epoch 1011, train NLL 0.13149255514144897\n",
      "Epoch 1012, train NLL 0.15317930281162262\n",
      "Epoch 1013, train NLL 0.13646887242794037\n",
      "Epoch 1014, train NLL 0.13595232367515564\n",
      "Epoch 1015, train NLL 0.13461199402809143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1016, train NLL 0.13367871940135956\n",
      "Epoch 1017, train NLL 0.13293994963169098\n",
      "Epoch 1018, train NLL 0.13226376473903656\n",
      "Epoch 1019, train NLL 0.13200296461582184\n",
      "Epoch 1020, train NLL 0.1321767270565033\n",
      "Epoch 1021, train NLL 0.13546501100063324\n",
      "Epoch 1022, train NLL 0.13160929083824158\n",
      "Epoch 1023, train NLL 0.14365743100643158\n",
      "Epoch 1024, train NLL 0.13383744657039642\n",
      "Epoch 1025, train NLL 0.13327236473560333\n",
      "Epoch 1026, train NLL 0.13326925039291382\n",
      "Epoch 1027, train NLL 0.1529477834701538\n",
      "Epoch 1028, train NLL 0.13914059102535248\n",
      "Epoch 1029, train NLL 0.1322118043899536\n",
      "Epoch 1030, train NLL 0.1330782175064087\n",
      "Epoch 1031, train NLL 0.1390732228755951\n",
      "Epoch 1032, train NLL 0.13366195559501648\n",
      "Epoch 1033, train NLL 0.13920971751213074\n",
      "Epoch 1034, train NLL 0.13395185768604279\n",
      "Epoch 1035, train NLL 0.13396476209163666\n",
      "Epoch 1036, train NLL 0.13601979613304138\n",
      "Epoch 1037, train NLL 0.13469550013542175\n",
      "Epoch 1038, train NLL 0.13255935907363892\n",
      "Epoch 1039, train NLL 0.1336858868598938\n",
      "Epoch 1040, train NLL 0.1315060704946518\n",
      "Epoch 1041, train NLL 0.13301686942577362\n",
      "Epoch 1042, train NLL 0.1397397220134735\n",
      "Epoch 1043, train NLL 0.14642009139060974\n",
      "Epoch 1044, train NLL 0.13399922847747803\n",
      "Epoch 1045, train NLL 0.13275635242462158\n",
      "Epoch 1046, train NLL 0.14027145504951477\n",
      "Epoch 1047, train NLL 0.132757306098938\n",
      "Epoch 1048, train NLL 0.13144899904727936\n",
      "Epoch 1049, train NLL 0.13283875584602356\n",
      "Epoch 1050, train NLL 0.1350889354944229\n",
      "Epoch 1051, train NLL 0.15807947516441345\n",
      "Epoch 1052, train NLL 0.13705427944660187\n",
      "Epoch 1053, train NLL 0.13160055875778198\n",
      "Epoch 1054, train NLL 0.13606223464012146\n",
      "Epoch 1055, train NLL 0.1316731572151184\n",
      "Epoch 1056, train NLL 0.13255935907363892\n",
      "Epoch 1057, train NLL 0.13152936100959778\n",
      "Epoch 1058, train NLL 0.1362183541059494\n",
      "Epoch 1059, train NLL 0.13233861327171326\n",
      "Epoch 1060, train NLL 0.13134188950061798\n",
      "Epoch 1061, train NLL 0.13756361603736877\n",
      "Epoch 1062, train NLL 0.1325753927230835\n",
      "Epoch 1063, train NLL 0.13230958580970764\n",
      "Epoch 1064, train NLL 0.1482074111700058\n",
      "Epoch 1065, train NLL 0.1319105178117752\n",
      "Epoch 1066, train NLL 0.13829898834228516\n",
      "Epoch 1067, train NLL 0.13974153995513916\n",
      "Epoch 1068, train NLL 0.13198895752429962\n",
      "Epoch 1069, train NLL 0.13152237236499786\n",
      "Epoch 1070, train NLL 0.13150052726268768\n",
      "Epoch 1071, train NLL 0.13155990839004517\n",
      "Epoch 1072, train NLL 0.13900598883628845\n",
      "Epoch 1073, train NLL 0.13716839253902435\n",
      "Epoch 1074, train NLL 0.1330828219652176\n",
      "Epoch 1075, train NLL 0.13164591789245605\n",
      "Epoch 1076, train NLL 0.13154882192611694\n",
      "Epoch 1077, train NLL 0.13875322043895721\n",
      "Epoch 1078, train NLL 0.1318317949771881\n",
      "Epoch 1079, train NLL 0.13179950416088104\n",
      "Epoch 1080, train NLL 0.13514776527881622\n",
      "Epoch 1081, train NLL 0.1314985603094101\n",
      "Epoch 1082, train NLL 0.1341816633939743\n",
      "Epoch 1083, train NLL 0.13214290142059326\n",
      "Epoch 1084, train NLL 0.1357690393924713\n",
      "Epoch 1085, train NLL 0.16106653213500977\n",
      "Epoch 1086, train NLL 0.1312730610370636\n",
      "Epoch 1087, train NLL 0.13389301300048828\n",
      "Epoch 1088, train NLL 0.13259434700012207\n",
      "Epoch 1089, train NLL 0.14817801117897034\n",
      "Epoch 1090, train NLL 0.13229423761367798\n",
      "Epoch 1091, train NLL 0.1344285011291504\n",
      "Epoch 1092, train NLL 0.13199235498905182\n",
      "Epoch 1093, train NLL 0.13375341892242432\n",
      "Epoch 1094, train NLL 0.13163451850414276\n",
      "Epoch 1095, train NLL 0.1319030523300171\n",
      "Epoch 1096, train NLL 0.13189037144184113\n",
      "Epoch 1097, train NLL 0.1353364884853363\n",
      "Epoch 1098, train NLL 0.13175588846206665\n",
      "Epoch 1099, train NLL 0.14461778104305267\n",
      "Epoch 1100, train NLL 0.13439010083675385\n",
      "Epoch 1101, train NLL 0.1312391608953476\n",
      "Epoch 1102, train NLL 0.13134832680225372\n",
      "Epoch 1103, train NLL 0.13188737630844116\n",
      "Epoch 1104, train NLL 0.13431213796138763\n",
      "Epoch 1105, train NLL 0.1314375400543213\n",
      "Epoch 1106, train NLL 0.13124622404575348\n",
      "Epoch 1107, train NLL 0.13130976259708405\n",
      "Epoch 1108, train NLL 0.1314093917608261\n",
      "Epoch 1109, train NLL 0.1334988921880722\n",
      "Epoch 1110, train NLL 0.131963312625885\n",
      "Epoch 1111, train NLL 0.13430480659008026\n",
      "Epoch 1112, train NLL 0.13190267980098724\n",
      "Epoch 1113, train NLL 0.14690931141376495\n",
      "Epoch 1114, train NLL 0.13151293992996216\n",
      "Epoch 1115, train NLL 0.1536978781223297\n",
      "Epoch 1116, train NLL 0.1319233477115631\n",
      "Epoch 1117, train NLL 0.13634084165096283\n",
      "Epoch 1118, train NLL 0.13184218108654022\n",
      "Epoch 1119, train NLL 0.13616685569286346\n",
      "Epoch 1120, train NLL 0.14005349576473236\n",
      "Epoch 1121, train NLL 0.13222819566726685\n",
      "Epoch 1122, train NLL 0.13267409801483154\n",
      "Epoch 1123, train NLL 0.1313307136297226\n",
      "Epoch 1124, train NLL 0.1311928778886795\n",
      "Epoch 1125, train NLL 0.13119733333587646\n",
      "Epoch 1126, train NLL 0.13785162568092346\n",
      "Epoch 1127, train NLL 0.13181044161319733\n",
      "Epoch 1128, train NLL 0.1313936561346054\n",
      "Epoch 1129, train NLL 0.13125313818454742\n",
      "Epoch 1130, train NLL 0.13136763870716095\n",
      "Epoch 1131, train NLL 0.13126030564308167\n",
      "Epoch 1132, train NLL 0.13180068135261536\n",
      "Epoch 1133, train NLL 0.13138526678085327\n",
      "Epoch 1134, train NLL 0.15143534541130066\n",
      "Epoch 1135, train NLL 0.13864177465438843\n",
      "Epoch 1136, train NLL 0.13353903591632843\n",
      "Epoch 1137, train NLL 0.13272756338119507\n",
      "Epoch 1138, train NLL 0.1312437504529953\n",
      "Epoch 1139, train NLL 0.14985251426696777\n",
      "Epoch 1140, train NLL 0.1321631371974945\n",
      "Epoch 1141, train NLL 0.13211721181869507\n",
      "Epoch 1142, train NLL 0.1322159320116043\n",
      "Epoch 1143, train NLL 0.13125789165496826\n",
      "Epoch 1144, train NLL 0.13678757846355438\n",
      "Epoch 1145, train NLL 0.13193395733833313\n",
      "Epoch 1146, train NLL 0.16080324351787567\n",
      "Epoch 1147, train NLL 0.13506482541561127\n",
      "Epoch 1148, train NLL 0.1328381448984146\n",
      "Epoch 1149, train NLL 0.1319347769021988\n",
      "Epoch 1150, train NLL 0.13115444779396057\n",
      "Epoch 1151, train NLL 0.13142023980617523\n",
      "Epoch 1152, train NLL 0.14576630294322968\n",
      "Epoch 1153, train NLL 0.13114728033542633\n",
      "Epoch 1154, train NLL 0.13281823694705963\n",
      "Epoch 1155, train NLL 0.13654208183288574\n",
      "Epoch 1156, train NLL 0.13113518059253693\n",
      "Epoch 1157, train NLL 0.1312105804681778\n",
      "Epoch 1158, train NLL 0.13116182386875153\n",
      "Epoch 1159, train NLL 0.131140798330307\n",
      "Epoch 1160, train NLL 0.13601379096508026\n",
      "Epoch 1161, train NLL 0.13354675471782684\n",
      "Epoch 1162, train NLL 0.14153096079826355\n",
      "Epoch 1163, train NLL 0.1410706490278244\n",
      "Epoch 1164, train NLL 0.13328638672828674\n",
      "Epoch 1165, train NLL 0.13221874833106995\n",
      "Epoch 1166, train NLL 0.14018772542476654\n",
      "Epoch 1167, train NLL 0.13111592829227448\n",
      "Epoch 1168, train NLL 0.13565289974212646\n",
      "Epoch 1169, train NLL 0.13190455734729767\n",
      "Epoch 1170, train NLL 0.13163800537586212\n",
      "Epoch 1171, train NLL 0.13224613666534424\n",
      "Epoch 1172, train NLL 0.13666857779026031\n",
      "Epoch 1173, train NLL 0.13110677897930145\n",
      "Epoch 1174, train NLL 0.13168232142925262\n",
      "Epoch 1175, train NLL 0.13580918312072754\n",
      "Epoch 1176, train NLL 0.13857461512088776\n",
      "Epoch 1177, train NLL 0.13174134492874146\n",
      "Epoch 1178, train NLL 0.137972891330719\n",
      "Epoch 1179, train NLL 0.1371031105518341\n",
      "Epoch 1180, train NLL 0.134863018989563\n",
      "Epoch 1181, train NLL 0.14063458144664764\n",
      "Epoch 1182, train NLL 0.13257570564746857\n",
      "Epoch 1183, train NLL 0.13115042448043823\n",
      "Epoch 1184, train NLL 0.13167399168014526\n",
      "Epoch 1185, train NLL 0.1316503882408142\n",
      "Epoch 1186, train NLL 0.13178423047065735\n",
      "Epoch 1187, train NLL 0.133117213845253\n",
      "Epoch 1188, train NLL 0.13321687281131744\n",
      "Epoch 1189, train NLL 0.13117653131484985\n",
      "Epoch 1190, train NLL 0.13147220015525818\n",
      "Epoch 1191, train NLL 0.1318778097629547\n",
      "Epoch 1192, train NLL 0.1314864307641983\n",
      "Epoch 1193, train NLL 0.15034189820289612\n",
      "Epoch 1194, train NLL 0.13174095749855042\n",
      "Epoch 1195, train NLL 0.13851076364517212\n",
      "Epoch 1196, train NLL 0.14102338254451752\n",
      "Epoch 1197, train NLL 0.13468213379383087\n",
      "Epoch 1198, train NLL 0.1315286010503769\n",
      "Epoch 1199, train NLL 0.13123272359371185\n",
      "Epoch 1200, train NLL 0.1342911571264267\n",
      "Epoch 1201, train NLL 0.13941790163516998\n",
      "Epoch 1202, train NLL 0.1351477950811386\n",
      "Epoch 1203, train NLL 0.1360773891210556\n",
      "Epoch 1204, train NLL 0.14261263608932495\n",
      "Epoch 1205, train NLL 0.1311974972486496\n",
      "Epoch 1206, train NLL 0.13626748323440552\n",
      "Epoch 1207, train NLL 0.13500593602657318\n",
      "Epoch 1208, train NLL 0.13121677935123444\n",
      "Epoch 1209, train NLL 0.1321708709001541\n",
      "Epoch 1210, train NLL 0.13123516738414764\n",
      "Epoch 1211, train NLL 0.13575303554534912\n",
      "Epoch 1212, train NLL 0.13847078382968903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1213, train NLL 0.13725484907627106\n",
      "Epoch 1214, train NLL 0.1317712515592575\n",
      "Epoch 1215, train NLL 0.13614624738693237\n",
      "Epoch 1216, train NLL 0.14327266812324524\n",
      "Epoch 1217, train NLL 0.13768422603607178\n",
      "Epoch 1218, train NLL 0.13439036905765533\n",
      "Epoch 1219, train NLL 0.13783498108386993\n",
      "Epoch 1220, train NLL 0.13132748007774353\n",
      "Epoch 1221, train NLL 0.1322454959154129\n",
      "Epoch 1222, train NLL 0.14096695184707642\n",
      "Epoch 1223, train NLL 0.13104234635829926\n",
      "Epoch 1224, train NLL 0.13776454329490662\n",
      "Epoch 1225, train NLL 0.13346834480762482\n",
      "Epoch 1226, train NLL 0.13106289505958557\n",
      "Epoch 1227, train NLL 0.13214243948459625\n",
      "Epoch 1228, train NLL 0.13137686252593994\n",
      "Epoch 1229, train NLL 0.14543786644935608\n",
      "Epoch 1230, train NLL 0.13310928642749786\n",
      "Epoch 1231, train NLL 0.13211342692375183\n",
      "Epoch 1232, train NLL 0.1325867772102356\n",
      "Epoch 1233, train NLL 0.14251886308193207\n",
      "Epoch 1234, train NLL 0.13221138715744019\n",
      "Epoch 1235, train NLL 0.1311318278312683\n",
      "Epoch 1236, train NLL 0.13343490660190582\n",
      "Epoch 1237, train NLL 0.13342750072479248\n",
      "Epoch 1238, train NLL 0.13390055298805237\n",
      "Epoch 1239, train NLL 0.14057746529579163\n",
      "Epoch 1240, train NLL 0.13285991549491882\n",
      "Epoch 1241, train NLL 0.13142019510269165\n",
      "Epoch 1242, train NLL 0.13175782561302185\n",
      "Epoch 1243, train NLL 0.13728320598602295\n",
      "Epoch 1244, train NLL 0.1388237178325653\n",
      "Epoch 1245, train NLL 0.13104845583438873\n",
      "Epoch 1246, train NLL 0.13722126185894012\n",
      "Epoch 1247, train NLL 0.1349937915802002\n",
      "Epoch 1248, train NLL 0.13114020228385925\n",
      "Epoch 1249, train NLL 0.1369868814945221\n",
      "Epoch 1250, train NLL 0.1336284875869751\n",
      "Epoch 1251, train NLL 0.13110196590423584\n",
      "Epoch 1252, train NLL 0.14719341695308685\n",
      "Epoch 1253, train NLL 0.133866086602211\n",
      "Epoch 1254, train NLL 0.14492398500442505\n",
      "Epoch 1255, train NLL 0.1432534009218216\n",
      "Epoch 1256, train NLL 0.13447636365890503\n",
      "Epoch 1257, train NLL 0.1328447461128235\n",
      "Epoch 1258, train NLL 0.13137371838092804\n",
      "Epoch 1259, train NLL 0.13098131120204926\n",
      "Epoch 1260, train NLL 0.1359577625989914\n",
      "Epoch 1261, train NLL 0.13525988161563873\n",
      "Epoch 1262, train NLL 0.1373816728591919\n",
      "Epoch 1263, train NLL 0.13104090094566345\n",
      "Epoch 1264, train NLL 0.13272076845169067\n",
      "Epoch 1265, train NLL 0.13127301633358002\n",
      "Epoch 1266, train NLL 0.13512343168258667\n",
      "Epoch 1267, train NLL 0.13349822163581848\n",
      "Epoch 1268, train NLL 0.13757945597171783\n",
      "Epoch 1269, train NLL 0.133692666888237\n",
      "Epoch 1270, train NLL 0.13213568925857544\n",
      "Epoch 1271, train NLL 0.13541583716869354\n",
      "Epoch 1272, train NLL 0.13397562503814697\n",
      "Epoch 1273, train NLL 0.1328771561384201\n",
      "Epoch 1274, train NLL 0.13400349020957947\n",
      "Epoch 1275, train NLL 0.13097093999385834\n",
      "Epoch 1276, train NLL 0.1316404640674591\n",
      "Epoch 1277, train NLL 0.13293206691741943\n",
      "Epoch 1278, train NLL 0.1313048005104065\n",
      "Epoch 1279, train NLL 0.13277626037597656\n",
      "Epoch 1280, train NLL 0.13271772861480713\n",
      "Epoch 1281, train NLL 0.1373237669467926\n",
      "Epoch 1282, train NLL 0.13567565381526947\n",
      "Epoch 1283, train NLL 0.1353258341550827\n",
      "Epoch 1284, train NLL 0.14044666290283203\n",
      "Epoch 1285, train NLL 0.13170087337493896\n",
      "Epoch 1286, train NLL 0.13097280263900757\n",
      "Epoch 1287, train NLL 0.13739699125289917\n",
      "Epoch 1288, train NLL 0.13418152928352356\n",
      "Epoch 1289, train NLL 0.1554865688085556\n",
      "Epoch 1290, train NLL 0.1455608308315277\n",
      "Epoch 1291, train NLL 0.13167519867420197\n",
      "Epoch 1292, train NLL 0.13165898621082306\n",
      "Epoch 1293, train NLL 0.13971726596355438\n",
      "Epoch 1294, train NLL 0.13632237911224365\n",
      "Epoch 1295, train NLL 0.1352567970752716\n",
      "Epoch 1296, train NLL 0.1318552941083908\n",
      "Epoch 1297, train NLL 0.1339513659477234\n",
      "Epoch 1298, train NLL 0.1317230463027954\n",
      "Epoch 1299, train NLL 0.135990709066391\n",
      "Epoch 1300, train NLL 0.13103559613227844\n",
      "Epoch 1301, train NLL 0.13146118819713593\n",
      "Epoch 1302, train NLL 0.1309787780046463\n",
      "Epoch 1303, train NLL 0.15709419548511505\n",
      "Epoch 1304, train NLL 0.13220839202404022\n",
      "Epoch 1305, train NLL 0.13241176307201385\n",
      "Epoch 1306, train NLL 0.13961806893348694\n",
      "Epoch 1307, train NLL 0.13762444257736206\n",
      "Epoch 1308, train NLL 0.1404247134923935\n",
      "Epoch 1309, train NLL 0.1327865868806839\n",
      "Epoch 1310, train NLL 0.13091030716896057\n",
      "Epoch 1311, train NLL 0.1315736025571823\n",
      "Epoch 1312, train NLL 0.1338645964860916\n",
      "Epoch 1313, train NLL 0.13243530690670013\n",
      "Epoch 1314, train NLL 0.13432618975639343\n",
      "Epoch 1315, train NLL 0.13206063210964203\n",
      "Epoch 1316, train NLL 0.1343603879213333\n",
      "Epoch 1317, train NLL 0.13193678855895996\n",
      "Epoch 1318, train NLL 0.13959549367427826\n",
      "Epoch 1319, train NLL 0.13130426406860352\n",
      "Epoch 1320, train NLL 0.1313488930463791\n",
      "Epoch 1321, train NLL 0.137600839138031\n",
      "Epoch 1322, train NLL 0.14354762434959412\n",
      "Epoch 1323, train NLL 0.13187606632709503\n",
      "Epoch 1324, train NLL 0.13102588057518005\n",
      "Epoch 1325, train NLL 0.13322380185127258\n",
      "Epoch 1326, train NLL 0.13147859275341034\n",
      "Epoch 1327, train NLL 0.1332249641418457\n",
      "Epoch 1328, train NLL 0.13600249588489532\n",
      "Epoch 1329, train NLL 0.13213418424129486\n",
      "Epoch 1330, train NLL 0.1323496550321579\n",
      "Epoch 1331, train NLL 0.14286135137081146\n",
      "Epoch 1332, train NLL 0.1378515660762787\n",
      "Epoch 1333, train NLL 0.15382584929466248\n",
      "Epoch 1334, train NLL 0.13088244199752808\n",
      "Epoch 1335, train NLL 0.13252757489681244\n",
      "Epoch 1336, train NLL 0.1364312618970871\n",
      "Epoch 1337, train NLL 0.13093136250972748\n",
      "Epoch 1338, train NLL 0.1337593048810959\n",
      "Epoch 1339, train NLL 0.1310780644416809\n",
      "Epoch 1340, train NLL 0.1339721828699112\n",
      "Epoch 1341, train NLL 0.1372237205505371\n",
      "Epoch 1342, train NLL 0.14497999846935272\n",
      "Epoch 1343, train NLL 0.13118480145931244\n",
      "Epoch 1344, train NLL 0.1312309205532074\n",
      "Epoch 1345, train NLL 0.13653871417045593\n",
      "Epoch 1346, train NLL 0.1311265528202057\n",
      "Epoch 1347, train NLL 0.13169816136360168\n",
      "Epoch 1348, train NLL 0.13090461492538452\n",
      "Epoch 1349, train NLL 0.13088540732860565\n",
      "Epoch 1350, train NLL 0.13090625405311584\n",
      "Epoch 1351, train NLL 0.13154754042625427\n",
      "Epoch 1352, train NLL 0.1443812996149063\n",
      "Epoch 1353, train NLL 0.1334337741136551\n",
      "Epoch 1354, train NLL 0.13101090490818024\n",
      "Epoch 1355, train NLL 0.13110269606113434\n",
      "Epoch 1356, train NLL 0.1311657577753067\n",
      "Epoch 1357, train NLL 0.14596696197986603\n",
      "Epoch 1358, train NLL 0.13177084922790527\n",
      "Epoch 1359, train NLL 0.13468948006629944\n",
      "Epoch 1360, train NLL 0.14543750882148743\n",
      "Epoch 1361, train NLL 0.13461826741695404\n",
      "Epoch 1362, train NLL 0.13152538239955902\n",
      "Epoch 1363, train NLL 0.13235417008399963\n",
      "Epoch 1364, train NLL 0.14435382187366486\n",
      "Epoch 1365, train NLL 0.1419498473405838\n",
      "Epoch 1366, train NLL 0.1308789998292923\n",
      "Epoch 1367, train NLL 0.13659462332725525\n",
      "Epoch 1368, train NLL 0.1376829594373703\n",
      "Epoch 1369, train NLL 0.13093210756778717\n",
      "Epoch 1370, train NLL 0.1341925859451294\n",
      "Epoch 1371, train NLL 0.13172052800655365\n",
      "Epoch 1372, train NLL 0.1433824747800827\n",
      "Epoch 1373, train NLL 0.1349812000989914\n",
      "Epoch 1374, train NLL 0.1310504823923111\n",
      "Epoch 1375, train NLL 0.13505129516124725\n",
      "Epoch 1376, train NLL 0.13429106771945953\n",
      "Epoch 1377, train NLL 0.1350986361503601\n",
      "Epoch 1378, train NLL 0.13195273280143738\n",
      "Epoch 1379, train NLL 0.14054180681705475\n",
      "Epoch 1380, train NLL 0.13122063875198364\n",
      "Epoch 1381, train NLL 0.13352718949317932\n",
      "Epoch 1382, train NLL 0.131132110953331\n",
      "Epoch 1383, train NLL 0.13145755231380463\n",
      "Epoch 1384, train NLL 0.13133853673934937\n",
      "Epoch 1385, train NLL 0.13550828397274017\n",
      "Epoch 1386, train NLL 0.13188093900680542\n",
      "Epoch 1387, train NLL 0.14291125535964966\n",
      "Epoch 1388, train NLL 0.13518495857715607\n",
      "Epoch 1389, train NLL 0.13086773455142975\n",
      "Epoch 1390, train NLL 0.13271388411521912\n",
      "Epoch 1391, train NLL 0.13093914091587067\n",
      "Epoch 1392, train NLL 0.13324642181396484\n",
      "Epoch 1393, train NLL 0.13105060160160065\n",
      "Epoch 1394, train NLL 0.13084520399570465\n",
      "Epoch 1395, train NLL 0.13293640315532684\n",
      "Epoch 1396, train NLL 0.13205267488956451\n",
      "Epoch 1397, train NLL 0.13163398206233978\n",
      "Epoch 1398, train NLL 0.13104581832885742\n",
      "Epoch 1399, train NLL 0.13190074265003204\n",
      "Epoch 1400, train NLL 0.13851231336593628\n",
      "Epoch 1401, train NLL 0.13082320988178253\n",
      "Epoch 1402, train NLL 0.1314525455236435\n",
      "Epoch 1403, train NLL 0.1314050555229187\n",
      "Epoch 1404, train NLL 0.1324036717414856\n",
      "Epoch 1405, train NLL 0.13104486465454102\n",
      "Epoch 1406, train NLL 0.1308467984199524\n",
      "Epoch 1407, train NLL 0.13192379474639893\n",
      "Epoch 1408, train NLL 0.1338813304901123\n",
      "Epoch 1409, train NLL 0.13179318606853485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1410, train NLL 0.1308572143316269\n",
      "Epoch 1411, train NLL 0.13341577351093292\n",
      "Epoch 1412, train NLL 0.13266326487064362\n",
      "Epoch 1413, train NLL 0.13120310008525848\n",
      "Epoch 1414, train NLL 0.1364264041185379\n",
      "Epoch 1415, train NLL 0.1330544501543045\n",
      "Epoch 1416, train NLL 0.13458485901355743\n",
      "Epoch 1417, train NLL 0.13466902077198029\n",
      "Epoch 1418, train NLL 0.132578507065773\n",
      "Epoch 1419, train NLL 0.1314946413040161\n",
      "Epoch 1420, train NLL 0.1308365911245346\n",
      "Epoch 1421, train NLL 0.13136516511440277\n",
      "Epoch 1422, train NLL 0.131162628531456\n",
      "Epoch 1423, train NLL 0.13456957042217255\n",
      "Epoch 1424, train NLL 0.1405373066663742\n",
      "Epoch 1425, train NLL 0.13101738691329956\n",
      "Epoch 1426, train NLL 0.13081306219100952\n",
      "Epoch 1427, train NLL 0.13166795670986176\n",
      "Epoch 1428, train NLL 0.1438812017440796\n",
      "Epoch 1429, train NLL 0.14180728793144226\n",
      "Epoch 1430, train NLL 0.1373157650232315\n",
      "Epoch 1431, train NLL 0.13572292029857635\n",
      "Epoch 1432, train NLL 0.1343516707420349\n",
      "Epoch 1433, train NLL 0.13905562460422516\n",
      "Epoch 1434, train NLL 0.1346142441034317\n",
      "Epoch 1435, train NLL 0.13158190250396729\n",
      "Epoch 1436, train NLL 0.13119222223758698\n",
      "Epoch 1437, train NLL 0.13700661063194275\n",
      "Epoch 1438, train NLL 0.13206422328948975\n",
      "Epoch 1439, train NLL 0.1590910702943802\n",
      "Epoch 1440, train NLL 0.13118715584278107\n",
      "Epoch 1441, train NLL 0.13194116950035095\n",
      "Epoch 1442, train NLL 0.13115371763706207\n",
      "Epoch 1443, train NLL 0.13218827545642853\n",
      "Epoch 1444, train NLL 0.13083910942077637\n",
      "Epoch 1445, train NLL 0.1386924833059311\n",
      "Epoch 1446, train NLL 0.13180893659591675\n",
      "Epoch 1447, train NLL 0.1336728036403656\n",
      "Epoch 1448, train NLL 0.13856001198291779\n",
      "Epoch 1449, train NLL 0.13256603479385376\n",
      "Epoch 1450, train NLL 0.13991759717464447\n",
      "Epoch 1451, train NLL 0.13090679049491882\n",
      "Epoch 1452, train NLL 0.13324381411075592\n",
      "Epoch 1453, train NLL 0.1330295354127884\n",
      "Epoch 1454, train NLL 0.13385595381259918\n",
      "Epoch 1455, train NLL 0.137628972530365\n",
      "Epoch 1456, train NLL 0.13192719221115112\n",
      "Epoch 1457, train NLL 0.1393386870622635\n",
      "Epoch 1458, train NLL 0.1332467496395111\n",
      "Epoch 1459, train NLL 0.1317601352930069\n",
      "Epoch 1460, train NLL 0.1367618441581726\n",
      "Epoch 1461, train NLL 0.1421823650598526\n",
      "Epoch 1462, train NLL 0.13139964640140533\n",
      "Epoch 1463, train NLL 0.14363113045692444\n",
      "Epoch 1464, train NLL 0.13976255059242249\n",
      "Epoch 1465, train NLL 0.13208122551441193\n",
      "Epoch 1466, train NLL 0.136423259973526\n",
      "Epoch 1467, train NLL 0.13095128536224365\n",
      "Epoch 1468, train NLL 0.13115330040454865\n",
      "Epoch 1469, train NLL 0.13140910863876343\n",
      "Epoch 1470, train NLL 0.13091328740119934\n",
      "Epoch 1471, train NLL 0.13784530758857727\n",
      "Epoch 1472, train NLL 0.13078023493289948\n",
      "Epoch 1473, train NLL 0.14143966138362885\n",
      "Epoch 1474, train NLL 0.150514155626297\n",
      "Epoch 1475, train NLL 0.1314781904220581\n",
      "Epoch 1476, train NLL 0.13086406886577606\n",
      "Epoch 1477, train NLL 0.13077247142791748\n",
      "Epoch 1478, train NLL 0.14002905786037445\n",
      "Epoch 1479, train NLL 0.13580374419689178\n",
      "Epoch 1480, train NLL 0.1345628798007965\n",
      "Epoch 1481, train NLL 0.13898499310016632\n",
      "Epoch 1482, train NLL 0.13076868653297424\n",
      "Epoch 1483, train NLL 0.14080601930618286\n",
      "Epoch 1484, train NLL 0.13745282590389252\n",
      "Epoch 1485, train NLL 0.13690593838691711\n",
      "Epoch 1486, train NLL 0.13615429401397705\n",
      "Epoch 1487, train NLL 0.13320545852184296\n",
      "Epoch 1488, train NLL 0.1314162164926529\n",
      "Epoch 1489, train NLL 0.1340429186820984\n",
      "Epoch 1490, train NLL 0.1308680772781372\n",
      "Epoch 1491, train NLL 0.13185259699821472\n",
      "Epoch 1492, train NLL 0.13104680180549622\n",
      "Epoch 1493, train NLL 0.1378071904182434\n",
      "Epoch 1494, train NLL 0.13159668445587158\n",
      "Epoch 1495, train NLL 0.14357833564281464\n",
      "Epoch 1496, train NLL 0.13331300020217896\n",
      "Epoch 1497, train NLL 0.14307890832424164\n",
      "Epoch 1498, train NLL 0.13462582230567932\n",
      "Epoch 1499, train NLL 0.13906866312026978\n",
      "Epoch 1500, train NLL 0.13077209889888763\n",
      "Epoch 1501, train NLL 0.13453100621700287\n",
      "Epoch 1502, train NLL 0.1456116884946823\n",
      "Epoch 1503, train NLL 0.13220177590847015\n",
      "Epoch 1504, train NLL 0.13271595537662506\n",
      "Epoch 1505, train NLL 0.1327708661556244\n",
      "Epoch 1506, train NLL 0.13096341490745544\n",
      "Epoch 1507, train NLL 0.1307934820652008\n",
      "Epoch 1508, train NLL 0.14414680004119873\n",
      "Epoch 1509, train NLL 0.14780259132385254\n",
      "Epoch 1510, train NLL 0.13154195249080658\n",
      "Epoch 1511, train NLL 0.15160883963108063\n",
      "Epoch 1512, train NLL 0.13078433275222778\n",
      "Epoch 1513, train NLL 0.13638344407081604\n",
      "Epoch 1514, train NLL 0.1391112357378006\n",
      "Epoch 1515, train NLL 0.1311570405960083\n",
      "Epoch 1516, train NLL 0.14101387560367584\n",
      "Epoch 1517, train NLL 0.13195131719112396\n",
      "Epoch 1518, train NLL 0.13144387304782867\n",
      "Epoch 1519, train NLL 0.13244834542274475\n",
      "Epoch 1520, train NLL 0.13829627633094788\n",
      "Epoch 1521, train NLL 0.14027240872383118\n",
      "Epoch 1522, train NLL 0.1346026510000229\n",
      "Epoch 1523, train NLL 0.14838054776191711\n",
      "Epoch 1524, train NLL 0.13074113428592682\n",
      "Epoch 1525, train NLL 0.13116008043289185\n",
      "Epoch 1526, train NLL 0.13075928390026093\n",
      "Epoch 1527, train NLL 0.13201433420181274\n",
      "Epoch 1528, train NLL 0.1344814896583557\n",
      "Epoch 1529, train NLL 0.13771294057369232\n",
      "Epoch 1530, train NLL 0.1393527090549469\n",
      "Epoch 1531, train NLL 0.13140779733657837\n",
      "Epoch 1532, train NLL 0.13115179538726807\n",
      "Epoch 1533, train NLL 0.13310325145721436\n",
      "Epoch 1534, train NLL 0.13125376403331757\n",
      "Epoch 1535, train NLL 0.1402941644191742\n",
      "Epoch 1536, train NLL 0.13522152602672577\n",
      "Epoch 1537, train NLL 0.14106470346450806\n",
      "Epoch 1538, train NLL 0.13073664903640747\n",
      "Epoch 1539, train NLL 0.130740687251091\n",
      "Epoch 1540, train NLL 0.13339684903621674\n",
      "Epoch 1541, train NLL 0.1381245106458664\n",
      "Epoch 1542, train NLL 0.14816097915172577\n",
      "Epoch 1543, train NLL 0.13272614777088165\n",
      "Epoch 1544, train NLL 0.14420519769191742\n",
      "Epoch 1545, train NLL 0.1325041949748993\n",
      "Epoch 1546, train NLL 0.13093584775924683\n",
      "Epoch 1547, train NLL 0.1360253095626831\n",
      "Epoch 1548, train NLL 0.1381516456604004\n",
      "Epoch 1549, train NLL 0.14759811758995056\n",
      "Epoch 1550, train NLL 0.1307336688041687\n",
      "Epoch 1551, train NLL 0.1464501917362213\n",
      "Epoch 1552, train NLL 0.13626743853092194\n",
      "Epoch 1553, train NLL 0.13431993126869202\n",
      "Epoch 1554, train NLL 0.13210824131965637\n",
      "Epoch 1555, train NLL 0.13089802861213684\n",
      "Epoch 1556, train NLL 0.13163438439369202\n",
      "Epoch 1557, train NLL 0.13095220923423767\n",
      "Epoch 1558, train NLL 0.1327059119939804\n",
      "Epoch 1559, train NLL 0.1375643014907837\n",
      "Epoch 1560, train NLL 0.13187089562416077\n",
      "Epoch 1561, train NLL 0.14259976148605347\n",
      "Epoch 1562, train NLL 0.14014436304569244\n",
      "Epoch 1563, train NLL 0.14070025086402893\n",
      "Epoch 1564, train NLL 0.1400529146194458\n",
      "Epoch 1565, train NLL 0.13187164068222046\n",
      "Epoch 1566, train NLL 0.1311071366071701\n",
      "Epoch 1567, train NLL 0.13071222603321075\n",
      "Epoch 1568, train NLL 0.14415603876113892\n",
      "Epoch 1569, train NLL 0.13159750401973724\n",
      "Epoch 1570, train NLL 0.1440882533788681\n",
      "Epoch 1571, train NLL 0.1373201608657837\n",
      "Epoch 1572, train NLL 0.13527393341064453\n",
      "Epoch 1573, train NLL 0.13071249425411224\n",
      "Epoch 1574, train NLL 0.13177724182605743\n",
      "Epoch 1575, train NLL 0.1307249814271927\n",
      "Epoch 1576, train NLL 0.1312841773033142\n",
      "Epoch 1577, train NLL 0.13387790322303772\n",
      "Epoch 1578, train NLL 0.13126637041568756\n",
      "Epoch 1579, train NLL 0.13096535205841064\n",
      "Epoch 1580, train NLL 0.13088470697402954\n",
      "Epoch 1581, train NLL 0.13449956476688385\n",
      "Epoch 1582, train NLL 0.13155989348888397\n",
      "Epoch 1583, train NLL 0.14508545398712158\n",
      "Epoch 1584, train NLL 0.1311328113079071\n",
      "Epoch 1585, train NLL 0.13114504516124725\n",
      "Epoch 1586, train NLL 0.13080185651779175\n",
      "Epoch 1587, train NLL 0.1332649439573288\n",
      "Epoch 1588, train NLL 0.14228768646717072\n",
      "Epoch 1589, train NLL 0.13075602054595947\n",
      "Epoch 1590, train NLL 0.14532776176929474\n",
      "Epoch 1591, train NLL 0.13245709240436554\n",
      "Epoch 1592, train NLL 0.14727117121219635\n",
      "Epoch 1593, train NLL 0.13293033838272095\n",
      "Epoch 1594, train NLL 0.1508922576904297\n",
      "Epoch 1595, train NLL 0.13129739463329315\n",
      "Epoch 1596, train NLL 0.13121362030506134\n",
      "Epoch 1597, train NLL 0.13657671213150024\n",
      "Epoch 1598, train NLL 0.13077309727668762\n",
      "Epoch 1599, train NLL 0.13073424994945526\n",
      "Epoch 1600, train NLL 0.13581523299217224\n",
      "Epoch 1601, train NLL 0.1321202516555786\n",
      "Epoch 1602, train NLL 0.137030690908432\n",
      "Epoch 1603, train NLL 0.1307813972234726\n",
      "Epoch 1604, train NLL 0.13075274229049683\n",
      "Epoch 1605, train NLL 0.13076555728912354\n",
      "Epoch 1606, train NLL 0.1316230446100235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1607, train NLL 0.1429332196712494\n",
      "Epoch 1608, train NLL 0.1310090869665146\n",
      "Epoch 1609, train NLL 0.1330127716064453\n",
      "Epoch 1610, train NLL 0.13133932650089264\n",
      "Epoch 1611, train NLL 0.13085275888442993\n",
      "Epoch 1612, train NLL 0.13109111785888672\n",
      "Epoch 1613, train NLL 0.13310183584690094\n",
      "Epoch 1614, train NLL 0.13198722898960114\n",
      "Epoch 1615, train NLL 0.1315641552209854\n",
      "Epoch 1616, train NLL 0.14690791070461273\n",
      "Epoch 1617, train NLL 0.13334514200687408\n",
      "Epoch 1618, train NLL 0.13404636085033417\n",
      "Epoch 1619, train NLL 0.130755215883255\n",
      "Epoch 1620, train NLL 0.13931646943092346\n",
      "Epoch 1621, train NLL 0.1308329701423645\n",
      "Epoch 1622, train NLL 0.13685236871242523\n",
      "Epoch 1623, train NLL 0.13350050151348114\n",
      "Epoch 1624, train NLL 0.13771846890449524\n",
      "Epoch 1625, train NLL 0.14073067903518677\n",
      "Epoch 1626, train NLL 0.13096489012241364\n",
      "Epoch 1627, train NLL 0.13858889043331146\n",
      "Epoch 1628, train NLL 0.1315440833568573\n",
      "Epoch 1629, train NLL 0.13089227676391602\n",
      "Epoch 1630, train NLL 0.13509853184223175\n",
      "Epoch 1631, train NLL 0.13362379372119904\n",
      "Epoch 1632, train NLL 0.13341329991817474\n",
      "Epoch 1633, train NLL 0.13136939704418182\n",
      "Epoch 1634, train NLL 0.13271251320838928\n",
      "Epoch 1635, train NLL 0.13450580835342407\n",
      "Epoch 1636, train NLL 0.130929097533226\n",
      "Epoch 1637, train NLL 0.13068217039108276\n",
      "Epoch 1638, train NLL 0.13078081607818604\n",
      "Epoch 1639, train NLL 0.13097669184207916\n",
      "Epoch 1640, train NLL 0.15867739915847778\n",
      "Epoch 1641, train NLL 0.1320911943912506\n",
      "Epoch 1642, train NLL 0.13869206607341766\n",
      "Epoch 1643, train NLL 0.1306794285774231\n",
      "Epoch 1644, train NLL 0.13097237050533295\n",
      "Epoch 1645, train NLL 0.131214901804924\n",
      "Epoch 1646, train NLL 0.13174130022525787\n",
      "Epoch 1647, train NLL 0.13924464583396912\n",
      "Epoch 1648, train NLL 0.1439538300037384\n",
      "Epoch 1649, train NLL 0.13681232929229736\n",
      "Epoch 1650, train NLL 0.13930724561214447\n",
      "Epoch 1651, train NLL 0.13173571228981018\n",
      "Epoch 1652, train NLL 0.13442255556583405\n",
      "Epoch 1653, train NLL 0.13346263766288757\n",
      "Epoch 1654, train NLL 0.13641636073589325\n",
      "Epoch 1655, train NLL 0.131154865026474\n",
      "Epoch 1656, train NLL 0.13294367492198944\n",
      "Epoch 1657, train NLL 0.13157802820205688\n",
      "Epoch 1658, train NLL 0.13269181549549103\n",
      "Epoch 1659, train NLL 0.13790325820446014\n",
      "Epoch 1660, train NLL 0.13102960586547852\n",
      "Epoch 1661, train NLL 0.13087067008018494\n",
      "Epoch 1662, train NLL 0.13174733519554138\n",
      "Epoch 1663, train NLL 0.13934655487537384\n",
      "Epoch 1664, train NLL 0.13152799010276794\n",
      "Epoch 1665, train NLL 0.13473305106163025\n",
      "Epoch 1666, train NLL 0.1373390555381775\n",
      "Epoch 1667, train NLL 0.1372593343257904\n",
      "Epoch 1668, train NLL 0.14461451768875122\n",
      "Epoch 1669, train NLL 0.13071808218955994\n",
      "Epoch 1670, train NLL 0.1315632313489914\n",
      "Epoch 1671, train NLL 0.1316973716020584\n",
      "Epoch 1672, train NLL 0.13146716356277466\n",
      "Epoch 1673, train NLL 0.13079921901226044\n",
      "Epoch 1674, train NLL 0.1329931765794754\n",
      "Epoch 1675, train NLL 0.13952381908893585\n",
      "Epoch 1676, train NLL 0.13158872723579407\n",
      "Epoch 1677, train NLL 0.13083752989768982\n",
      "Epoch 1678, train NLL 0.134639173746109\n",
      "Epoch 1679, train NLL 0.1306958794593811\n",
      "Epoch 1680, train NLL 0.13354147970676422\n",
      "Epoch 1681, train NLL 0.13219796121120453\n",
      "Epoch 1682, train NLL 0.1309046745300293\n",
      "Epoch 1683, train NLL 0.13090817630290985\n",
      "Epoch 1684, train NLL 0.1377105861902237\n",
      "Epoch 1685, train NLL 0.13374224305152893\n",
      "Epoch 1686, train NLL 0.1323845088481903\n",
      "Epoch 1687, train NLL 0.1407463401556015\n",
      "Epoch 1688, train NLL 0.13878150284290314\n",
      "Epoch 1689, train NLL 0.1308603286743164\n",
      "Epoch 1690, train NLL 0.13669992983341217\n",
      "Epoch 1691, train NLL 0.1311693787574768\n",
      "Epoch 1692, train NLL 0.1311715841293335\n",
      "Epoch 1693, train NLL 0.13122797012329102\n",
      "Epoch 1694, train NLL 0.1314629316329956\n",
      "Epoch 1695, train NLL 0.1393362283706665\n",
      "Epoch 1696, train NLL 0.13831675052642822\n",
      "Epoch 1697, train NLL 0.13535070419311523\n",
      "Epoch 1698, train NLL 0.1311686933040619\n",
      "Epoch 1699, train NLL 0.13334575295448303\n",
      "Epoch 1700, train NLL 0.13618916273117065\n",
      "Epoch 1701, train NLL 0.13288848102092743\n",
      "Epoch 1702, train NLL 0.13269811868667603\n",
      "Epoch 1703, train NLL 0.1306719183921814\n",
      "Epoch 1704, train NLL 0.13696515560150146\n",
      "Epoch 1705, train NLL 0.1497315913438797\n",
      "Epoch 1706, train NLL 0.13574431836605072\n",
      "Epoch 1707, train NLL 0.13445855677127838\n",
      "Epoch 1708, train NLL 0.13602976500988007\n",
      "Epoch 1709, train NLL 0.1384550929069519\n",
      "Epoch 1710, train NLL 0.1314973384141922\n",
      "Epoch 1711, train NLL 0.1446477621793747\n",
      "Epoch 1712, train NLL 0.13068611919879913\n",
      "Epoch 1713, train NLL 0.13066184520721436\n",
      "Epoch 1714, train NLL 0.13167013227939606\n",
      "Epoch 1715, train NLL 0.13768163323402405\n",
      "Epoch 1716, train NLL 0.13830257952213287\n",
      "Epoch 1717, train NLL 0.1319313794374466\n",
      "Epoch 1718, train NLL 0.13222438097000122\n",
      "Epoch 1719, train NLL 0.13076147437095642\n",
      "Epoch 1720, train NLL 0.14059413969516754\n",
      "Epoch 1721, train NLL 0.13222502171993256\n",
      "Epoch 1722, train NLL 0.14054496586322784\n",
      "Epoch 1723, train NLL 0.13131262362003326\n",
      "Epoch 1724, train NLL 0.14115573465824127\n",
      "Epoch 1725, train NLL 0.13227631151676178\n",
      "Epoch 1726, train NLL 0.1340968757867813\n",
      "Epoch 1727, train NLL 0.13242772221565247\n",
      "Epoch 1728, train NLL 0.13137286901474\n",
      "Epoch 1729, train NLL 0.13067123293876648\n",
      "Epoch 1730, train NLL 0.13325870037078857\n",
      "Epoch 1731, train NLL 0.1309291124343872\n",
      "Epoch 1732, train NLL 0.14158156514167786\n",
      "Epoch 1733, train NLL 0.1307114213705063\n",
      "Epoch 1734, train NLL 0.13284265995025635\n",
      "Epoch 1735, train NLL 0.1340959519147873\n",
      "Epoch 1736, train NLL 0.13256649672985077\n",
      "Epoch 1737, train NLL 0.13319100439548492\n",
      "Epoch 1738, train NLL 0.1309853047132492\n",
      "Epoch 1739, train NLL 0.13312633335590363\n",
      "Epoch 1740, train NLL 0.1364389806985855\n",
      "Epoch 1741, train NLL 0.13520967960357666\n",
      "Epoch 1742, train NLL 0.14940790832042694\n",
      "Epoch 1743, train NLL 0.1398516297340393\n",
      "Epoch 1744, train NLL 0.14124451577663422\n",
      "Epoch 1745, train NLL 0.13246943056583405\n",
      "Epoch 1746, train NLL 0.13348864018917084\n",
      "Epoch 1747, train NLL 0.13714686036109924\n",
      "Epoch 1748, train NLL 0.1307106912136078\n",
      "Epoch 1749, train NLL 0.13381904363632202\n",
      "Epoch 1750, train NLL 0.13488426804542542\n",
      "Epoch 1751, train NLL 0.13109692931175232\n",
      "Epoch 1752, train NLL 0.13875558972358704\n",
      "Epoch 1753, train NLL 0.13225679099559784\n",
      "Epoch 1754, train NLL 0.13396835327148438\n",
      "Epoch 1755, train NLL 0.14243434369564056\n",
      "Epoch 1756, train NLL 0.13072222471237183\n",
      "Epoch 1757, train NLL 0.14089946448802948\n",
      "Epoch 1758, train NLL 0.13156557083129883\n",
      "Epoch 1759, train NLL 0.13177698850631714\n",
      "Epoch 1760, train NLL 0.13067558407783508\n",
      "Epoch 1761, train NLL 0.14355455338954926\n",
      "Epoch 1762, train NLL 0.1343185305595398\n",
      "Epoch 1763, train NLL 0.13160160183906555\n",
      "Epoch 1764, train NLL 0.140422061085701\n",
      "Epoch 1765, train NLL 0.13126219809055328\n",
      "Epoch 1766, train NLL 0.13831181824207306\n",
      "Epoch 1767, train NLL 0.1313118040561676\n",
      "Epoch 1768, train NLL 0.13240239024162292\n",
      "Epoch 1769, train NLL 0.13263557851314545\n",
      "Epoch 1770, train NLL 0.13350920379161835\n",
      "Epoch 1771, train NLL 0.13907566666603088\n",
      "Epoch 1772, train NLL 0.13065332174301147\n",
      "Epoch 1773, train NLL 0.13578815758228302\n",
      "Epoch 1774, train NLL 0.13067547976970673\n",
      "Epoch 1775, train NLL 0.13302505016326904\n",
      "Epoch 1776, train NLL 0.1328650563955307\n",
      "Epoch 1777, train NLL 0.13069812953472137\n",
      "Epoch 1778, train NLL 0.1315397024154663\n",
      "Epoch 1779, train NLL 0.13122369349002838\n",
      "Epoch 1780, train NLL 0.13651256263256073\n",
      "Epoch 1781, train NLL 0.13237690925598145\n",
      "Epoch 1782, train NLL 0.1320735514163971\n",
      "Epoch 1783, train NLL 0.1337001919746399\n",
      "Epoch 1784, train NLL 0.13122615218162537\n",
      "Epoch 1785, train NLL 0.1307263821363449\n",
      "Epoch 1786, train NLL 0.1314743161201477\n",
      "Epoch 1787, train NLL 0.14091180264949799\n",
      "Epoch 1788, train NLL 0.13075704872608185\n",
      "Epoch 1789, train NLL 0.13078543543815613\n",
      "Epoch 1790, train NLL 0.13310550153255463\n",
      "Epoch 1791, train NLL 0.13736845552921295\n",
      "Epoch 1792, train NLL 0.13207900524139404\n",
      "Epoch 1793, train NLL 0.13073940575122833\n",
      "Epoch 1794, train NLL 0.13187849521636963\n",
      "Epoch 1795, train NLL 0.13063710927963257\n",
      "Epoch 1796, train NLL 0.13182130455970764\n",
      "Epoch 1797, train NLL 0.13084709644317627\n",
      "Epoch 1798, train NLL 0.1328483372926712\n",
      "Epoch 1799, train NLL 0.13072198629379272\n",
      "Epoch 1800, train NLL 0.1459750235080719\n",
      "Epoch 1801, train NLL 0.16163130104541779\n",
      "Epoch 1802, train NLL 0.13165724277496338\n",
      "Epoch 1803, train NLL 0.13092301785945892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1804, train NLL 0.13498657941818237\n",
      "Epoch 1805, train NLL 0.1323242038488388\n",
      "Epoch 1806, train NLL 0.13072218000888824\n",
      "Epoch 1807, train NLL 0.1546664834022522\n",
      "Epoch 1808, train NLL 0.13513103127479553\n",
      "Epoch 1809, train NLL 0.1321146935224533\n",
      "Epoch 1810, train NLL 0.13422806560993195\n",
      "Epoch 1811, train NLL 0.13108697533607483\n",
      "Epoch 1812, train NLL 0.13662713766098022\n",
      "Epoch 1813, train NLL 0.13359177112579346\n",
      "Epoch 1814, train NLL 0.1308627724647522\n",
      "Epoch 1815, train NLL 0.1371617615222931\n",
      "Epoch 1816, train NLL 0.13077111542224884\n",
      "Epoch 1817, train NLL 0.13803283870220184\n",
      "Epoch 1818, train NLL 0.13069887459278107\n",
      "Epoch 1819, train NLL 0.13264505565166473\n",
      "Epoch 1820, train NLL 0.1307155042886734\n",
      "Epoch 1821, train NLL 0.1309652179479599\n",
      "Epoch 1822, train NLL 0.13776201009750366\n",
      "Epoch 1823, train NLL 0.13928623497486115\n",
      "Epoch 1824, train NLL 0.13753364980220795\n",
      "Epoch 1825, train NLL 0.13481466472148895\n",
      "Epoch 1826, train NLL 0.1325581967830658\n",
      "Epoch 1827, train NLL 0.13819260895252228\n",
      "Epoch 1828, train NLL 0.13116344809532166\n",
      "Epoch 1829, train NLL 0.13171061873435974\n",
      "Epoch 1830, train NLL 0.13181175291538239\n",
      "Epoch 1831, train NLL 0.1315128654241562\n",
      "Epoch 1832, train NLL 0.13155096769332886\n",
      "Epoch 1833, train NLL 0.14427457749843597\n",
      "Epoch 1834, train NLL 0.13063810765743256\n",
      "Epoch 1835, train NLL 0.13066576421260834\n",
      "Epoch 1836, train NLL 0.13246040046215057\n",
      "Epoch 1837, train NLL 0.13152897357940674\n",
      "Epoch 1838, train NLL 0.13063660264015198\n",
      "Epoch 1839, train NLL 0.13088615238666534\n",
      "Epoch 1840, train NLL 0.13166426122188568\n",
      "Epoch 1841, train NLL 0.1316060572862625\n",
      "Epoch 1842, train NLL 0.1491462141275406\n",
      "Epoch 1843, train NLL 0.13078828155994415\n",
      "Epoch 1844, train NLL 0.13354912400245667\n",
      "Epoch 1845, train NLL 0.14506621658802032\n",
      "Epoch 1846, train NLL 0.14513616263866425\n",
      "Epoch 1847, train NLL 0.13242202997207642\n",
      "Epoch 1848, train NLL 0.13604363799095154\n",
      "Epoch 1849, train NLL 0.13106684386730194\n",
      "Epoch 1850, train NLL 0.13114403188228607\n",
      "Epoch 1851, train NLL 0.1350189447402954\n",
      "Epoch 1852, train NLL 0.13134831190109253\n",
      "Epoch 1853, train NLL 0.13356788456439972\n",
      "Epoch 1854, train NLL 0.1317453235387802\n",
      "Epoch 1855, train NLL 0.13190941512584686\n",
      "Epoch 1856, train NLL 0.13833104074001312\n",
      "Epoch 1857, train NLL 0.1384345144033432\n",
      "Epoch 1858, train NLL 0.13134561479091644\n",
      "Epoch 1859, train NLL 0.13070660829544067\n",
      "Epoch 1860, train NLL 0.13138911128044128\n",
      "Epoch 1861, train NLL 0.1307171881198883\n",
      "Epoch 1862, train NLL 0.13431960344314575\n",
      "Epoch 1863, train NLL 0.13062813878059387\n",
      "Epoch 1864, train NLL 0.13064850866794586\n",
      "Epoch 1865, train NLL 0.13292628526687622\n",
      "Epoch 1866, train NLL 0.13185694813728333\n",
      "Epoch 1867, train NLL 0.13160297274589539\n",
      "Epoch 1868, train NLL 0.1326533406972885\n",
      "Epoch 1869, train NLL 0.13439618051052094\n",
      "Epoch 1870, train NLL 0.1421387642621994\n",
      "Epoch 1871, train NLL 0.13437579572200775\n",
      "Epoch 1872, train NLL 0.13070973753929138\n",
      "Epoch 1873, train NLL 0.1356593519449234\n",
      "Epoch 1874, train NLL 0.13594654202461243\n",
      "Epoch 1875, train NLL 0.1442985087633133\n",
      "Epoch 1876, train NLL 0.13259965181350708\n",
      "Epoch 1877, train NLL 0.13728627562522888\n",
      "Epoch 1878, train NLL 0.1309901773929596\n",
      "Epoch 1879, train NLL 0.1306917518377304\n",
      "Epoch 1880, train NLL 0.13067279756069183\n",
      "Epoch 1881, train NLL 0.1309501826763153\n",
      "Epoch 1882, train NLL 0.14737410843372345\n",
      "Epoch 1883, train NLL 0.1522412747144699\n",
      "Epoch 1884, train NLL 0.13062536716461182\n",
      "Epoch 1885, train NLL 0.13069039583206177\n",
      "Epoch 1886, train NLL 0.14125899970531464\n",
      "Epoch 1887, train NLL 0.13205666840076447\n",
      "Epoch 1888, train NLL 0.14532992243766785\n",
      "Epoch 1889, train NLL 0.15864509344100952\n",
      "Epoch 1890, train NLL 0.13063710927963257\n",
      "Epoch 1891, train NLL 0.154636949300766\n",
      "Epoch 1892, train NLL 0.13611914217472076\n",
      "Epoch 1893, train NLL 0.13154713809490204\n",
      "Epoch 1894, train NLL 0.13065145909786224\n",
      "Epoch 1895, train NLL 0.13066352903842926\n",
      "Epoch 1896, train NLL 0.13069652020931244\n",
      "Epoch 1897, train NLL 0.13388055562973022\n",
      "Epoch 1898, train NLL 0.14292535185813904\n",
      "Epoch 1899, train NLL 0.13115042448043823\n",
      "Epoch 1900, train NLL 0.13181042671203613\n",
      "Epoch 1901, train NLL 0.13172240555286407\n",
      "Epoch 1902, train NLL 0.13208027184009552\n",
      "Epoch 1903, train NLL 0.13336187601089478\n",
      "Epoch 1904, train NLL 0.13064999878406525\n",
      "Epoch 1905, train NLL 0.13157488405704498\n",
      "Epoch 1906, train NLL 0.14811377227306366\n",
      "Epoch 1907, train NLL 0.13192592561244965\n",
      "Epoch 1908, train NLL 0.13448181748390198\n",
      "Epoch 1909, train NLL 0.13203833997249603\n",
      "Epoch 1910, train NLL 0.13063567876815796\n",
      "Epoch 1911, train NLL 0.13075247406959534\n",
      "Epoch 1912, train NLL 0.1375970095396042\n",
      "Epoch 1913, train NLL 0.13142038881778717\n",
      "Epoch 1914, train NLL 0.13064897060394287\n",
      "Epoch 1915, train NLL 0.1311895102262497\n",
      "Epoch 1916, train NLL 0.13732121884822845\n",
      "Epoch 1917, train NLL 0.13071852922439575\n",
      "Epoch 1918, train NLL 0.13088488578796387\n",
      "Epoch 1919, train NLL 0.1400269865989685\n",
      "Epoch 1920, train NLL 0.13143222033977509\n",
      "Epoch 1921, train NLL 0.14239312708377838\n",
      "Epoch 1922, train NLL 0.1362326443195343\n",
      "Epoch 1923, train NLL 0.13643310964107513\n",
      "Epoch 1924, train NLL 0.13066884875297546\n",
      "Epoch 1925, train NLL 0.13086636364459991\n",
      "Epoch 1926, train NLL 0.13095048069953918\n",
      "Epoch 1927, train NLL 0.1307128369808197\n",
      "Epoch 1928, train NLL 0.13941575586795807\n",
      "Epoch 1929, train NLL 0.13096630573272705\n",
      "Epoch 1930, train NLL 0.13404341042041779\n",
      "Epoch 1931, train NLL 0.1314644068479538\n",
      "Epoch 1932, train NLL 0.13696423172950745\n",
      "Epoch 1933, train NLL 0.14586329460144043\n",
      "Epoch 1934, train NLL 0.13114303350448608\n",
      "Epoch 1935, train NLL 0.14237375557422638\n",
      "Epoch 1936, train NLL 0.13116991519927979\n",
      "Epoch 1937, train NLL 0.1418875902891159\n",
      "Epoch 1938, train NLL 0.1429256796836853\n",
      "Epoch 1939, train NLL 0.1324542760848999\n",
      "Epoch 1940, train NLL 0.13433465361595154\n",
      "Epoch 1941, train NLL 0.14543381333351135\n",
      "Epoch 1942, train NLL 0.1312437206506729\n",
      "Epoch 1943, train NLL 0.1315954178571701\n",
      "Epoch 1944, train NLL 0.1307181864976883\n",
      "Epoch 1945, train NLL 0.1312667429447174\n",
      "Epoch 1946, train NLL 0.13412590324878693\n",
      "Epoch 1947, train NLL 0.1311749964952469\n",
      "Epoch 1948, train NLL 0.1314837485551834\n",
      "Epoch 1949, train NLL 0.13887324929237366\n",
      "Epoch 1950, train NLL 0.1306811422109604\n",
      "Epoch 1951, train NLL 0.13277371227741241\n",
      "Epoch 1952, train NLL 0.14418841898441315\n",
      "Epoch 1953, train NLL 0.13311800360679626\n",
      "Epoch 1954, train NLL 0.1340840458869934\n",
      "Epoch 1955, train NLL 0.13299444317817688\n",
      "Epoch 1956, train NLL 0.13065212965011597\n",
      "Epoch 1957, train NLL 0.13097676634788513\n",
      "Epoch 1958, train NLL 0.1361204981803894\n",
      "Epoch 1959, train NLL 0.13093024492263794\n",
      "Epoch 1960, train NLL 0.14357414841651917\n",
      "Epoch 1961, train NLL 0.13062028586864471\n",
      "Epoch 1962, train NLL 0.13493096828460693\n",
      "Epoch 1963, train NLL 0.13066977262496948\n",
      "Epoch 1964, train NLL 0.13084259629249573\n",
      "Epoch 1965, train NLL 0.13070979714393616\n",
      "Epoch 1966, train NLL 0.1307355761528015\n",
      "Epoch 1967, train NLL 0.13901157677173615\n",
      "Epoch 1968, train NLL 0.13244982063770294\n",
      "Epoch 1969, train NLL 0.13425937294960022\n",
      "Epoch 1970, train NLL 0.13073702156543732\n",
      "Epoch 1971, train NLL 0.13249528408050537\n",
      "Epoch 1972, train NLL 0.13061052560806274\n",
      "Epoch 1973, train NLL 0.1317799985408783\n",
      "Epoch 1974, train NLL 0.1308111697435379\n",
      "Epoch 1975, train NLL 0.13216239213943481\n",
      "Epoch 1976, train NLL 0.13065265119075775\n",
      "Epoch 1977, train NLL 0.1308431476354599\n",
      "Epoch 1978, train NLL 0.13298428058624268\n",
      "Epoch 1979, train NLL 0.1324232518672943\n",
      "Epoch 1980, train NLL 0.13842056691646576\n",
      "Epoch 1981, train NLL 0.1486165076494217\n",
      "Epoch 1982, train NLL 0.13620011508464813\n",
      "Epoch 1983, train NLL 0.1358352154493332\n",
      "Epoch 1984, train NLL 0.1362893432378769\n",
      "Epoch 1985, train NLL 0.1312176138162613\n",
      "Epoch 1986, train NLL 0.13061712682247162\n",
      "Epoch 1987, train NLL 0.13273656368255615\n",
      "Epoch 1988, train NLL 0.13529561460018158\n",
      "Epoch 1989, train NLL 0.13314048945903778\n",
      "Epoch 1990, train NLL 0.13075323402881622\n",
      "Epoch 1991, train NLL 0.13192513585090637\n",
      "Epoch 1992, train NLL 0.13062258064746857\n",
      "Epoch 1993, train NLL 0.13834373652935028\n",
      "Epoch 1994, train NLL 0.13080258667469025\n",
      "Epoch 1995, train NLL 0.13397257030010223\n",
      "Epoch 1996, train NLL 0.13085921108722687\n",
      "Epoch 1997, train NLL 0.13183817267417908\n",
      "Epoch 1998, train NLL 0.13077162206172943\n",
      "Epoch 1999, train NLL 0.13952139019966125\n",
      "[-5.325 -4.805  9.687]\n",
      "[0.8947761654853821, 0.4824911952018738, 0.4509454667568207, 0.4528477191925049, 0.8981762528419495, 0.5361756086349487, 0.3979598581790924, 0.3691052496433258, 0.46815353631973267, 0.3966706097126007, 0.36759117245674133, 0.39221227169036865, 0.3439350128173828, 0.3351922333240509, 0.3376809358596802, 0.31386876106262207, 0.3150806725025177, 0.348689466714859, 0.31649914383888245, 0.3438558280467987, 0.2909436821937561, 0.2879621088504791, 0.2820524275302887, 0.3101750910282135, 0.28492245078086853, 0.3714085519313812, 0.27555203437805176, 0.2660960853099823, 0.2631305456161499, 0.27020037174224854, 0.334455668926239, 0.25727248191833496, 0.2802165448665619, 0.26199352741241455, 0.24892841279506683, 0.2811424732208252, 0.24817445874214172, 0.24715225398540497, 0.2466714233160019, 0.3042636811733246, 0.239762082695961, 0.2385147511959076, 0.24040529131889343, 0.23272697627544403, 0.23069815337657928, 0.236114040017128, 0.2885432541370392, 0.23225350677967072, 0.22992408275604248, 0.22491854429244995, 0.2286291867494583, 0.22543612122535706, 0.2235066294670105, 0.21741239726543427, 0.21763162314891815, 0.2920655906200409, 0.2813718914985657, 0.220219686627388, 0.21237613260746002, 0.21042530238628387, 0.2109304964542389, 0.22182431817054749, 0.25304096937179565, 0.20946654677391052, 0.27163058519363403, 0.20933417975902557, 0.21246597170829773, 0.2091362178325653, 0.2053808718919754, 0.20730488002300262, 0.22066721320152283, 0.21628160774707794, 0.21179033815860748, 0.22777605056762695, 0.2168767899274826, 0.20367896556854248, 0.19525325298309326, 0.21592290699481964, 0.20874062180519104, 0.1936051994562149, 0.19232060015201569, 0.19535943865776062, 0.2023833841085434, 0.1958199441432953, 0.1893857717514038, 0.18880225718021393, 0.1949765831232071, 0.18783828616142273, 0.2133907824754715, 0.18921393156051636, 0.19776448607444763, 0.18519653379917145, 0.18871736526489258, 0.19792991876602173, 0.1859096884727478, 0.18444938957691193, 0.2936038374900818, 0.1883985698223114, 0.19452641904354095, 0.18879851698875427, 0.18823175132274628, 0.1860133558511734, 0.184955894947052, 0.19589298963546753, 0.1785244345664978, 0.1794901341199875, 0.18334037065505981, 0.1794460117816925, 0.17886123061180115, 0.1796283721923828, 0.17628218233585358, 0.17969505488872528, 0.20117363333702087, 0.17530155181884766, 0.17594289779663086, 0.1748616099357605, 0.1792544424533844, 0.18107476830482483, 0.1872270554304123, 0.17242133617401123, 0.1739545315504074, 0.17153428494930267, 0.17183339595794678, 0.1720171719789505, 0.17974157631397247, 0.19102083146572113, 0.18054097890853882, 0.17861776053905487, 0.16913659870624542, 0.1739560067653656, 0.17159461975097656, 0.16879838705062866, 0.16964216530323029, 0.17272338271141052, 0.16915041208267212, 0.17364105582237244, 0.20441438257694244, 0.16664770245552063, 0.16754847764968872, 0.16592271625995636, 0.16743579506874084, 0.18674150109291077, 0.16583143174648285, 0.19117394089698792, 0.17417651414871216, 0.17364679276943207, 0.16871678829193115, 0.16635777056217194, 0.1784692257642746, 0.166129931807518, 0.1632494330406189, 0.16266870498657227, 0.1639423966407776, 0.16243234276771545, 0.16768163442611694, 0.16694091260433197, 0.16150453686714172, 0.16647255420684814, 0.17978893220424652, 0.16346019506454468, 0.16191184520721436, 0.16152271628379822, 0.1627214401960373, 0.16541215777397156, 0.16654638946056366, 0.17642731964588165, 0.1689227819442749, 0.19581495225429535, 0.2570040225982666, 0.16063424944877625, 0.16617979109287262, 0.16332274675369263, 0.16109023988246918, 0.15980204939842224, 0.15762494504451752, 0.16897210478782654, 0.18079786002635956, 0.16726751625537872, 0.16633857786655426, 0.1619562953710556, 0.15650144219398499, 0.18166078627109528, 0.16505485773086548, 0.16391488909721375, 0.1563694328069687, 0.16040915250778198, 0.15554705262184143, 0.1554422527551651, 0.17071887850761414, 0.165578693151474, 0.17453324794769287, 0.15844495594501495, 0.17386314272880554, 0.1596575379371643, 0.1545742005109787, 0.17078697681427002, 0.1572847217321396, 0.15430240333080292, 0.1536901891231537, 0.15800078213214874, 0.15348516404628754, 0.1532420516014099, 0.1629810333251953, 0.15345823764801025, 0.15287059545516968, 0.1590842455625534, 0.18633180856704712, 0.18721412122249603, 0.1646568924188614, 0.15484680235385895, 0.16276364028453827, 0.15241259336471558, 0.15300457179546356, 0.1515686959028244, 0.16509757936000824, 0.16148896515369415, 0.1519506573677063, 0.15625137090682983, 0.15091173350811005, 0.15137307345867157, 0.15132762491703033, 0.1574711799621582, 0.15231293439865112, 0.16328200697898865, 0.1506117284297943, 0.15034520626068115, 0.15724167227745056, 0.14984656870365143, 0.14972393214702606, 0.14963938295841217, 0.1496841311454773, 0.15892404317855835, 0.15416841208934784, 0.1639273464679718, 0.15022531151771545, 0.15715590119361877, 0.14886684715747833, 0.15455389022827148, 0.15650565922260284, 0.14857117831707, 0.15376879274845123, 0.14836987853050232, 0.17311973869800568, 0.1512061506509781, 0.1647668331861496, 0.14867323637008667, 0.15845388174057007, 0.15867967903614044, 0.1567123830318451, 0.15076252818107605, 0.1591021567583084, 0.18393228948116302, 0.14773404598236084, 0.14970465004444122, 0.14928755164146423, 0.15853525698184967, 0.14730459451675415, 0.14745116233825684, 0.15620841085910797, 0.14722852408885956, 0.157900869846344, 0.15652106702327728, 0.1479092240333557, 0.16955448687076569, 0.14687535166740417, 0.17688623070716858, 0.14778445661067963, 0.15532808005809784, 0.16581465303897858, 0.14698205888271332, 0.1472371220588684, 0.16378702223300934, 0.15856127440929413, 0.14997650682926178, 0.14539067447185516, 0.1465667188167572, 0.1526610404253006, 0.1586940437555313, 0.14568868279457092, 0.14510513842105865, 0.14534899592399597, 0.14842970669269562, 0.15381760895252228, 0.14717285335063934, 0.14470785856246948, 0.15779359638690948, 0.15016666054725647, 0.14445632696151733, 0.15412446856498718, 0.1447286456823349, 0.1445588618516922, 0.14418835937976837, 0.14902283251285553, 0.18988855183124542, 0.15407724678516388, 0.15029534697532654, 0.14555047452449799, 0.15294213593006134, 0.14728672802448273, 0.14456157386302948, 0.15045441687107086, 0.14705616235733032, 0.15665008127689362, 0.14365337789058685, 0.14940756559371948, 0.15161897242069244, 0.16582277417182922, 0.1479550302028656, 0.1463186889886856, 0.1488507241010666, 0.14502692222595215, 0.1432015746831894, 0.1867367923259735, 0.15448981523513794, 0.1473766267299652, 0.14291226863861084, 0.15749883651733398, 0.14886169135570526, 0.14342020452022552, 0.14973896741867065, 0.15719082951545715, 0.14329953491687775, 0.14237023890018463, 0.14697663486003876, 0.14552700519561768, 0.1431635022163391, 0.14895448088645935, 0.14193451404571533, 0.14185050129890442, 0.146844744682312, 0.1418333351612091, 0.1470569521188736, 0.14229512214660645, 0.1513698846101761, 0.181928813457489, 0.14910274744033813, 0.1418086439371109, 0.14316721260547638, 0.14948756992816925, 0.1443968117237091, 0.15662971138954163, 0.14115923643112183, 0.1457729935646057, 0.17620840668678284, 0.141055628657341, 0.14107659459114075, 0.14137983322143555, 0.16762004792690277, 0.14268462359905243, 0.14105331897735596, 0.14587798714637756, 0.14287284016609192, 0.1424284130334854, 0.15240760147571564, 0.14765727519989014, 0.142545685172081, 0.1408117711544037, 0.1453731507062912, 0.14083531498908997, 0.15495431423187256, 0.1403295397758484, 0.14059031009674072, 0.14619846642017365, 0.1472715437412262, 0.14165352284908295, 0.14684545993804932, 0.13998478651046753, 0.14540988206863403, 0.140699103474617, 0.15939143300056458, 0.1454160064458847, 0.159879669547081, 0.14566963911056519, 0.13983826339244843, 0.1398358941078186, 0.1398359090089798, 0.1443185657262802, 0.13954228162765503, 0.139905646443367, 0.14346246421337128, 0.1493263840675354, 0.14428573846817017, 0.14309614896774292, 0.14894327521324158, 0.15299829840660095, 0.16723927855491638, 0.144594207406044, 0.14031197130680084, 0.13949038088321686, 0.14345881342887878, 0.1538548320531845, 0.13945403695106506, 0.13903068006038666, 0.13919003307819366, 0.14374969899654388, 0.14100150763988495, 0.13886140286922455, 0.1402813196182251, 0.1396946758031845, 0.1514630913734436, 0.1451580971479416, 0.1441916823387146, 0.1493956446647644, 0.13887318968772888, 0.15002015233039856, 0.1453702449798584, 0.14508359134197235, 0.1410275399684906, 0.16923058032989502, 0.13924555480480194, 0.1400509476661682, 0.14456646144390106, 0.13939201831817627, 0.14510685205459595, 0.15961337089538574, 0.1459842026233673, 0.17169107496738434, 0.14021776616573334, 0.1401820331811905, 0.13852208852767944, 0.150923952460289, 0.15743140876293182, 0.14411446452140808, 0.13942688703536987, 0.1423778086900711, 0.1515454798936844, 0.14561130106449127, 0.13775163888931274, 0.14315997064113617, 0.1440708488225937, 0.15946729481220245, 0.1529712826013565, 0.13900813460350037, 0.1376572549343109, 0.1935531497001648, 0.13810311257839203, 0.1376216560602188, 0.15407414734363556, 0.1397857815027237, 0.14043937623500824, 0.148426353931427, 0.14635328948497772, 0.1382765918970108, 0.14341308176517487, 0.13741426169872284, 0.16956105828285217, 0.1373118758201599, 0.1410684883594513, 0.147114560008049, 0.13865943253040314, 0.14726771414279938, 0.13865211606025696, 0.13981732726097107, 0.13842922449111938, 0.13941225409507751, 0.13712097704410553, 0.13922491669654846, 0.13883619010448456, 0.13693800568580627, 0.13699832558631897, 0.14650049805641174, 0.15392865240573883, 0.14091162383556366, 0.13748474419116974, 0.13774536550045013, 0.13930268585681915, 0.14034977555274963, 0.13664814829826355, 0.15823127329349518, 0.1372230052947998, 0.14976441860198975, 0.13759079575538635, 0.1385035663843155, 0.1371629685163498, 0.13673029839992523, 0.1655530482530594, 0.13665953278541565, 0.13689875602722168, 0.13645190000534058, 0.13806217908859253, 0.1480497121810913, 0.1412646621465683, 0.1372445523738861, 0.13699200749397278, 0.13649173080921173, 0.14168408513069153, 0.14171138405799866, 0.14253409206867218, 0.13889461755752563, 0.14590921998023987, 0.14614681899547577, 0.1375826895236969, 0.13723218441009521, 0.1376865655183792, 0.13640116155147552, 0.13708661496639252, 0.1403483897447586, 0.16241176426410675, 0.14643701910972595, 0.13593454658985138, 0.1374945342540741, 0.13591738045215607, 0.1368248015642166, 0.13588301837444305, 0.14035063982009888, 0.1378990113735199, 0.13605941832065582, 0.1358678638935089, 0.136726975440979, 0.1370011270046234, 0.1358920931816101, 0.13575351238250732, 0.1437942534685135, 0.13939358294010162, 0.13989375531673431, 0.13774052262306213, 0.13644738495349884, 0.139950692653656, 0.13698641955852509, 0.13568097352981567, 0.15757587552070618, 0.1357613354921341, 0.13560542464256287, 0.13557395339012146, 0.13700498640537262, 0.13720445334911346, 0.13688121736049652, 0.135980024933815, 0.13547265529632568, 0.13962195813655853, 0.2184477001428604, 0.13959906995296478, 0.144973024725914, 0.17918699979782104, 0.13541512191295624, 0.13768631219863892, 0.13729968667030334, 0.13544894754886627, 0.15148526430130005, 0.13648003339767456, 0.14484012126922607, 0.13511228561401367, 0.1561087667942047, 0.14965739846229553, 0.14073030650615692, 0.14003701508045197, 0.13543453812599182, 0.13503479957580566, 0.14232178032398224, 0.13502340018749237, 0.13754482567310333, 0.13506552577018738, 0.13492217659950256, 0.14114119112491608, 0.14202040433883667, 0.1359807550907135, 0.1349879950284958, 0.13654722273349762, 0.14552374184131622, 0.13655807077884674, 0.14021696150302887, 0.13724148273468018, 0.13488206267356873, 0.1405438482761383, 0.14940808713436127, 0.14736051857471466, 0.1386735588312149, 0.13471084833145142, 0.1396171897649765, 0.1433563381433487, 0.13748246431350708, 0.1378825157880783, 0.14777344465255737, 0.1429247260093689, 0.13458094000816345, 0.13474604487419128, 0.13790880143642426, 0.13543300330638885, 0.14867913722991943, 0.1421191543340683, 0.15187132358551025, 0.15160278975963593, 0.18877750635147095, 0.13465827703475952, 0.15612730383872986, 0.13500051200389862, 0.13971354067325592, 0.1352730691432953, 0.14314189553260803, 0.1497146189212799, 0.13453011214733124, 0.13446877896785736, 0.1343124806880951, 0.13937291502952576, 0.13900931179523468, 0.13435788452625275, 0.14321491122245789, 0.1342458724975586, 0.13445362448692322, 0.13433192670345306, 0.13447850942611694, 0.13640163838863373, 0.16809871792793274, 0.14468470215797424, 0.13581493496894836, 0.14674142003059387, 0.13465577363967896, 0.137776181101799, 0.13696859776973724, 0.13675624132156372, 0.1412438452243805, 0.1469709724187851, 0.1351913958787918, 0.14272460341453552, 0.1345098912715912, 0.1357787847518921, 0.13638199865818024, 0.1339971125125885, 0.13429498672485352, 0.13546636700630188, 0.1339806318283081, 0.1349126100540161, 0.13462679088115692, 0.1389562040567398, 0.1733931005001068, 0.13604310154914856, 0.15720678865909576, 0.13395866751670837, 0.1348501741886139, 0.13776586949825287, 0.1355365514755249, 0.13993515074253082, 0.14055489003658295, 0.14677004516124725, 0.13414545357227325, 0.14027351140975952, 0.13380469381809235, 0.13404354453086853, 0.13626892864704132, 0.1418302208185196, 0.14832496643066406, 0.14518991112709045, 0.13870735466480255, 0.15022005140781403, 0.13438841700553894, 0.13380467891693115, 0.13368752598762512, 0.13564233481884003, 0.13365420699119568, 0.13561278581619263, 0.133926659822464, 0.13632696866989136, 0.13803483545780182, 0.13428634405136108, 0.13395501673221588, 0.1337660849094391, 0.15029992163181305, 0.13832606375217438, 0.1349363774061203, 0.142250195145607, 0.13656479120254517, 0.1418115198612213, 0.140089750289917, 0.13891923427581787, 0.1414913535118103, 0.14838773012161255, 0.13636957108974457, 0.15609750151634216, 0.14806020259857178, 0.13444173336029053, 0.1344297230243683, 0.14278994500637054, 0.1362718790769577, 0.13397948443889618, 0.133759006857872, 0.13460862636566162, 0.14757677912712097, 0.1336074322462082, 0.14349038898944855, 0.13337667286396027, 0.13336805999279022, 0.1343861073255539, 0.13674652576446533, 0.13974928855895996, 0.15434600412845612, 0.13458001613616943, 0.1395190805196762, 0.1339205652475357, 0.1334492415189743, 0.13333170115947723, 0.1332986056804657, 0.13426493108272552, 0.1356549710035324, 0.13678404688835144, 0.13407394289970398, 0.13317938148975372, 0.13418839871883392, 0.1332128345966339, 0.13489669561386108, 0.13313958048820496, 0.13957904279232025, 0.14292117953300476, 0.13370190560817719, 0.1405612826347351, 0.13371865451335907, 0.13598482310771942, 0.13348856568336487, 0.13510017096996307, 0.13483992218971252, 0.13414490222930908, 0.14410704374313354, 0.13315778970718384, 0.13415895402431488, 0.14122945070266724, 0.14014019072055817, 0.13832233846187592, 0.13551363348960876, 0.13316060602664948, 0.13772247731685638, 0.13453085720539093, 0.1340484619140625, 0.1361369639635086, 0.13340520858764648, 0.13350379467010498, 0.1329306960105896, 0.1372012346982956, 0.1379227638244629, 0.14790648221969604, 0.14746643602848053, 0.1348172426223755, 0.1329152137041092, 0.1349298059940338, 0.13730774819850922, 0.13761144876480103, 0.13383272290229797, 0.15117517113685608, 0.1349881887435913, 0.13483260571956635, 0.14110909402370453, 0.15085893869400024, 0.13673801720142365, 0.13373038172721863, 0.15121889114379883, 0.13291755318641663, 0.1363559514284134, 0.13552454113960266, 0.1359698325395584, 0.13273116946220398, 0.13365888595581055, 0.13628675043582916, 0.1356096714735031, 0.13904260098934174, 0.13568933308124542, 0.1353912502527237, 0.13387373089790344, 0.14012576639652252, 0.1368296593427658, 0.13359247148036957, 0.15644502639770508, 0.13820503652095795, 0.13263626396656036, 0.13573934137821198, 0.1326739341020584, 0.1345566213130951, 0.13553158938884735, 0.14338389039039612, 0.132650688290596, 0.13474445044994354, 0.13416004180908203, 0.1329902708530426, 0.13369104266166687, 0.13930580019950867, 0.13262537121772766, 0.13678409159183502, 0.13255104422569275, 0.1375211626291275, 0.13953204452991486, 0.13316667079925537, 0.13321048021316528, 0.1341007500886917, 0.1332450807094574, 0.13725721836090088, 0.13318976759910583, 0.1336926519870758, 0.13495881855487823, 0.13278624415397644, 0.13575288653373718, 0.13309821486473083, 0.1338401436805725, 0.13557833433151245, 0.13820403814315796, 0.1394745111465454, 0.13337883353233337, 0.1423872411251068, 0.1425221562385559, 0.14070357382297516, 0.13253404200077057, 0.1356768161058426, 0.13342304527759552, 0.13259176909923553, 0.13325057923793793, 0.1335424929857254, 0.13605445623397827, 0.1357206404209137, 0.14255879819393158, 0.13268403708934784, 0.13286477327346802, 0.13246378302574158, 0.13533884286880493, 0.13678814470767975, 0.13592639565467834, 0.1335136741399765, 0.1326725035905838, 0.1348489671945572, 0.13963404297828674, 0.13420981168746948, 0.1325499564409256, 0.14294621348381042, 0.13332805037498474, 0.13619403541088104, 0.16551277041435242, 0.1340806782245636, 0.13416951894760132, 0.13639084994792938, 0.13287357985973358, 0.13326039910316467, 0.13373667001724243, 0.13420967757701874, 0.1404944807291031, 0.13226987421512604, 0.13657937943935394, 0.13399627804756165, 0.1330520063638687, 0.13820239901542664, 0.13220630586147308, 0.13337409496307373, 0.13859711587429047, 0.14252527058124542, 0.13282530009746552, 0.13990940153598785, 0.14489516615867615, 0.14103348553180695, 0.13256390392780304, 0.1421465426683426, 0.13500085473060608, 0.1449560672044754, 0.14074009656906128, 0.13767428696155548, 0.13225889205932617, 0.1582661271095276, 0.1545833796262741, 0.1693289577960968, 0.1321733146905899, 0.1377771645784378, 0.13223610818386078, 0.14804551005363464, 0.13829059898853302, 0.13849617540836334, 0.13799431920051575, 0.13212737441062927, 0.13824313879013062, 0.13692989945411682, 0.13207866251468658, 0.13277266919612885, 0.13447050750255585, 0.13204017281532288, 0.13243389129638672, 0.13205505907535553, 0.13261865079402924, 0.14855529367923737, 0.13631051778793335, 0.13767969608306885, 0.1499500423669815, 0.1327047199010849, 0.13290779292583466, 0.14049161970615387, 0.1403222531080246, 0.14399176836013794, 0.13301986455917358, 0.1550677865743637, 0.13211660087108612, 0.13231335580348969, 0.13305087387561798, 0.13199645280838013, 0.13200260698795319, 0.14133958518505096, 0.13447105884552002, 0.1357947289943695, 0.13370917737483978, 0.16602358222007751, 0.13191257417201996, 0.13193413615226746, 0.13207553327083588, 0.13396182656288147, 0.13210910558700562, 0.14092284440994263, 0.16424649953842163, 0.15503858029842377, 0.13189174234867096, 0.1319587677717209, 0.13630114495754242, 0.13266639411449432, 0.15189775824546814, 0.13307900726795197, 0.1366196870803833, 0.1442744880914688, 0.13250187039375305, 0.13242103159427643, 0.13226546347141266, 0.13203099370002747, 0.13187938928604126, 0.13283675909042358, 0.13615337014198303, 0.13342709839344025, 0.13396748900413513, 0.13193483650684357, 0.13218384981155396, 0.13335204124450684, 0.13670486211776733, 0.13527874648571014, 0.13183635473251343, 0.13194310665130615, 0.13341303169727325, 0.1356634944677353, 0.13829092681407928, 0.131765216588974, 0.14927004277706146, 0.14509636163711548, 0.13202737271785736, 0.13314954936504364, 0.13720516860485077, 0.13439743220806122, 0.1345333606004715, 0.13768310844898224, 0.1346706598997116, 0.14980605244636536, 0.13600051403045654, 0.137877956032753, 0.13187919557094574, 0.13734662532806396, 0.13176803290843964, 0.13768017292022705, 0.14023390412330627, 0.13222290575504303, 0.1382717788219452, 0.1324572116136551, 0.14949610829353333, 0.13381275534629822, 0.13205094635486603, 0.1316939890384674, 0.1316683143377304, 0.13167569041252136, 0.13199324905872345, 0.13955597579479218, 0.13939054310321808, 0.13211718201637268, 0.13951130211353302, 0.13725200295448303, 0.13212084770202637, 0.13536380231380463, 0.13207879662513733, 0.13265341520309448, 0.1318030059337616, 0.132656991481781, 0.13637512922286987, 0.13319042325019836, 0.13349294662475586, 0.13548602163791656, 0.13419923186302185, 0.13171277940273285, 0.1407267302274704, 0.132642924785614, 0.1329934000968933, 0.14072827994823456, 0.15686039626598358, 0.1321452409029007, 0.13234134018421173, 0.13449764251708984, 0.13168834149837494, 0.13200190663337708, 0.13512839376926422, 0.13371925055980682, 0.13192501664161682, 0.13212689757347107, 0.13165754079818726, 0.13173602521419525, 0.13466346263885498, 0.13155022263526917, 0.13207800686359406, 0.1354239135980606, 0.14274246990680695, 0.13915221393108368, 0.13828544318675995, 0.14566822350025177, 0.14166782796382904, 0.1315191388130188, 0.1326497495174408, 0.1315276026725769, 0.13199496269226074, 0.13909515738487244, 0.13635067641735077, 0.13185226917266846, 0.13499316573143005, 0.13151796162128448, 0.15302874147891998, 0.14408954977989197, 0.13212423026561737, 0.14097405970096588, 0.13547037541866302, 0.13539591431617737, 0.13724757730960846, 0.13149255514144897, 0.15317930281162262, 0.13646887242794037, 0.13595232367515564, 0.13461199402809143, 0.13367871940135956, 0.13293994963169098, 0.13226376473903656, 0.13200296461582184, 0.1321767270565033, 0.13546501100063324, 0.13160929083824158, 0.14365743100643158, 0.13383744657039642, 0.13327236473560333, 0.13326925039291382, 0.1529477834701538, 0.13914059102535248, 0.1322118043899536, 0.1330782175064087, 0.1390732228755951, 0.13366195559501648, 0.13920971751213074, 0.13395185768604279, 0.13396476209163666, 0.13601979613304138, 0.13469550013542175, 0.13255935907363892, 0.1336858868598938, 0.1315060704946518, 0.13301686942577362, 0.1397397220134735, 0.14642009139060974, 0.13399922847747803, 0.13275635242462158, 0.14027145504951477, 0.132757306098938, 0.13144899904727936, 0.13283875584602356, 0.1350889354944229, 0.15807947516441345, 0.13705427944660187, 0.13160055875778198, 0.13606223464012146, 0.1316731572151184, 0.13255935907363892, 0.13152936100959778, 0.1362183541059494, 0.13233861327171326, 0.13134188950061798, 0.13756361603736877, 0.1325753927230835, 0.13230958580970764, 0.1482074111700058, 0.1319105178117752, 0.13829898834228516, 0.13974153995513916, 0.13198895752429962, 0.13152237236499786, 0.13150052726268768, 0.13155990839004517, 0.13900598883628845, 0.13716839253902435, 0.1330828219652176, 0.13164591789245605, 0.13154882192611694, 0.13875322043895721, 0.1318317949771881, 0.13179950416088104, 0.13514776527881622, 0.1314985603094101, 0.1341816633939743, 0.13214290142059326, 0.1357690393924713, 0.16106653213500977, 0.1312730610370636, 0.13389301300048828, 0.13259434700012207, 0.14817801117897034, 0.13229423761367798, 0.1344285011291504, 0.13199235498905182, 0.13375341892242432, 0.13163451850414276, 0.1319030523300171, 0.13189037144184113, 0.1353364884853363, 0.13175588846206665, 0.14461778104305267, 0.13439010083675385, 0.1312391608953476, 0.13134832680225372, 0.13188737630844116, 0.13431213796138763, 0.1314375400543213, 0.13124622404575348, 0.13130976259708405, 0.1314093917608261, 0.1334988921880722, 0.131963312625885, 0.13430480659008026, 0.13190267980098724, 0.14690931141376495, 0.13151293992996216, 0.1536978781223297, 0.1319233477115631, 0.13634084165096283, 0.13184218108654022, 0.13616685569286346, 0.14005349576473236, 0.13222819566726685, 0.13267409801483154, 0.1313307136297226, 0.1311928778886795, 0.13119733333587646, 0.13785162568092346, 0.13181044161319733, 0.1313936561346054, 0.13125313818454742, 0.13136763870716095, 0.13126030564308167, 0.13180068135261536, 0.13138526678085327, 0.15143534541130066, 0.13864177465438843, 0.13353903591632843, 0.13272756338119507, 0.1312437504529953, 0.14985251426696777, 0.1321631371974945, 0.13211721181869507, 0.1322159320116043, 0.13125789165496826, 0.13678757846355438, 0.13193395733833313, 0.16080324351787567, 0.13506482541561127, 0.1328381448984146, 0.1319347769021988, 0.13115444779396057, 0.13142023980617523, 0.14576630294322968, 0.13114728033542633, 0.13281823694705963, 0.13654208183288574, 0.13113518059253693, 0.1312105804681778, 0.13116182386875153, 0.131140798330307, 0.13601379096508026, 0.13354675471782684, 0.14153096079826355, 0.1410706490278244, 0.13328638672828674, 0.13221874833106995, 0.14018772542476654, 0.13111592829227448, 0.13565289974212646, 0.13190455734729767, 0.13163800537586212, 0.13224613666534424, 0.13666857779026031, 0.13110677897930145, 0.13168232142925262, 0.13580918312072754, 0.13857461512088776, 0.13174134492874146, 0.137972891330719, 0.1371031105518341, 0.134863018989563, 0.14063458144664764, 0.13257570564746857, 0.13115042448043823, 0.13167399168014526, 0.1316503882408142, 0.13178423047065735, 0.133117213845253, 0.13321687281131744, 0.13117653131484985, 0.13147220015525818, 0.1318778097629547, 0.1314864307641983, 0.15034189820289612, 0.13174095749855042, 0.13851076364517212, 0.14102338254451752, 0.13468213379383087, 0.1315286010503769, 0.13123272359371185, 0.1342911571264267, 0.13941790163516998, 0.1351477950811386, 0.1360773891210556, 0.14261263608932495, 0.1311974972486496, 0.13626748323440552, 0.13500593602657318, 0.13121677935123444, 0.1321708709001541, 0.13123516738414764, 0.13575303554534912, 0.13847078382968903, 0.13725484907627106, 0.1317712515592575, 0.13614624738693237, 0.14327266812324524, 0.13768422603607178, 0.13439036905765533, 0.13783498108386993, 0.13132748007774353, 0.1322454959154129, 0.14096695184707642, 0.13104234635829926, 0.13776454329490662, 0.13346834480762482, 0.13106289505958557, 0.13214243948459625, 0.13137686252593994, 0.14543786644935608, 0.13310928642749786, 0.13211342692375183, 0.1325867772102356, 0.14251886308193207, 0.13221138715744019, 0.1311318278312683, 0.13343490660190582, 0.13342750072479248, 0.13390055298805237, 0.14057746529579163, 0.13285991549491882, 0.13142019510269165, 0.13175782561302185, 0.13728320598602295, 0.1388237178325653, 0.13104845583438873, 0.13722126185894012, 0.1349937915802002, 0.13114020228385925, 0.1369868814945221, 0.1336284875869751, 0.13110196590423584, 0.14719341695308685, 0.133866086602211, 0.14492398500442505, 0.1432534009218216, 0.13447636365890503, 0.1328447461128235, 0.13137371838092804, 0.13098131120204926, 0.1359577625989914, 0.13525988161563873, 0.1373816728591919, 0.13104090094566345, 0.13272076845169067, 0.13127301633358002, 0.13512343168258667, 0.13349822163581848, 0.13757945597171783, 0.133692666888237, 0.13213568925857544, 0.13541583716869354, 0.13397562503814697, 0.1328771561384201, 0.13400349020957947, 0.13097093999385834, 0.1316404640674591, 0.13293206691741943, 0.1313048005104065, 0.13277626037597656, 0.13271772861480713, 0.1373237669467926, 0.13567565381526947, 0.1353258341550827, 0.14044666290283203, 0.13170087337493896, 0.13097280263900757, 0.13739699125289917, 0.13418152928352356, 0.1554865688085556, 0.1455608308315277, 0.13167519867420197, 0.13165898621082306, 0.13971726596355438, 0.13632237911224365, 0.1352567970752716, 0.1318552941083908, 0.1339513659477234, 0.1317230463027954, 0.135990709066391, 0.13103559613227844, 0.13146118819713593, 0.1309787780046463, 0.15709419548511505, 0.13220839202404022, 0.13241176307201385, 0.13961806893348694, 0.13762444257736206, 0.1404247134923935, 0.1327865868806839, 0.13091030716896057, 0.1315736025571823, 0.1338645964860916, 0.13243530690670013, 0.13432618975639343, 0.13206063210964203, 0.1343603879213333, 0.13193678855895996, 0.13959549367427826, 0.13130426406860352, 0.1313488930463791, 0.137600839138031, 0.14354762434959412, 0.13187606632709503, 0.13102588057518005, 0.13322380185127258, 0.13147859275341034, 0.1332249641418457, 0.13600249588489532, 0.13213418424129486, 0.1323496550321579, 0.14286135137081146, 0.1378515660762787, 0.15382584929466248, 0.13088244199752808, 0.13252757489681244, 0.1364312618970871, 0.13093136250972748, 0.1337593048810959, 0.1310780644416809, 0.1339721828699112, 0.1372237205505371, 0.14497999846935272, 0.13118480145931244, 0.1312309205532074, 0.13653871417045593, 0.1311265528202057, 0.13169816136360168, 0.13090461492538452, 0.13088540732860565, 0.13090625405311584, 0.13154754042625427, 0.1443812996149063, 0.1334337741136551, 0.13101090490818024, 0.13110269606113434, 0.1311657577753067, 0.14596696197986603, 0.13177084922790527, 0.13468948006629944, 0.14543750882148743, 0.13461826741695404, 0.13152538239955902, 0.13235417008399963, 0.14435382187366486, 0.1419498473405838, 0.1308789998292923, 0.13659462332725525, 0.1376829594373703, 0.13093210756778717, 0.1341925859451294, 0.13172052800655365, 0.1433824747800827, 0.1349812000989914, 0.1310504823923111, 0.13505129516124725, 0.13429106771945953, 0.1350986361503601, 0.13195273280143738, 0.14054180681705475, 0.13122063875198364, 0.13352718949317932, 0.131132110953331, 0.13145755231380463, 0.13133853673934937, 0.13550828397274017, 0.13188093900680542, 0.14291125535964966, 0.13518495857715607, 0.13086773455142975, 0.13271388411521912, 0.13093914091587067, 0.13324642181396484, 0.13105060160160065, 0.13084520399570465, 0.13293640315532684, 0.13205267488956451, 0.13163398206233978, 0.13104581832885742, 0.13190074265003204, 0.13851231336593628, 0.13082320988178253, 0.1314525455236435, 0.1314050555229187, 0.1324036717414856, 0.13104486465454102, 0.1308467984199524, 0.13192379474639893, 0.1338813304901123, 0.13179318606853485, 0.1308572143316269, 0.13341577351093292, 0.13266326487064362, 0.13120310008525848, 0.1364264041185379, 0.1330544501543045, 0.13458485901355743, 0.13466902077198029, 0.132578507065773, 0.1314946413040161, 0.1308365911245346, 0.13136516511440277, 0.131162628531456, 0.13456957042217255, 0.1405373066663742, 0.13101738691329956, 0.13081306219100952, 0.13166795670986176, 0.1438812017440796, 0.14180728793144226, 0.1373157650232315, 0.13572292029857635, 0.1343516707420349, 0.13905562460422516, 0.1346142441034317, 0.13158190250396729, 0.13119222223758698, 0.13700661063194275, 0.13206422328948975, 0.1590910702943802, 0.13118715584278107, 0.13194116950035095, 0.13115371763706207, 0.13218827545642853, 0.13083910942077637, 0.1386924833059311, 0.13180893659591675, 0.1336728036403656, 0.13856001198291779, 0.13256603479385376, 0.13991759717464447, 0.13090679049491882, 0.13324381411075592, 0.1330295354127884, 0.13385595381259918, 0.137628972530365, 0.13192719221115112, 0.1393386870622635, 0.1332467496395111, 0.1317601352930069, 0.1367618441581726, 0.1421823650598526, 0.13139964640140533, 0.14363113045692444, 0.13976255059242249, 0.13208122551441193, 0.136423259973526, 0.13095128536224365, 0.13115330040454865, 0.13140910863876343, 0.13091328740119934, 0.13784530758857727, 0.13078023493289948, 0.14143966138362885, 0.150514155626297, 0.1314781904220581, 0.13086406886577606, 0.13077247142791748, 0.14002905786037445, 0.13580374419689178, 0.1345628798007965, 0.13898499310016632, 0.13076868653297424, 0.14080601930618286, 0.13745282590389252, 0.13690593838691711, 0.13615429401397705, 0.13320545852184296, 0.1314162164926529, 0.1340429186820984, 0.1308680772781372, 0.13185259699821472, 0.13104680180549622, 0.1378071904182434, 0.13159668445587158, 0.14357833564281464, 0.13331300020217896, 0.14307890832424164, 0.13462582230567932, 0.13906866312026978, 0.13077209889888763, 0.13453100621700287, 0.1456116884946823, 0.13220177590847015, 0.13271595537662506, 0.1327708661556244, 0.13096341490745544, 0.1307934820652008, 0.14414680004119873, 0.14780259132385254, 0.13154195249080658, 0.15160883963108063, 0.13078433275222778, 0.13638344407081604, 0.1391112357378006, 0.1311570405960083, 0.14101387560367584, 0.13195131719112396, 0.13144387304782867, 0.13244834542274475, 0.13829627633094788, 0.14027240872383118, 0.1346026510000229, 0.14838054776191711, 0.13074113428592682, 0.13116008043289185, 0.13075928390026093, 0.13201433420181274, 0.1344814896583557, 0.13771294057369232, 0.1393527090549469, 0.13140779733657837, 0.13115179538726807, 0.13310325145721436, 0.13125376403331757, 0.1402941644191742, 0.13522152602672577, 0.14106470346450806, 0.13073664903640747, 0.130740687251091, 0.13339684903621674, 0.1381245106458664, 0.14816097915172577, 0.13272614777088165, 0.14420519769191742, 0.1325041949748993, 0.13093584775924683, 0.1360253095626831, 0.1381516456604004, 0.14759811758995056, 0.1307336688041687, 0.1464501917362213, 0.13626743853092194, 0.13431993126869202, 0.13210824131965637, 0.13089802861213684, 0.13163438439369202, 0.13095220923423767, 0.1327059119939804, 0.1375643014907837, 0.13187089562416077, 0.14259976148605347, 0.14014436304569244, 0.14070025086402893, 0.1400529146194458, 0.13187164068222046, 0.1311071366071701, 0.13071222603321075, 0.14415603876113892, 0.13159750401973724, 0.1440882533788681, 0.1373201608657837, 0.13527393341064453, 0.13071249425411224, 0.13177724182605743, 0.1307249814271927, 0.1312841773033142, 0.13387790322303772, 0.13126637041568756, 0.13096535205841064, 0.13088470697402954, 0.13449956476688385, 0.13155989348888397, 0.14508545398712158, 0.1311328113079071, 0.13114504516124725, 0.13080185651779175, 0.1332649439573288, 0.14228768646717072, 0.13075602054595947, 0.14532776176929474, 0.13245709240436554, 0.14727117121219635, 0.13293033838272095, 0.1508922576904297, 0.13129739463329315, 0.13121362030506134, 0.13657671213150024, 0.13077309727668762, 0.13073424994945526, 0.13581523299217224, 0.1321202516555786, 0.137030690908432, 0.1307813972234726, 0.13075274229049683, 0.13076555728912354, 0.1316230446100235, 0.1429332196712494, 0.1310090869665146, 0.1330127716064453, 0.13133932650089264, 0.13085275888442993, 0.13109111785888672, 0.13310183584690094, 0.13198722898960114, 0.1315641552209854, 0.14690791070461273, 0.13334514200687408, 0.13404636085033417, 0.130755215883255, 0.13931646943092346, 0.1308329701423645, 0.13685236871242523, 0.13350050151348114, 0.13771846890449524, 0.14073067903518677, 0.13096489012241364, 0.13858889043331146, 0.1315440833568573, 0.13089227676391602, 0.13509853184223175, 0.13362379372119904, 0.13341329991817474, 0.13136939704418182, 0.13271251320838928, 0.13450580835342407, 0.130929097533226, 0.13068217039108276, 0.13078081607818604, 0.13097669184207916, 0.15867739915847778, 0.1320911943912506, 0.13869206607341766, 0.1306794285774231, 0.13097237050533295, 0.131214901804924, 0.13174130022525787, 0.13924464583396912, 0.1439538300037384, 0.13681232929229736, 0.13930724561214447, 0.13173571228981018, 0.13442255556583405, 0.13346263766288757, 0.13641636073589325, 0.131154865026474, 0.13294367492198944, 0.13157802820205688, 0.13269181549549103, 0.13790325820446014, 0.13102960586547852, 0.13087067008018494, 0.13174733519554138, 0.13934655487537384, 0.13152799010276794, 0.13473305106163025, 0.1373390555381775, 0.1372593343257904, 0.14461451768875122, 0.13071808218955994, 0.1315632313489914, 0.1316973716020584, 0.13146716356277466, 0.13079921901226044, 0.1329931765794754, 0.13952381908893585, 0.13158872723579407, 0.13083752989768982, 0.134639173746109, 0.1306958794593811, 0.13354147970676422, 0.13219796121120453, 0.1309046745300293, 0.13090817630290985, 0.1377105861902237, 0.13374224305152893, 0.1323845088481903, 0.1407463401556015, 0.13878150284290314, 0.1308603286743164, 0.13669992983341217, 0.1311693787574768, 0.1311715841293335, 0.13122797012329102, 0.1314629316329956, 0.1393362283706665, 0.13831675052642822, 0.13535070419311523, 0.1311686933040619, 0.13334575295448303, 0.13618916273117065, 0.13288848102092743, 0.13269811868667603, 0.1306719183921814, 0.13696515560150146, 0.1497315913438797, 0.13574431836605072, 0.13445855677127838, 0.13602976500988007, 0.1384550929069519, 0.1314973384141922, 0.1446477621793747, 0.13068611919879913, 0.13066184520721436, 0.13167013227939606, 0.13768163323402405, 0.13830257952213287, 0.1319313794374466, 0.13222438097000122, 0.13076147437095642, 0.14059413969516754, 0.13222502171993256, 0.14054496586322784, 0.13131262362003326, 0.14115573465824127, 0.13227631151676178, 0.1340968757867813, 0.13242772221565247, 0.13137286901474, 0.13067123293876648, 0.13325870037078857, 0.1309291124343872, 0.14158156514167786, 0.1307114213705063, 0.13284265995025635, 0.1340959519147873, 0.13256649672985077, 0.13319100439548492, 0.1309853047132492, 0.13312633335590363, 0.1364389806985855, 0.13520967960357666, 0.14940790832042694, 0.1398516297340393, 0.14124451577663422, 0.13246943056583405, 0.13348864018917084, 0.13714686036109924, 0.1307106912136078, 0.13381904363632202, 0.13488426804542542, 0.13109692931175232, 0.13875558972358704, 0.13225679099559784, 0.13396835327148438, 0.14243434369564056, 0.13072222471237183, 0.14089946448802948, 0.13156557083129883, 0.13177698850631714, 0.13067558407783508, 0.14355455338954926, 0.1343185305595398, 0.13160160183906555, 0.140422061085701, 0.13126219809055328, 0.13831181824207306, 0.1313118040561676, 0.13240239024162292, 0.13263557851314545, 0.13350920379161835, 0.13907566666603088, 0.13065332174301147, 0.13578815758228302, 0.13067547976970673, 0.13302505016326904, 0.1328650563955307, 0.13069812953472137, 0.1315397024154663, 0.13122369349002838, 0.13651256263256073, 0.13237690925598145, 0.1320735514163971, 0.1337001919746399, 0.13122615218162537, 0.1307263821363449, 0.1314743161201477, 0.14091180264949799, 0.13075704872608185, 0.13078543543815613, 0.13310550153255463, 0.13736845552921295, 0.13207900524139404, 0.13073940575122833, 0.13187849521636963, 0.13063710927963257, 0.13182130455970764, 0.13084709644317627, 0.1328483372926712, 0.13072198629379272, 0.1459750235080719, 0.16163130104541779, 0.13165724277496338, 0.13092301785945892, 0.13498657941818237, 0.1323242038488388, 0.13072218000888824, 0.1546664834022522, 0.13513103127479553, 0.1321146935224533, 0.13422806560993195, 0.13108697533607483, 0.13662713766098022, 0.13359177112579346, 0.1308627724647522, 0.1371617615222931, 0.13077111542224884, 0.13803283870220184, 0.13069887459278107, 0.13264505565166473, 0.1307155042886734, 0.1309652179479599, 0.13776201009750366, 0.13928623497486115, 0.13753364980220795, 0.13481466472148895, 0.1325581967830658, 0.13819260895252228, 0.13116344809532166, 0.13171061873435974, 0.13181175291538239, 0.1315128654241562, 0.13155096769332886, 0.14427457749843597, 0.13063810765743256, 0.13066576421260834, 0.13246040046215057, 0.13152897357940674, 0.13063660264015198, 0.13088615238666534, 0.13166426122188568, 0.1316060572862625, 0.1491462141275406, 0.13078828155994415, 0.13354912400245667, 0.14506621658802032, 0.14513616263866425, 0.13242202997207642, 0.13604363799095154, 0.13106684386730194, 0.13114403188228607, 0.1350189447402954, 0.13134831190109253, 0.13356788456439972, 0.1317453235387802, 0.13190941512584686, 0.13833104074001312, 0.1384345144033432, 0.13134561479091644, 0.13070660829544067, 0.13138911128044128, 0.1307171881198883, 0.13431960344314575, 0.13062813878059387, 0.13064850866794586, 0.13292628526687622, 0.13185694813728333, 0.13160297274589539, 0.1326533406972885, 0.13439618051052094, 0.1421387642621994, 0.13437579572200775, 0.13070973753929138, 0.1356593519449234, 0.13594654202461243, 0.1442985087633133, 0.13259965181350708, 0.13728627562522888, 0.1309901773929596, 0.1306917518377304, 0.13067279756069183, 0.1309501826763153, 0.14737410843372345, 0.1522412747144699, 0.13062536716461182, 0.13069039583206177, 0.14125899970531464, 0.13205666840076447, 0.14532992243766785, 0.15864509344100952, 0.13063710927963257, 0.154636949300766, 0.13611914217472076, 0.13154713809490204, 0.13065145909786224, 0.13066352903842926, 0.13069652020931244, 0.13388055562973022, 0.14292535185813904, 0.13115042448043823, 0.13181042671203613, 0.13172240555286407, 0.13208027184009552, 0.13336187601089478, 0.13064999878406525, 0.13157488405704498, 0.14811377227306366, 0.13192592561244965, 0.13448181748390198, 0.13203833997249603, 0.13063567876815796, 0.13075247406959534, 0.1375970095396042, 0.13142038881778717, 0.13064897060394287, 0.1311895102262497, 0.13732121884822845, 0.13071852922439575, 0.13088488578796387, 0.1400269865989685, 0.13143222033977509, 0.14239312708377838, 0.1362326443195343, 0.13643310964107513, 0.13066884875297546, 0.13086636364459991, 0.13095048069953918, 0.1307128369808197, 0.13941575586795807, 0.13096630573272705, 0.13404341042041779, 0.1314644068479538, 0.13696423172950745, 0.14586329460144043, 0.13114303350448608, 0.14237375557422638, 0.13116991519927979, 0.1418875902891159, 0.1429256796836853, 0.1324542760848999, 0.13433465361595154, 0.14543381333351135, 0.1312437206506729, 0.1315954178571701, 0.1307181864976883, 0.1312667429447174, 0.13412590324878693, 0.1311749964952469, 0.1314837485551834, 0.13887324929237366, 0.1306811422109604, 0.13277371227741241, 0.14418841898441315, 0.13311800360679626, 0.1340840458869934, 0.13299444317817688, 0.13065212965011597, 0.13097676634788513, 0.1361204981803894, 0.13093024492263794, 0.14357414841651917, 0.13062028586864471, 0.13493096828460693, 0.13066977262496948, 0.13084259629249573, 0.13070979714393616, 0.1307355761528015, 0.13901157677173615, 0.13244982063770294, 0.13425937294960022, 0.13073702156543732, 0.13249528408050537, 0.13061052560806274, 0.1317799985408783, 0.1308111697435379, 0.13216239213943481, 0.13065265119075775, 0.1308431476354599, 0.13298428058624268, 0.1324232518672943, 0.13842056691646576, 0.1486165076494217, 0.13620011508464813, 0.1358352154493332, 0.1362893432378769, 0.1312176138162613, 0.13061712682247162, 0.13273656368255615, 0.13529561460018158, 0.13314048945903778, 0.13075323402881622, 0.13192513585090637, 0.13062258064746857, 0.13834373652935028, 0.13080258667469025, 0.13397257030010223, 0.13085921108722687, 0.13183817267417908, 0.13077162206172943, 0.13952139019966125]\n",
      "parameters from optimal\n",
      "[-5.457 -4.877  9.756]\n",
      "parameters from SGD-0.1\n",
      "[-5.325 -4.805  9.687]\n",
      "max delta: 0.06894683837890625\n",
      "predictions from optimal\n",
      "[0.251 0.    1.    0.048 0.019 0.    0.    0.047 0.34  0.001 0.21  0.\n",
      " 0.    0.    0.    0.01  0.998 0.009 0.297 0.998 0.    0.471 0.    0.998\n",
      " 0.689 0.386 1.    0.992 0.    0.    0.    0.    0.    0.    0.    0.901\n",
      " 0.001 0.    0.    0.    0.993 0.004 0.003 0.    0.    0.004 0.848 0.974\n",
      " 0.    0.935]\n",
      "predictions from SGD-0.1\n",
      "[0.398 0.    1.    0.091 0.04  0.    0.    0.096 0.5   0.002 0.357 0.\n",
      " 0.    0.    0.    0.02  0.999 0.017 0.445 0.999 0.    0.637 0.    0.999\n",
      " 0.841 0.568 1.    0.996 0.    0.    0.    0.    0.    0.    0.    0.946\n",
      " 0.003 0.    0.    0.    0.996 0.009 0.006 0.    0.    0.007 0.917 0.987\n",
      " 0.001 0.969]\n",
      "max delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Now run longer and compare to global opt\n",
    "\n",
    "schedule = optimizers.constant(step_size=lr)\n",
    "opt_init, opt_update, get_params = optimizers.sgd(step_size=schedule)\n",
    "max_epochs = 2000\n",
    "batcher = MyBatcher(X_train, y_train, batch_size=10, seed=0)\n",
    "w_mle_sgd3, history = sgd_jax(w_init, NLL, batcher, max_epochs,\n",
    "                              opt_init, opt_update, get_params)\n",
    "print(w_mle_sgd3)\n",
    "#print(history)\n",
    "\n",
    "evaluate_params(w_mle_sklearn, w_mle_sgd3, \"SGD-0.1\")\n",
    "evaluate_preds(w_mle_sklearn, w_mle_sgd3, \"SGD-0.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train NLL 0.6990975141525269\n",
      "Epoch 200, train NLL 0.1549871861934662\n",
      "Epoch 400, train NLL 0.14150398969650269\n",
      "Epoch 600, train NLL 0.1348927915096283\n",
      "Epoch 800, train NLL 0.13298003375530243\n",
      "Epoch 1000, train NLL 0.13282859325408936\n",
      "Epoch 1200, train NLL 0.13134409487247467\n",
      "Epoch 1400, train NLL 0.13436338305473328\n",
      "Epoch 1600, train NLL 0.133067324757576\n",
      "Epoch 1800, train NLL 0.13249127566814423\n",
      "parameters from optimal\n",
      "[-5.457 -4.877  9.756]\n",
      "parameters from SGD-mom-0.1\n",
      "[-5.281 -4.715  9.413]\n",
      "max delta: 0.34227943420410156\n",
      "predictions from optimal\n",
      "[0.251 0.    1.    0.048 0.019 0.    0.    0.047 0.34  0.001 0.21  0.\n",
      " 0.    0.    0.    0.01  0.998 0.009 0.297 0.998 0.    0.471 0.    0.998\n",
      " 0.689 0.386 1.    0.992 0.    0.    0.    0.    0.    0.    0.    0.901\n",
      " 0.001 0.    0.    0.    0.993 0.004 0.003 0.    0.    0.004 0.848 0.974\n",
      " 0.    0.935]\n",
      "predictions from SGD-mom-0.1\n",
      "[0.237 0.    1.    0.048 0.019 0.    0.    0.045 0.32  0.001 0.197 0.\n",
      " 0.    0.    0.    0.01  0.998 0.009 0.28  0.997 0.    0.442 0.    0.997\n",
      " 0.648 0.36  0.999 0.989 0.    0.    0.    0.    0.    0.    0.    0.882\n",
      " 0.001 0.    0.    0.    0.991 0.005 0.003 0.    0.    0.004 0.823 0.966\n",
      " 0.001 0.919]\n",
      "max delta: 0.04067748785018921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schedule = optimizers.constant(step_size=lr)\n",
    "#schedule = optimizers.exponential_decay(step_size=0.1, decay_steps=10, decay_rate=0.9)\n",
    "#schedule = optimizers.piecewise_constant([50, 100], [0.1, 0.05, 0.01])\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.momentum(step_size=schedule, mass=0.9)\n",
    "#opt_init, opt_update, get_params = optimizers.adam(step_size=schedule)\n",
    "\n",
    "max_epochs = 2000\n",
    "batcher = MyBatcher(X_train, y_train, batch_size=10, seed=0)\n",
    "w_mle_sgd4, history = sgd_jax(w_init, NLL, batcher, max_epochs,\n",
    "                              opt_init, opt_update, get_params)\n",
    "\n",
    "evaluate_params(w_mle_sklearn, w_mle_sgd4, \"SGD-mom-0.1\")\n",
    "evaluate_preds(w_mle_sklearn, w_mle_sgd4, \"SGD-mom-0.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.131, dtype=float32), DeviceArray(0.14, dtype=float32))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.subplot(121)\n",
    "ax.plot(history)\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('train NLL')\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "ax.plot(history)\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('train NLL')\n",
    "ax.set_ylim(np.min(history), 0.2*np.max(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
