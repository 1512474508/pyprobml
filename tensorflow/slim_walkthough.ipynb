{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-Slim 2.0 Walkthrough\n",
    "\n",
    "This notebook will walk you through the basics of using TF-Slim to define, train and evaluate neural networks on various tasks. It assumes a basic knowledge of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<a href=\"#Install\">Installation and setup</a><br>\n",
    "<a href='#MLP'>Creating your first neural network with slim</a><br>\n",
    "<a href='#SlimData'>Slim datasets</a><br>\n",
    "<a href='#CNN'>Training a convolutional neural network (CNN)</a><br>\n",
    "<a href='#Pretained'>Using pre-trained models</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and setup\n",
    "<a id='Install'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if your version of TF has latest version of slim.\n",
    "\n",
    "There are two ways to test if you have the latest version of slim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "\n",
    "Execute this command at the command line.\n",
    "If it runs without errors, you are good to go.\n",
    "\n",
    "python -c \"import tensorflow.contrib.slim as slim; import tensorflow.contrib.slim.nets; mynet = slim.nets.resnet_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "\n",
    "If you don't want to leave your notebook, execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: You have an up to date snapshot of TF version 0.10\n"
     ]
    }
   ],
   "source": [
    "ver = tf.__version__\n",
    "# e.g., ver = \"0.9.0\" or \"0.10.0rc0\" \n",
    "ver_parts = ver.split(\".\")\n",
    "ver_main = int(ver_parts[0])\n",
    "ver_sub = int(ver_parts[1])\n",
    "# r0.10 was released on 8/22/16. However, this does not contain the latest version of slim.\n",
    "# For that, we need to check if you have the latest snapshot. One simple test is to see if the\n",
    "# resnet has been imported...\n",
    "try:\n",
    "    mynet = slim.nets.resnet_v1\n",
    "    print 'Pass: You have an up to date snapshot of TF version {}.{}'.format(ver_main, ver_sub)\n",
    "except ImportError, e:\n",
    "    print 'Fail: You have TF version {}.{}, you need latest snapshot'.format(ver_main, ver_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do if your version of TF does not contain the latest version of slim\n",
    "\n",
    "To get the latest snapshot, go to https://github.com/tensorflow/tensorflow, navigate to section\n",
    "that says \"People who are a little more adventurous can also try our nightly binaries\",\n",
    "and copy the relevant URL, depending on what kind of computer you have.\n",
    "Next, pip install that file. Finally, restart this notebook.\n",
    "\n",
    "Example:\n",
    " \n",
    "export TF_BINARY_URL=https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n",
    "\n",
    "sudo pip install --upgrade $TF_BINARY_URL\n",
    "\n",
    "ipython notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first neural network with slim\n",
    "<a id='MLP'></a>\n",
    "\n",
    "Below we give some code to create a simple multilayer perceptron (MLP)  which can be used\n",
    "for regression problems. The model has 2 hidden layers.\n",
    "The output is a single node. \n",
    "When this function is called, it will create various nodes, and silently add them to whichever global TF graph is currently in scope. When a node which corresponds to a layer with adjustable parameters (eg., a fully connected layer) is created, additional parameter variable nodes are silently created, and added to the graph. (We will discuss how to train the parameters later.)\n",
    "\n",
    "We use variable scope to put all the nodes under a common name,\n",
    "so that the graph has some hierarchical structure.\n",
    "This is useful when we want to visualize the TF graph in tensorboard, or if we want to query related\n",
    "variables. \n",
    "The fully connected layers all use the same L2 weight decay and ReLu activations, as specified by **arg_scope**. (However, the final layer overrides these defaults, and uses an identity activation function.)\n",
    "\n",
    "We also illustrate how to add a dropout layer after the first fully connected layer (FC1). Note that at test time, \n",
    "we do not drop out nodes, but instead use the average activations; hence we need to know whether the model is being\n",
    "constructed for training or testing, since the computational graph will be different in the two cases\n",
    "(although the variables, storing the model parameters, will be shared, since they have the same name/scope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_model(inputs, is_training=True, scope=\"deep_regression\"):\n",
    "  \"\"\"Creates the regression model.\n",
    "  \n",
    "  Args:\n",
    "    input_node: A node that yields a `Tensor` of size [batch_size, dimensions].\n",
    "    is_training: Whether or not we're currently training the model.\n",
    "    scope: An optional variable_op scope for the model.\n",
    "  \n",
    "  Returns:\n",
    "    output_node: 1-D `Tensor` of shape [batch_size] of responses.\n",
    "    nodes: A dict of nodes representing the hidden layers.\n",
    "  \"\"\"\n",
    "  with tf.variable_op_scope([input_node], scope):\n",
    "    nodes = {}\n",
    "    # Set the default weight _regularizer and acvitation for each fully_connected layer.\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_regularizer=slim.l2_regularizer(0.01)):\n",
    "      \n",
    "      # Creates a fully connected layer from the inputs with 10 hidden units.\n",
    "      fc1_node = slim.fully_connected(inputs, 10, scope='fc1')\n",
    "      nodes['fc1'] = fc1_node\n",
    "        \n",
    "      # Adds a dropout layer to prevent over-fitting.\n",
    "      dropout_node = slim.dropout(fc1_node, 0.8, is_training=is_training)\n",
    "      \n",
    "      # Adds another fully connected layer with 5 hidden units.\n",
    "      fc2_node = slim.fully_connected(dropout_node, 5, scope='fc2')\n",
    "      nodes['fc2'] = fc2_node\n",
    "      \n",
    "      # Creates a fully-connected layer with a single hidden unit. Note that the\n",
    "      # layer is made linear by setting activation_fn=None.\n",
    "      prediction_node = slim.fully_connected(fc2_node, 1, activation_fn=None, scope='prediction')\n",
    "      nodes['out'] = prediction_node\n",
    "\n",
    "      return prediction_node, nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the model and examine its structure.\n",
    "\n",
    "We create a TF graph and call regression_model(), which adds nodes (tensors) to the graph. We then examine their shape, and print the names of all the model variables which have been implicitly created inside of each layer. We see that the names of the variables follow the scopes that we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers\n",
      "name = deep_regression/fc1/Relu:0, shape = (?, 10)\n",
      "name = deep_regression/fc2/Relu:0, shape = (?, 5)\n",
      "name = deep_regression/prediction/BiasAdd:0, shape = (?, 1)\n",
      "Parameters\n",
      "name = deep_regression/fc1/weights:0, shape = (1, 10)\n",
      "name = deep_regression/fc1/biases:0, shape = (10,)\n",
      "name = deep_regression/fc2/weights:0, shape = (10, 5)\n",
      "name = deep_regression/fc2/biases:0, shape = (5,)\n",
      "name = deep_regression/prediction/weights:0, shape = (5, 1)\n",
      "name = deep_regression/prediction/biases:0, shape = (1,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Dummy placeholders for arbitrary number of 1d inputs and outputs\n",
    "  input_node = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "  output_node = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "  \n",
    "  # Build model\n",
    "  prediction_node, all_nodes = regression_model(input_node)\n",
    " \n",
    "  # Print name and shape of each tensor.\n",
    "  print \"Layers\"\n",
    "  for k, v in all_nodes.iteritems():\n",
    "    print 'name = {}, shape = {}'.format(v.name, v.get_shape())\n",
    "    \n",
    "  # Print name and shape of parameter nodes  (values not yet initialized)\n",
    "  print \"Parameters\"\n",
    "  for v in slim.get_model_variables():\n",
    "    print 'name = {}, shape = {}'.format(v.name, v.get_shape())\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create some 1d regression data .\n",
    "\n",
    "We will train and test the model on some noisy observations of a nonlinear function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5a98604a50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/collections.py:548: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == 'face':\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD/CAYAAAADvzaFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a6d29a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def produce_batch(batch_size, noise=0.3):\n",
    "  xs = np.random.random(size=[batch_size, 1]) * 10\n",
    "  ys = np.sin(xs) + 5 + np.random.normal(size=[batch_size, 1], scale=noise)\n",
    "  return [xs.astype(np.float32), ys.astype(np.float32)]\n",
    "\n",
    "x_train, y_train = produce_batch(100)\n",
    "x_test, y_test = produce_batch(100)\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit the model to the data\n",
    "\n",
    "The user has to specify the loss function and the optimizer, and slim does the rest.\n",
    "In particular,  the slim.learning.train function does the following:\n",
    "\n",
    "- For each iteration, evaluate the train_op, which updates the parameters using the optimizer applied to the current minibatch. Also, update the global_step.\n",
    "- Occasionally store the model checkpoint in the specified directory. This is useful in case your machine crashes  - then you can simply restart from the specified checkpoint.\n",
    "- Occasionally write summaries to ???. These can be examined in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /tmp/tf/regression_model/model1471957940.55\n"
     ]
    }
   ],
   "source": [
    "# Everytime we run training, we need to store the model checkpoint in a new directory,\n",
    "# in case anything has changed.\n",
    "import time\n",
    "ts = time.time()\n",
    "ckpt_dir = '/tmp/tf/regression_model/model{}'.format(ts) # Place to store the checkpoint.\n",
    "print('Saving to {}'.format(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data_to_tensors(x, y):\n",
    "  input_tensor = tf.constant(x)\n",
    "  input_tensor.set_shape([None, 1])\n",
    "  output_tensor = tf.constant(y)\n",
    "  output_tensor.set_shape([None, 1])\n",
    "  return input_tensor, output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished training. Last batch loss:', 0.45064953)\n",
      "Checkpoint saved in /tmp/tf/regression_model/model1471957940.55\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph() # new graph\n",
    "with graph.as_default():\n",
    "  input_node, output_node = convert_data_to_tensors(x_train, y_train)\n",
    "\n",
    "  # Make the model.\n",
    "  prediction_node, nodes = regression_model(input_node, is_training=True)\n",
    " \n",
    "  # Add the loss function to the graph.\n",
    "  loss_node = slim.losses.sum_of_squares(prediction_node, output_node)\n",
    "  # The total loss is the uers's loss plus any regularization losses.\n",
    "  total_loss_node = slim.losses.get_total_loss()\n",
    "\n",
    "  # Create some summaries to visualize the training process:\n",
    "  ## TODO: add summaries.py to 3p\n",
    "  #slim.summaries.add_scalar_summary(total_loss, 'Total Loss', print_summary=True)\n",
    "  \n",
    "  # Specify the optimizer and create the train op:\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "  train_op_node = slim.learning.create_train_op(total_loss_node, optimizer) \n",
    "\n",
    "  # Run the training inside a session.\n",
    "  final_loss = slim.learning.train(\n",
    "    train_op_node,\n",
    "    logdir=ckpt_dir,\n",
    "    number_of_steps=500,\n",
    "    save_summaries_secs=1)\n",
    "  \n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with multiple loss functions.\n",
    "\n",
    "Sometimes we have multiple objectives we want to simultaneously optimize.\n",
    "In slim, it is easy to add more losses, as we show below. (We do not optimize the total loss in this example,\n",
    "but we show how to compute it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss1: 63.857437\n",
      "Total Loss2: 63.857437\n",
      "Regularization Losses:\n",
      "Tensor(\"deep_regression/fc1/weights/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Tensor(\"deep_regression/fc2/weights/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Tensor(\"deep_regression/prediction/weights/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Loss Functions:\n",
      "Tensor(\"sum_of_squares_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"absolute_difference/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()  # Make a new graph\n",
    "with graph.as_default():\n",
    "    input_node, output_node = convert_data_to_tensors(x_train, y_train)\n",
    "    prediction_node, nodes = regression_model(input_node, is_training=True)\n",
    "\n",
    "    # Add multiple loss nodes.\n",
    "    sum_of_squares_loss_node = slim.losses.sum_of_squares(prediction_node, output_node)\n",
    "    absolute_difference_loss_node = slim.losses.absolute_difference(prediction_node, output_node)\n",
    "\n",
    "    # The following two ways to compute the total loss are equivalent\n",
    "    regularization_loss_node = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    total_loss1_node = sum_of_squares_loss_node + absolute_difference_loss_node + regularization_loss_node\n",
    "\n",
    "    # Regularization Loss is included in the total loss by default.\n",
    "    # This is good for training, but not for testing.\n",
    "    total_loss2_node = slim.losses.get_total_loss(add_regularization_losses=True)\n",
    "    \n",
    "    init_node = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_node) # Will randomize the parameters.\n",
    "        total_loss1, total_loss2 = sess.run([total_loss1_node, total_loss2_node])\n",
    "        print('Total Loss1: %f' % total_loss1)\n",
    "        print('Total Loss2: %f' % total_loss2)\n",
    "\n",
    "        print('Regularization Losses:')\n",
    "        for loss_node in slim.losses.get_regularization_losses():\n",
    "            print(loss_node)\n",
    "\n",
    "        print('Loss Functions:')\n",
    "        for loss_node in slim.losses.get_losses():\n",
    "            print(loss_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the saved model and use it for prediction.\n",
    "\n",
    "The predictive accuracy is not very good, because we used a small model,\n",
    "and only trained for 500 steps, to keep the demo fast. \n",
    "Running for 5000 steps improves performance a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f5a647a5950>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEHCAYAAABcCaZFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a6ce66090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_node, output_node = convert_data_to_tensors(x_test, y_test)\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    prediction_node, nodes = regression_model(input_node, is_training=False)\n",
    "\n",
    "    # Make a session which restores the old parameters from a checkpoint.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        inputs, predictions, true_outputs = sess.run([input_node, prediction_node, output_node])\n",
    "\n",
    "plt.scatter(inputs, true_outputs, c='r');\n",
    "plt.scatter(inputs, predictions, c='b');\n",
    "plt.title('red=true, blue=predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine the learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_regression/fc1/weights:0 (1, 10) [[ -5.06266467e-02   5.45447910e-13   1.21284246e-01  -1.44770980e-01\n",
      "   -2.18129363e-02  -3.88705172e-02   1.23600796e-01  -1.58917261e-04\n",
      "    5.34504652e-01   4.70542043e-01]]\n",
      "deep_regression/fc1/biases:0 (10,) [ 0.51114881 -0.1092848   0.42604351  0.59958661  0.47088459  0.43627158\n",
      " -0.2441307  -0.09360857  0.20583071 -0.02956541]\n",
      "deep_regression/fc2/weights:0 (10, 5) [[ -1.16052706e-11  -1.54837683e-01   5.02475441e-01   4.13749188e-01\n",
      "   -4.51538116e-01]\n",
      " [ -6.81778906e-13   9.83630538e-12  -6.79897588e-12   3.31871358e-13\n",
      "   -1.24219775e-11]\n",
      " [ -2.67401170e-02  -1.67030662e-01   1.13806441e-01  -3.86184417e-02\n",
      "    2.97718614e-01]\n",
      " [  1.05787178e-12   2.26782888e-01   1.51686773e-01   1.58880055e-01\n",
      "    1.96304187e-01]\n",
      " [ -3.65290524e-12   8.95788777e-04   6.14808314e-02   2.61927787e-02\n",
      "    3.90173346e-02]\n",
      " [ -1.18928066e-12   8.73013437e-02  -8.43310803e-02   7.38054886e-02\n",
      "    1.69781461e-01]\n",
      " [  3.83405853e-03  -2.50683814e-01   3.50693315e-01  -6.17958382e-02\n",
      "   -3.09542567e-01]\n",
      " [ -7.97761813e-13   9.05892711e-13   2.87100801e-13   1.57931827e-12\n",
      "    1.28446923e-11]\n",
      " [  2.66012740e-10  -2.30664194e-01  -1.36213779e-01   6.93414688e-01\n",
      "   -2.07919747e-01]\n",
      " [ -8.37271443e-08   2.22258657e-01  -8.80518779e-02   1.19137987e-01\n",
      "   -2.23481551e-01]]\n",
      "deep_regression/fc2/biases:0 (5,) [-0.10564776  0.94643998  1.58608937  0.68163437  1.20217288]\n",
      "deep_regression/prediction/weights:0 (5, 1) [[ -8.74219222e-06]\n",
      " [  1.10868037e+00]\n",
      " [  9.91535127e-01]\n",
      " [  6.82840288e-01]\n",
      " [  1.02710557e+00]]\n",
      "deep_regression/prediction/biases:0 (1,) [ 0.57118797]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_node = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    output_node = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    prediction_node, nodes = regression_model(input_node, is_training=False)\n",
    "  \n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        model_variables = slim.get_model_variables()\n",
    "        for v in model_variables:\n",
    "            val = sess.run(v)\n",
    "            print v.name, val.shape, val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute various evaluation metrics on the test set.\n",
    "\n",
    "In slim termiology, losses are optimized, but metrics (which may not be differentiable, e.g., precision and recall) are just measured.\n",
    "As an illustration, the code below computes mean squared error and mean absolute error metrics on the test set.\n",
    "\n",
    "Each metric declaration creates several local variables (which must be initialized via tf.initialize_local_variables()) and returns both a value_op and an update_op. When evaluated, the value_op returns the current value of the metric. The update_op loads a new batch of data, runs the model, obtains the predictions and accumulates the metric statistics appropriately before returning the current value of the metric. We store these value nodes and update nodes in 2 dictionaries.\n",
    "\n",
    "After creating the metric nodes, we can pass them to slim.evaluation.evaluation, which repeatedly evaluates these nodes the specified number of times. (This allows us to compute the evaluation in a streaming fashion across minibatches, which is usefulf for large datasets.) Finally, we print the final value of each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 23.741724\n",
      "Mean Absolute Error: 4.810077\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    input_node, output_node = convert_data_to_tensors(x_test, y_test)\n",
    "    prediction_node, nodes = regression_model(input_node, is_training=False)\n",
    "\n",
    "    # Specify metrics to evaluate:\n",
    "    names_to_value_nodes, names_to_update_nodes = slim.metrics.aggregate_metric_map({\n",
    "      'Mean Squared Error': slim.metrics.streaming_mean_squared_error(prediction_node, output_node),\n",
    "      'Mean Absolute Error': slim.metrics.streaming_mean_absolute_error(prediction_node, output_node)\n",
    "    })\n",
    "\n",
    "\n",
    "    init_node = tf.group(\n",
    "        tf.initialize_all_variables(),\n",
    "        tf.initialize_local_variables())\n",
    "\n",
    "    # Make a session which restores the old graph parameters, and then run eval.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        metric_values = slim.evaluation.evaluation(\n",
    "            sess,\n",
    "            num_evals=1, # Single pass over data\n",
    "            init_op=init_node,\n",
    "            eval_op=names_to_update_nodes.values(),\n",
    "            final_op=names_to_value_nodes.values())\n",
    "\n",
    "    names_to_values = dict(zip(names_to_value_nodes.keys(), metric_values))\n",
    "    for key, value in names_to_values.iteritems():\n",
    "      print('%s: %f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slim datasets\n",
    "<a id='SlimData'></a>\n",
    "\n",
    "A slim Dataset object contains a pointer to a data file, and metadata, so it knows how big the file is, what kind of data is stored in the file, etc. For example, some files contain images with labels, some also have bounding box annotations, etc. The Dataset object allows us to write generic code using the same API, regardless of the data format. \n",
    "\n",
    "The easiest way to create a slim Dataset is if the data is stored as a (possibly sharded)\n",
    "[TFRecords file](https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#file-formats), where each record contains a [tf.train.Example protocol buffer](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/core/example/example.proto).\n",
    "Slim uses a consistent convention for naming the keys and values inside each example. Code to convert various common academic datasets into TFRecord format can be found at  https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow_models/slim/datasets/.\n",
    "In particular, we include scripts to download and process the following image datasets:\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "<title>HTML Tables</title>\n",
    "</head>\n",
    "<body>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>Dataset</td>\n",
    "<td>Script</td>\n",
    "<td>Contains</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a></td>\n",
    "<td>Script</td>\n",
    "<td>60k+10k 28x28 grayscale images of handwritten digits, 10 class labels\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a></td>\n",
    "<td>Script</td>\n",
    "<td>50k+10k 32x32 color images, 10 class labels\n",
    "</tr>\n",
    "<tr>\n",
    "<tr>\n",
    "<td><a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100</a></td>\n",
    "<td>Script</td>\n",
    "<td>50k+10k 32x32 color images, 100 class labels\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://www.image-net.org/challenges/LSVRC/2012/index\">ImageNet ILSVRC2012</td>\n",
    "<td>Script</td>\n",
    "<td>1.1M color images (various sizes), 1000 class labels\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\">Pascal VOC 2012</a></td>\n",
    "<td>Script</td>\n",
    "<td>~20k color images (various sizes), bounding boxes and class segmentation masks for 20 categories\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://mscoco.org/\">MSCOCO</a></td>\n",
    "<td>Script</td>\n",
    "<td>~120k color images (various sizes), bounding boxes and instance segmentation masks for 80 categories, image captions from 5 raters, 16 keypoints for human pose\n",
    "</tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "Once we have the data in TFRecord format, we can easily create a slim Dataset from it. Finally, we can create a slim DataProvider, which is an object which can \"provide\" minibatches of data on demand. A DataProvider can read data in parallel using queue runners, to support high throughput processing.\n",
    "\n",
    "Below we give an example of using a small open source dataset of XXX flower images from Flickr, each of which has 1 of 5 mutually exclusive class labels. (Source of images is listd in the license file XXX.) \n",
    "The bash script to download and convert this data into TFRecord format is [here](https://github.com/tensorflow/models/blob/master/inception/inception/data/download_and_preprocess_flowers.sh).\n",
    "To save time, we have done this step for you, so you can quickly do some experiments, as we show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the TFRecord file.\n",
    "\n",
    "To save time, we just download the testset, not all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the data.\n",
    "import six\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import os\n",
    "url = 'https://github.com/probml/pyprobml/blob/master/tensorflow/cifar10_test.tfrecord'\n",
    "cifar10_folder = '/tmp/tf/cifar10'\n",
    "filename = os.path.join(cifar10_folder, 'cifar10_test.tfrecord')\n",
    "urllib.request.urlretrieve(url, filename) # Corrupted?\n",
    "cifar10_folder = '/tmp/cifar10' # Data put here using Nathan's script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TFRecord file to slim Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load https://raw.githubusercontent.com/probml/pyprobml/master/tensorflow/cifar10_make_slim_dataset.py\n",
    "import tensorflow as tf\n",
    "\n",
    " \n",
    "def make_cifar_dataset(split_name, tf_folder):\n",
    "    \"\"\"Make a dataset object from cifar10 tfrecord file.\n",
    "\n",
    "    Args:\n",
    "      split_name: \"train\" or \"test\"\n",
    "      tf_folder: The base directory of the dataset sources.\n",
    "\n",
    "    Returns:\n",
    "      A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if `split_name` is not a valid train/test split.\n",
    "    \"\"\"\n",
    "    SPLITS_TO_SIZES = {'train': 50000, 'test': 10000}\n",
    "\n",
    "    ITEMS_TO_DESCRIPTIONS = {\n",
    "        'image': 'A [32 x 32 x 3] color image.',\n",
    "        'label': 'A single integer between 0 and 9',\n",
    "    }\n",
    "    if split_name not in SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    file_pattern =  '%s/cifar10_%s.tfrecord' % (tf_folder, split_name)\n",
    "\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/class/label': tf.FixedLenFeature(\n",
    "            [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "   \n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=tf.TFRecordReader,\n",
    "        decoder=decoder,\n",
    "        num_samples=SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=ITEMS_TO_DESCRIPTIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the first image and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"ParseSingleExample/ParseExample/Reshape:0\", shape=(), dtype=string) must be from the same graph as Tensor(\"zeros_1:0\", shape=(), dtype=int64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-959bc329be54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#dataset = make_cifar_dataset('test', cifar10_folder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     data_provider = slim.dataset_data_provider.DatasetDataProvider(\n\u001b[0;32m----> 7\u001b[0;31m         dataset, common_queue_capacity=32, common_queue_min=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This dataset contains data of type {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, num_readers, shuffle, num_epochs, common_queue_capacity, common_queue_min)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     super(DatasetDataProvider, self).__init__(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, serialized_example, items)\u001b[0m\n\u001b[1;32m    321\u001b[0m     example = parsing_ops.parse_single_example(\n\u001b[1;32m    322\u001b[0m         \u001b[0mserialized_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         self._keys_to_features)\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# Reshape non-sparse elements just once:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parsing_ops.pyc\u001b[0m in \u001b[0;36mparse_single_example\u001b[0;34m(serialized, features, name, example_names)\u001b[0m\n\u001b[1;32m    451\u001b[0m   return _parse_single_example_raw(\n\u001b[1;32m    452\u001b[0m       \u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m       dense_types, dense_defaults, dense_shapes, name)\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parsing_ops.pyc\u001b[0m in \u001b[0;36m_parse_single_example_raw\u001b[0;34m(serialized, names, sparse_keys, sparse_types, dense_keys, dense_types, dense_defaults, dense_shapes, name)\u001b[0m\n\u001b[1;32m    523\u001b[0m                                  \u001b[0mdense_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                                  \u001b[0mdense_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                                  name=name)\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdense_keys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdense_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parsing_ops.pyc\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[0;34m(serialized, names, sparse_keys, sparse_types, dense_keys, dense_types, dense_defaults, dense_shapes, name)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mdense_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mdense_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_parsing_ops.pyc\u001b[0m in \u001b[0;36m_parse_example\u001b[0;34m(serialized, names, sparse_keys, dense_keys, dense_defaults, sparse_types, dense_shapes, name)\u001b[0m\n\u001b[1;32m    163\u001b[0m                                 \u001b[0mdense_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                                 \u001b[0msparse_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                                 dense_shapes=dense_shapes, name=name)\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_ParseExampleOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3813\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3814\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3815\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3816\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3758\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3759\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"ParseSingleExample/ParseExample/Reshape:0\", shape=(), dtype=string) must be from the same graph as Tensor(\"zeros_1:0\", shape=(), dtype=int64)."
     ]
    }
   ],
   "source": [
    "# This code hangs! DO NOT RUN\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "    dataset = make_cifar_dataset('test', cifar10_folder) # Must make dataset in same graph as data_provider and model\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "    print(\"This dataset contains data of type {}\".format(data_provider.list_items()))\n",
    "    \n",
    "    image_node, label_node = data_provider.get(['image', 'label'])\n",
    "    with tf.Session() as sess:    \n",
    "        image, label = sess.run([image_node, label_node])\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            image, label = sess.run([image_node, label_node])   ## Hangs \n",
    "        \n",
    "    print(label)\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural nets (CNNs).\n",
    "<a id='CNN'></a>\n",
    "\n",
    "In this section, we show how to train an image classifier using a simple CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model.\n",
    "\n",
    "Note that the output layer is linear function. We will apply softmax transformation externally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_cnn(images, num_classes, is_training):  # is_training is not used...\n",
    "    with slim.arg_scope([slim.max_pool2d], kernel_size=[3, 3], stride=2):\n",
    "        net = slim.conv2d(images, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.conv2d(net, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 192)\n",
    "        net = slim.fully_connected(net, num_classes, activation_fn=None)       \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to some random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "[[ 0.067437    0.11198212  0.09995704  0.07805576  0.15142384  0.1151979\n",
      "   0.0974801   0.09685149  0.09943112  0.08218364]\n",
      " [ 0.06851638  0.10642001  0.09527245  0.0856739   0.14052564  0.11118399\n",
      "   0.10146013  0.10071222  0.10334437  0.08689094]\n",
      " [ 0.06625742  0.10794784  0.09770861  0.08191212  0.14782986  0.11310407\n",
      "   0.09899553  0.10049271  0.09934841  0.08640333]]\n",
      "[ 1.          1.          0.99999988]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # The model can handle any input size because the first layer is convolutional.\n",
    "    # The size of the model is determined when image_node is passed into the Model function.\n",
    "    # All images must be the same size, because of the fully connected layers, which require a fixed size input.\n",
    "    n_images = 3\n",
    "    image_node = tf.random_uniform([n_images, 28, 28, 3], maxval=1)\n",
    "    \n",
    "    # Create the model.\n",
    "    logits_node = my_cnn(image_node, 10, True)\n",
    "    prob_node = tf.nn.softmax(logits_node)\n",
    "  \n",
    "    # Initialize all the variables (including parameters) randomly.\n",
    "    init_op = tf.initialize_all_variables()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        # Run the init_op, evaluate the model outputs and print the results:\n",
    "        sess.run(init_op)\n",
    "        probs = sess.run(prob_node)\n",
    "        \n",
    "print(probs.shape)  # 3x10\n",
    "print(probs)\n",
    "print(np.sum(probs, 1)) # Each row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on CIFAR10.\n",
    "\n",
    "Make sure you run the code in the \"Slim datasets\" section first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(image, is_training):\n",
    "    \"\"\"Preprocesses the given image.\n",
    "\n",
    "    Args:\n",
    "        image: An image `Tensor` of size [32, 32, 3].\n",
    "        is_training: A boolean, whether or not we're in training mode.\n",
    "\n",
    "    Returns:\n",
    "        A preprocessed and cropped image of shape [24, 24, 3]\n",
    "    \"\"\"\n",
    "    height = 24\n",
    "    width = 24\n",
    "\n",
    "    if is_training:\n",
    "        # Randomly crop a [height, width] section of the image.\n",
    "        image = tf.random_crop(image, [height, width, 3])\n",
    "\n",
    "        # Randomly flip the image horizontally.\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "        # Randomize the pixel values.\n",
    "        image = tf.image.random_brightness(image, max_delta=63)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "    else:\n",
    "        # Crop the central [height, width] of the image.\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    return tf.image.per_image_whitening(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cifar_tensors(split_name='test'):\n",
    "    dataset = make_cifar_dataset(split_name, cifar10_folder) \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # We have to use the preprocess function, otherwise we get this error:\n",
    "    # TypeError: Expected uint8, got -0.059850560166457976 of type 'float' instead.\n",
    "    image = preprocess(image, is_training=True)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, labels = tf.train.batch(\n",
    "          [image, label],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          num_threads=2,\n",
    "          capacity=10 * BATCH_SIZE)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /tmp/tf/cifar10_model/model1471960271.78\n",
      "Finished training. Last batch loss 2.30703520775\n"
     ]
    }
   ],
   "source": [
    "# Train the model on some labeled data.\n",
    "\n",
    "CHECKPOINT_DIR = '/tmp/tf/cifar10_model/model{}'.format(time.time()) \n",
    "print('Saving model to {}'.format(CHECKPOINT_DIR))\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # We train on the test set, just because it's smaller.\n",
    "    images, labels = create_cifar_tensors('test')\n",
    "  \n",
    "    # Create the model:\n",
    "    logits = my_cnn(images, num_classes=NUM_CLASSES, is_training=True)\n",
    " \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, 10)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    # TODO: Make this be printed to stdout during training.\n",
    "    tf.scalar_summary('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "      train_op,\n",
    "      logdir=CHECKPOINT_DIR,\n",
    "      number_of_steps=50,\n",
    "      save_summaries_secs=1)\n",
    "  \n",
    "    print('Finished training. Last batch loss {}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate some metrics.\n",
    "\n",
    "As we discussed above, we can compute various metrics besides the loss.\n",
    "Below we show how to compute prediction accuracy of the trained model, as well as top-5 classification accuracy. (The difference between evaluation and evaluation_loop is that the latter writes the results to a log directory, so they can be viewed in tensorboard.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation Loop...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "BATCH_SIZE = 64\n",
    "EVAL_DIR = CHECKPOINT_DIR\n",
    "NUM_TEST_IMAGES = 10000\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    images, labels = create_cifar_tensors('test')\n",
    "    logits = my_cnn(images, num_classes=NUM_CLASSES, is_training=False)\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "\n",
    "    # Define the metrics:\n",
    "    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "        'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\n",
    "        'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, labels, 5),\n",
    "    })\n",
    "    \n",
    "    # Create the summary ops such that they also print out to std output:\n",
    "    summary_ops = []\n",
    "    for name, value in names_to_values.iteritems():\n",
    "        op = tf.scalar_summary(name, value, collections=[])\n",
    "        op = tf.Print(op, [value], name)\n",
    "        tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n",
    "        summary_ops.append(op)\n",
    "\n",
    "    \n",
    "    # This ensures that we make a single pass over all of the data.\n",
    "    # TODO: How to handle case where test size is not integer multiple of batch size?\n",
    "    num_batches = math.ceil(NUM_TEST_IMAGES / float(BATCH_SIZE))\n",
    "\n",
    "    print('Running evaluation Loop...')\n",
    "    \n",
    "    slim.evaluation.evaluation_loop(\n",
    "        master='',\n",
    "        checkpoint_dir=CHECKPOINT_DIR,  # Restores model from this checkpoint.\n",
    "        logdir=EVAL_DIR,\n",
    "        num_evals=num_batches,\n",
    "        eval_op=names_to_updates.values(),\n",
    "        summary_op=tf.merge_summary(summary_ops),\n",
    "        max_number_of_evaluations=1, # Only run loop once\n",
    "        eval_interval_secs=1)\n",
    "    print('Done!')\n",
    "    \n",
    "# To print the eval, we need to use tensorboard to decode the files in EVAL_DIR...\n",
    "# TODO: Make this print to screen while running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "[[ 0.10011527  0.09608188  0.10562347  0.09711935  0.09762073  0.10073517\n",
      "   0.09946882  0.09867113  0.10392854  0.10063568]\n",
      " [ 0.10011527  0.09608188  0.10562347  0.09711935  0.09762073  0.10073517\n",
      "   0.09946882  0.09867113  0.10392854  0.10063568]\n",
      " [ 0.10011527  0.09608188  0.10562347  0.09711935  0.09762073  0.10073517\n",
      "   0.09946882  0.09867113  0.10392854  0.10063568]\n",
      " [ 0.10011527  0.09608188  0.10562347  0.09711935  0.09762073  0.10073517\n",
      "   0.09946882  0.09867113  0.10392854  0.10063568]\n",
      " [ 0.10011527  0.09608188  0.10562347  0.09711935  0.09762073  0.10073517\n",
      "   0.09946882  0.09867113  0.10392854  0.10063568]]\n",
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Use random images. They must be the same size as the preprocessed training set, because we are restoring\n",
    "    # fully connected layers from the checkpoint.\n",
    "    n_images = 5\n",
    "    image_node = tf.random_uniform([n_images, 24, 24, 3], maxval=1)\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    logits_node = my_cnn(image_node, num_classes=NUM_CLASSES, is_training=False)\n",
    "    prob_node = tf.nn.softmax(logits_node)\n",
    "    \n",
    "    # Make a session which restores the old parameters from a checkpoint.\n",
    "    sv = tf.train.Supervisor(logdir=CHECKPOINT_DIR)\n",
    "    with sv.managed_session() as sess:\n",
    "        probs = sess.run(prob_node)\n",
    "\n",
    "print(probs.shape)  # 5x10\n",
    "print(probs)\n",
    "print(np.sum(probs, 1)) # Each row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained models\n",
    "<a id='Pretrained'></a>\n",
    "\n",
    "Neural nets work best when they have many parameters, making them very flexible function approximators.\n",
    "However, this  means they must be trained on big datasets. Since this process is slow, we provide various pre-trained models. In particular, you can download the following pre-trained CNNs for image classification, all trained on [ILSVRC 2012](http://www.image-net.org/challenges/LSVRC/2012/index) with 1000 mutually exclusive class labels:\n",
    "\n",
    "<html>\n",
    "<body>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>Model</td>\n",
    "<td>Slim spec</td>\n",
    "<td>Checkpoint</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\">VGG-16</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/vgg_16.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\">VGG-19</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/vgg_19.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://arxiv.org/abs/1409.4842\">Inception-v1</a></td>\n",
    "<td><a href=\"\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v1.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/inception_v1.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://arxiv.org/abs/1502.03167\">Inception-v2</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception-v2.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/inception_v2.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://arxiv.org/abs/1512.00567\">Inception-v3</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception-v3.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/inception_v3.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://arxiv.org/abs/1602.07261\">Inception-v4 (ResCeption)</a></td>\n",
    "<td>Coming soon</td>\n",
    "<td>Coming soon</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://arxiv.org/abs/1512.03385\">Resnet-50</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/resnet_v1_50.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://arxiv.org/abs/1512.03385\">Resnet-101</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/resnet_v1_101.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"https://arxiv.org/abs/1512.03385\">Resnet-152</a></td>\n",
    "<td><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py\">File</a></td>\n",
    "<td><a href=\"http://download.tensorflow.org/models/resnet_v1_152.tar.gz\">File</a></td>\n",
    "</tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "You can either use these models as-is, or you can perform \"surgery\" on them, to modify them for some other task. For example, it is common to \"chop off\" the final pre-softmax layer, and replace it with a new set of weights corresponding to some new set of labels. You can then quickly fine tune the new model on a small new dataset. We illustrate this below, using inception-v3 as the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download inception checkpoint.\n",
    "\n",
    "The checkpoint just contains the parameters, not the model structure, so we need to know which python function was used to create the model. The checkpoint and the model spec file must use the same layer names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading inception-v3-2016-03-01.tar.gz 100.0%('Successfully downloaded', 'inception-v3-2016-03-01.tar.gz', 399307177, 'bytes.')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tarfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9fc7ca022710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Successfully downloaded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bytes.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r:gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minception_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tarfile' is not defined"
     ]
    }
   ],
   "source": [
    "import six\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "#url = 'http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz'\n",
    "url = \"http://download.tensorflow.org/models/inception_v3.tar.gz\"\n",
    "inception_folder = '/tmp/tf/inception-v3'\n",
    "if not os.path.exists(inception_folder):\n",
    "    os.mkdir(inception_folder)\n",
    "filename = url.split('/')[-1]\n",
    "filepath = os.path.join(inception_folder, filename)\n",
    "if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "        sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "            float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(inception_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply pretrained model to some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'nets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8378cd9c86d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Adapt the inception V3 architecture for a 10-class task:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3_arg_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'nets'"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.slim.nets as nets\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # Create 32 dummy images\n",
    "    images = tf.ones((32, 224, 224, 3))\n",
    "\n",
    "    # Adapt the inception V3 architecture for a 10-class task:\n",
    "    scope = nets.inception.inception_v3_arg_scope(is_training=True)\n",
    "    with slim.arg_scope(scope):\n",
    "        logits, end_points = nets.inception.inception_v3(images, num_classes=10)\n",
    "\n",
    "    print('Output size: ')\n",
    "    print(logits.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model on a different set of labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
