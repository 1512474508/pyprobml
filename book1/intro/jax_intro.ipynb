{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_intro.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/intro/jax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# JAX <a class=\"anchor\" id=\"jax\"></a>\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a  version of Numpy that runs fast on CPU, GPU and TPU.\n",
        "In addition to having a fast backend, JAX supports\n",
        "several useful python-level program transformations:\n",
        "\n",
        "* [vmap](#vmap), vectorized map operator for automatic vectorization or batching.\n",
        "* [autograd](#AD), for automatic differentiation.\n",
        "* [jit](#jit), just in time compiler for speeding up your code (even on a CPU!).\n",
        "\n",
        "We illustrate these below.\n",
        "\n",
        "More details can be found at the \n",
        "* [official JAX quickstart page](https://github.com/google/jax#quickstart-colab-in-the-cloud).\n",
        "* [official JAX quickstart colab](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/quickstart.ipynb#scrollTo=SY8mDvEvCGqk)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83137d92-dc43-484c-aa12-83f175b17052"
      },
      "source": [
        "\n",
        "# Load JAX\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "import numpy as onp # original numpy\n",
        "from jax import grad, hessian, jit, vmap\n",
        "print(\"jax version {}\".format(jax.__version__))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExOyfluzIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f20997e-9cda-478a-f24b-192b45b14e07"
      },
      "source": [
        "# Check if GPU is available\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 30 00:38:38 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W /  70W |    111MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4Y-hI3FR8f",
        "outputId": "26eac6b7-c3f5-4133-d4e7-c24ee7fe24bc"
      },
      "source": [
        "\n",
        "# Check if JAX is using GPU\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax backend gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsh5DdOF4R1"
      },
      "source": [
        "### Vmap <a class=\"anchor\" id=\"vmap\"></a>\n",
        "\n",
        "\n",
        "To illustrate vmap, consider a binary logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAMcxMsF0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdc132e-df10-428e-f368-3e6e6d6d4234"
      },
      "source": [
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(np.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(np.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "onp.random.seed(42)\n",
        "w = onp.random.randn(D)\n",
        "X = onp.random.randn(N, D)\n",
        "y = onp.random.randint(0, 2, N)\n",
        "\n",
        "# We can apply predict_batch to a matrix of data, but we cannot apply predict_single in this way\n",
        "# because the order of the arguments to np.dot is incorrect.\n",
        "\n",
        "p1 = predict_batch(w, X)\n",
        "try:\n",
        "    p2 = predict_single(w, X)\n",
        "except:\n",
        "    print('cannot apply to batch')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cannot apply to batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qWaSNBGIqg"
      },
      "source": [
        "To avoid having to think about batch shape, it is often easier to write a function that works on single\n",
        "input vectors. We can then apply this in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqzy2ZFF7Fc"
      },
      "source": [
        "p3 = [predict_single(w, x) for x in X]\n",
        "assert np.allclose(p1, p3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZSksj-1GZcU"
      },
      "source": [
        "Unfortunately, mapping down a list is slow.\n",
        "Fortunately, JAX provides `vmap`, which has the same effect, but can be parallelized.\n",
        "\n",
        "We first apply the `predict_single` function to its first arugment, w, to get a function that only\n",
        "depends on x. We then vectorize this, and map the resulting modified function along rows (dimension 0)\n",
        "of the data matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMNkPb9GGZ1O"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "predict_single_w = partial(predict_single, w)\n",
        "predict_batch_w = vmap(predict_single_w)\n",
        "p4 = predict_batch_w(X)\n",
        "p5 = vmap(predict_single, in_axes=(None, 0))(w, X)\n",
        "\n",
        "assert np.allclose(p1, p4)\n",
        "assert np.allclose(p1, p5)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2RYhnNG9hh"
      },
      "source": [
        "### Autograd <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section, we illustrate automatic differentiation by using it to compute the gradient of the negative log likelihood of a logistic regression model for each example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01975f7-afe2-4e3f-e757-6b5258192f11"
      },
      "source": [
        "\n",
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = np.log(preds) * targets + np.log(1 - preds) * (1 - targets)\n",
        "    return -np.sum(logprobs)\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = np.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7016186\n",
            "[[-0.306 -0.719]\n",
            " [-0.112 -0.112]\n",
            " [-0.532 -0.258]]\n",
            "[-0.95 -1.09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "0f75431f-b117-4559-c83f-d7b5caccde34"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.95 -1.09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGxDFho3H5ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e974c028-0e77-4195-da6e-e22847c8eb75"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "from jax import hessian\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = np.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian(example 0)\n",
            "[[0.105 0.246]\n",
            " [0.246 0.578]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[0.675 0.53 ]\n",
            " [0.53  0.723]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsveS95sIxMh"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = np.diag(mu * (1-mu))\n",
        "  H = np.dot(np.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOIJRGxJigp"
      },
      "source": [
        "\n",
        "### JIT (just in time compilation) <a class=\"anchor\" id=\"JIT\"></a>\n",
        "\n",
        "In this section, we illustrate how to use the Jax JIT compiler to make code go faster (even on a CPU). However, it does not work on arbitrary Python code, as we explain below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsaHkNovICfd"
      },
      "source": [
        "grad_fun_jit = jit(grad_fun) # speedup gradient function\n",
        "grads_jit = vmap(partial(grad_fun_jit, w))(X,y)\n",
        "assert np.allclose(grads, grads_jit)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZUbPDLKIkv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4192101c-2c6a-4019-d939-4d358fa34507"
      },
      "source": [
        "# We can apply JIT to non ML applications as well.\n",
        "\n",
        "def slow_f(x):\n",
        "  # Element-wise ops see a large benefit from fusion\n",
        "  return x * x + x * 2.0\n",
        "\n",
        "x = np.ones((5000, 5000))\n",
        "fast_f = jit(slow_f)\n",
        "%timeit -n10 -r3 fast_f(x)  \n",
        "%timeit -n10 -r3 slow_f(x)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 4.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10 loops, best of 3: 4.64 ms per loop\n",
            "10 loops, best of 3: 12.5 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-33YadKNni"
      },
      "source": [
        "We can also add the `%jit` decorator in front of a function.\n",
        "\n",
        "Note that JIT compilation requires that the control flow through the function  can be determined by the shape (but not concrete value) of its inputs. The function below violates this, since when x<3, it takes one branch, whereas when x>= 3, it takes the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ps1W8LhKKj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "560b98e7-ff00-43af-c7f8-9ea28c91fb47"
      },
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "# This will fail!\n",
        "try:\n",
        "  print(f(2))\n",
        "except Exception as e:\n",
        "  print(\"ERROR:\", e)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Abstract value passed to `bool`, which requires a concrete value. The function to be transformed can't be traced at the required level of abstraction. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHh4tcXKTRf"
      },
      "source": [
        "We can fix this by telling JAX to trace the control flow through the function using concrete values of some of its arguments. JAX will then compile different versions, depending on the input values. See below for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMaRplccKRHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e1150b3-a530-4dcc-cc5c-418b4792219d"
      },
      "source": [
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "f = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f(2.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1KDjxB7KXwi"
      },
      "source": [
        "One solution to this is to use `lax.fori_loop`, which can compile just a single version of the function. Unfortunately, you cannot apply `grad` to code that uses this construct; instead, it is typically used inside an optimization loop, where the inner part contains the gradient, and the outer (non differentiable) part just iterates until convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDIOlLcBKVVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04998507-1985-4488-e053-63de1cca0f46"
      },
      "source": [
        "from jax import lax\n",
        "\n",
        "init_val = 0\n",
        "start = 0\n",
        "stop = 10\n",
        "body_fun = lambda i,x: x+i\n",
        "y = lax.fori_loop(start, stop, body_fun, init_val)\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zrYp3-KcMc"
      },
      "source": [
        "There are a few other subtleties. If your function has global side-effects, JAX's tracer can cause weird things to happen. A common gotcha is trying to print arrays inside jit'd functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG5HQ2dnKZ67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e4399277-07a7-49cf-eae2-465f737cd5ba"
      },
      "source": [
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y1 = f(2)\n",
        "\n",
        "@jit\n",
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y2 = f(2)\n",
        "\n",
        "print(y1)\n",
        "print(y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "Traced<ShapedArray(int32[]):JaxprTrace(level=-1/1)>\n",
            "Traced<ShapedArray(int32[]):JaxprTrace(level=-1/1)>\n",
            "4\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ0T4PxnKgBN"
      },
      "source": [
        "## A few differences from Numpy\n",
        "\n",
        "Below we list a few items where Jax differs from Numpy.\n",
        "See also the official [list of common gotchas](https://colab.research.google.com/github/google/jax/blob/master/notebooks/Common_Gotchas_in_JAX.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOZdeYBKjWc"
      },
      "source": [
        "### Random number generation\n",
        "\n",
        "The API for Jax is basically identical to Numpy, except for pseudo random number\n",
        "generation (PRNG).\n",
        "This is because Jax does not maintain any global state, i.e., it is purely functional.\n",
        "This design \"provides reproducible results invariant to compilation boundaries and backends,\n",
        "while also maximizing performance by enabling vectorized generation and parallelization across random calls\"\n",
        "(to quote [the official page](https://github.com/google/jax#a-brief-tour)).\n",
        "                              \n",
        "Thus, whenever we do anything stochastic, we need to give it a fresh RNG key. We can do this by splitting the existing key into pieces. We can do this indefinitely, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcTYfznjKeHC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "018242a2-ce96-45a9-9078-0a33dcc07017"
      },
      "source": [
        "import jax.random as random\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]  ## identical results\n",
        "\n",
        "# To make a new key, we split the current key into two pieces.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n",
        "\n",
        "# We can continue to split off new pieces from the global key.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n",
        "\n",
        "# We can always use original numpy if we like (although this may interfere with the deterministic behavior of jax)\n",
        "onp.random.seed(42)\n",
        "print(onp.random.randn(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.138 -1.221 -0.592]\n",
            "[-0.066  0.167  1.178]\n",
            "[ 0.497 -0.138  0.648]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmI9DH2K_nl"
      },
      "source": [
        "### Implicitly casting lists to vectors\n",
        "\n",
        "You cannot treat a list of numbers as a vector. Instead you must explicitly create the vector using the np.array() constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUURw01jKnXC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "ebddc278-19e6-4287-8aea-c65a80e3fb71"
      },
      "source": [
        "# You cannot treat a list of numbers as a vector. \n",
        "S = np.diag([1.0, 2.0, 3.0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f418fdbb17da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdiag\u001b[0;34m(v, k)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1721\u001b[0m   \u001b[0mv_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m     \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36mfull_like\u001b[0;34m(x, fill_value, dtype, shape)\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \"\"\"\n\u001b[1;32m   1237\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_canonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtie_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: data type not understood"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMgaegpLCYw"
      },
      "source": [
        "# Instead you should explicitly construct the vector.\n",
        "\n",
        "S = np.diag(np.array([1.0, 2.0, 3.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AieugSOZLGi5"
      },
      "source": [
        "### Mutation of arrays \n",
        "\n",
        "Since JAX is functional, you cannot mutate arrays in place,\n",
        "since this makes program analysis and transformation very difficult. JAX requires a pure functional expression of a numerical program.\n",
        "Instead, JAX offers the functional update functions: `index_update`, `index_add`, `index_min`, `index_max`, and the `index` helper. These are illustrated below. \n",
        "\n",
        "Note: If the input values of `index_update` aren't reused, jit-compiled code will perform these operations in-place, rather than making a copy. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKfhvrTLEBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "7694ce2b-8813-4c60-ea28-ca0e5bf6e844"
      },
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "jax_array = np.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "jax_array[1, :] = 1.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9ec259a55556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# In place update of JAX's array will yield an error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjax_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   2847\u001b[0m          \u001b[0;34m\"immutable; perhaps you want jax.ops.index_update or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m          \"jax.ops.index_add instead?\")\n\u001b[0;32m-> 2849\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m _operators = {\n",
            "\u001b[0;31mTypeError\u001b[0m: '<class 'jax.lax.lax._FilledConstant'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBi15EAcLIru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0546aab4-12bb-4aa9-d983-8744079b78ca"
      },
      "source": [
        "from jax.ops import index, index_add, index_update\n",
        "\n",
        "jax_array = np.zeros((3, 3))\n",
        "print(\"original array:\")\n",
        "print(jax_array)\n",
        "\n",
        "new_jax_array = index_update(jax_array, index[1, :], 1.)\n",
        "\n",
        "new_jax_array2 = index_add(new_jax_array, index[:, 2], 7.)\n",
        "print(\"new array post update\")\n",
        "print(new_jax_array)\n",
        "\n",
        "print(\"new array post add\")\n",
        "print(new_jax_array2)\n",
        "\n",
        "print(\"old array unchanged:\")\n",
        "print(jax_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original array:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "new array post update\n",
            "[[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "new array post add\n",
            "[[0. 0. 7.]\n",
            " [1. 1. 8.]\n",
            " [0. 0. 7.]]\n",
            "old array unchanged:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7U9Ee15LLC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}